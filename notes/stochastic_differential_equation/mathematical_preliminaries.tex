\begin{exercise}{2.1}\label{ex:2.1}
    Suppose that $X:\Omega\to\R$ is a function which takes only countably 
    many values $a_1, a_2, \ldots\in\R$. 
    \begin{thmenum}
        \item Show that $X$ is a random variable if and only if 
        \begin{equation*}
            X^{-1}(a_i) \in \F \text{ for all } i\in\N.
        \end{equation*}
        \item Suppose that $X$ is a random variable. Show that 
        \begin{equation*}
            E\sbrc{\abs{X}} = \sum_{i=1}^\infty \abs{a_i} P(X = a_i).
        \end{equation*}
        \item If $X$ is a random variable and $E\sbrc{\abs{X}}<\infty$, 
        show that 
        \begin{equation*}
            E\sbrc{X} = \sum_{i=1}^\infty a_i P(X = a_i).
        \end{equation*}
        \item If $X$ is a random variable and $f:\R\to\R$ is measurable and 
        bounded, show that 
        \begin{equation*}
            E\sbrc{f(X)} = \sum_{i=1}^\infty f(a_i) P(X = a_i).
        \end{equation*}
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), suppose first that $X$ is a random variable. Since $\set{a_i}$ are 
    Borel sets, $X^{-1}(a_i)\in\F$ for all $i\in\N$. Conversely, assume that 
    $X^{-1}(a_i)\in\F$ for all $a_i$. Since the range of $X$ is $\set{a_i}_{i\in\N}$, 
    for any Borel set $B\subset\R$, $X^{-1}(B) = \bigcup_{a_i\in B} X^{-1}(a_i)\in\F$, 
    by the definition of $\sigma$-algebra. Thus, $X$ is a random variable. 

    For (b), since $X$ takes only countably many values, so does $\abs{X}$ with 
    $\set{\abs{a_i}}_{i\in\N}$. By the definition of expectation, we have 
    \begin{equation*}
        E\sbrc{\abs{X}} = \sum_{i=1}^\infty \abs{a_i} P(X = a_i)
    \end{equation*}
    in the extended sense. 

    For (c), since $E\sbrc{\abs{X}} < \infty$ and $X$ is a random variable, 
    the series converges absolutely and is well-defined. Hence 
    \begin{equation*}
        E\sbrc{X} = \sum_{i=1}^\infty a_i P(X = a_i).
    \end{equation*}

    For (d), since $f$ is measurable, $f^{-1}(B)$ is Borel and $X^{-1}f^{-1}(B)$ 
    is measurable. $f(X)$ takes only countably many values, $f(a_1), f(a_2), \ldots$. 
    The definition of expectation gives us 
    \begin{equation*}
        E\sbrc{f(X)} = \sum_{i=1}^\infty f(a_i) P(f(X) = f(a_i)) = \sum_{i=1}^\infty f(a_i) P(X = a_i).
    \end{equation*}
\end{solution}

\begin{exercise}{2.2}\label{ex:2.2}
    $X:\Omega\to\R$ is a random variable. The distribution function $F$ of $X$ is 
    defined as 
    \begin{equation*}
        F(x) = P(X\leq x). 
    \end{equation*}
    \begin{thmenum}
        \item Prove that $F$ has the following properties:
        \begin{enumerate}[label=(\roman*)]
            \item $0\leq F\leq 1$, $\lim_{x\to-\infty} F(x) = 0$ and $\lim_{x\to\infty} F(x) = 1$.
            \item $F$ is non-decreasing. 
            \item $F$ is right-continuous. 
        \end{enumerate}
        \item $g:\R\to\R$ is measurable such that $E\sbrc{\abs{g(X)}} < \infty$. Show 
        that 
        \begin{equation*}
            E\sbrc{g(X)} = \int_{-\infty}^\infty g(x) dF(x).
        \end{equation*}
        \item Let $p(x)\geq 0$ be measurable on $\R$ be the density of $X$, i.e., 
        \begin{equation*}
            F(x) = \int_{-\infty}^x p(t) dt.
        \end{equation*}
        Find density of $B_t^2$. 
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), since $P$ is a probability measure, $0\leq P(S)\leq 1$ for any $S\in\F$. 
    In particular, $0\leq P(X\leq x)\leq 1$ for all $x\in\R$. Also, we can take 
    $x_n\searrow-\infty$ and $\abs{X\leq x_n}\searrow\varnothing$ as $n\to\infty$. 
    Hence 
    \begin{equation*}
        \lim_{x\to-\infty} F(x) = \lim_{n\to\infty} P(X\leq x_n) = P(\varnothing) = 0.
    \end{equation*} 
    Similarly, we can take $x_n\nearrow\infty$ and $\abs{X\leq x_n}\nearrow\Omega$ as
    $n\to\infty$. Hence
    \begin{equation*}
        \lim_{x\to\infty} F(x) = \lim_{n\to\infty} P(X\leq x_n) = P(\Omega) = 1.
    \end{equation*}
    (i) is proved. For (ii), $F$ is non-decreasing because if $x_1 < x_2$, then 
    \begin{equation*}
        F(x_1) = P(X\leq x_1) \leq P(X\leq x_2) = F(x_2).
    \end{equation*}
    For (iii), let $h>0$. 
    \begin{equation*}
        F(x+h) - F(x) = P(X\leq x+h) - P(X\leq x) = P(x < X \leq x+h).
    \end{equation*}
    For any $y>x$, there exists $h>0$ such that $y > x+h$. Thus $(x, x+h]\searrow\varnothing$ as $h\to 0$. 
    Hence 
    \begin{equation*}
        F(x+h) - F(x) = P(x < X \leq x+h) \to P(\varnothing) = 0
    \end{equation*}
    as $h\to 0$. Therefore, $F$ is right-continuous.

    For (b), by definition of expectation, the left-hand side is
    \begin{equation*}
        E\sbrc{g(X)} = \int_\R g(x) d\mu_X(x), 
    \end{equation*}
    where $\mu_X(B) = P(X^{-1}(B))$ for any Borel set $B\subset\R$.

    For (c), 
    \begin{equation*}
        F(x) = P(B_t^2\leq x) = P(B_t \leq \sqrt{x}) = \int_{-\infty}^{\sqrt{x}} \frac{1}{\sqrt{2\pi t}}\exp\pth{-\frac{u^2}{2t}} du.
    \end{equation*}
    Hence, 
    \begin{equation*}
        p(u) = \od{}{x}F(x) = \frac{1}{\sqrt{2\pi t}}\exp\pth{-\frac{x}{2t}}\frac{1}{2\sqrt{x}}.
    \end{equation*}
\end{solution}

\begin{exercise}{2.3}\label{ex:2.3}
    Let $\set{\F_i}_{i\in\I}$ be a collection of $\sigma$-algebras on $\Omega$. Prove that 
    \begin{equation*}
        \F = \bigcap_{i\in\I} \F_i
    \end{equation*}
    is again a $\sigma$-algebra.
\end{exercise}
\begin{solution}
    First, since $\F_i$ are $\sigma$-algebras, they contain $\varnothing$ and hence 
    $\varnothing\in\F$. For any $A\in\F$, $A\in\F_i$ for all $i\in\I$ and hence 
    $A^c\in\F_i$ for all $i\in\I$. Thus $A^c\in\F$. Finally, for any countable 
    collection $\set{A_n}_{n\in\N}\subset\F$, we have $A_n\in\F_i$ for all 
    $i\in\I$ and all $n\in\N$. Then $\bigcup_n A_n\in\F_i$ for all $i\in\I$. 
    Hence $\bigcup_n A_n\in\F$. Therefore, $\F$ is a $\sigma$-algebra.
\end{solution}

\begin{exercise}{2.4}\label{ex:2.4}$ $\vspace{-1.5em}
    \begin{thmenum}
        \item Let $X:\Omega\to\R$ be a random variable such that $E\sbrc{\abs{X}^p}<\infty$ 
        for some $p\in(0,\infty)$. Prove the Chebyshev's inequality:
        \begin{equation*}
            P(\abs{X}\geq \lambda) \leq \frac{1}{\lambda^p}E\sbrc{\abs{X}^p}
        \end{equation*}
        for any $\lambda>0$. 
        \item Suppose there exists $k>0$ such that $M = E\sbrc{\exp(k\abs{X})}<\infty$. 
        Prove that $P(\abs{X}\geq \lambda)\leq Me^{-k\lambda}$ for any $\lambda>0$.
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), directly estimate that 
    \begin{equation*}
        P(\abs{X}\geq \lambda) = \int_\Omega \chi_{\set{\abs{X}^p\geq \lambda^p}}dP 
        \leq \int_\Omega \frac{\abs{X}^p}{\lambda^p}dP = \frac{1}{\lambda^p}E\sbrc{\abs{X}^p}.
    \end{equation*}

    (b) is similar: 
    \begin{equation*}
        P(\abs{X}\geq \lambda) = \int_\Omega \chi_{\set{\exp(k\abs{X})\geq \exp(k\lambda)}}dP 
        \leq \int_\Omega \exp(k\abs{X})\exp(-k\lambda)dP = M\exp(-k\lambda).
    \end{equation*}
\end{solution}

\begin{exercise}{2.5}\label{ex:2.5}
    Let $X,Y:\Omega\to\R$ be two independent random variables and assume for simplicity 
    that $X,Y$ are bounded. Prove that 
    \begin{equation*}
        E\sbrc{XY} = E\sbrc{X}E\sbrc{Y}.
    \end{equation*}
\end{exercise}
\begin{solution}
    For any $\epsilon>0$, by definition of the expectation, we can find simple functions 
    $s$ and $t$ on $\Omega$ such that 
    \begin{equation*}
        \int\abs{s-X}dP < \epsilon, \quad
        \int\abs{t-Y}dP < \epsilon, \quad\Rightarrow\quad
        \abs{E\sbrc{X} - \int s dP} < \epsilon, \quad
        \abs{E\sbrc{Y} - \int t dP} < \epsilon,
    \end{equation*}
    where $s$ and $t$ can be written as 
    \begin{equation*}
        s = \sum_{i=1}^n s_i \chi_{X^{-1}[s_i,s_{i+1})}\quad\text{and}\quad
        t = \sum_{j=1}^m t_j \chi_{Y^{-1}[t_j,t_{j+1})},
    \end{equation*}
    with $s_i$ and $t_j$ being arranged in ascending order. Thus, 
    \begin{equation*}
        \begin{split}
            \int st dP &= \sum_{i=1}^n\sum_{j=1}^m s_i t_j P(\set{X\in [s_i,s_{i+1})}\cap \set{Y\in [t_j,t_{j+1})}) \\
            &= \sum_{i=1}^n\sum_{j=1}^m s_i t_j P(X\in [s_i,s_{i+1}))P(Y\in [t_j,t_{j+1})) \\
            &= \pth{\sum_{i=1}^n s_i P(X\in [s_i,s_{i+1}))}\pth{\sum_{j=1}^m t_j P(Y\in [t_j,t_{j+1}))} 
            = \pth{\int s dP} \pth{\int t dP}.
        \end{split}
    \end{equation*}
    Also, 
    \begin{equation*}
        \abs{E\sbrc{XY} - \int stdP} 
        \leq \abs{\int \abs{X-s}\abs{t}dP} + \abs{\int \abs{Y-t}\abs{X}dP}.
    \end{equation*}
    $X$ and $Y$ are bounded, say by $M$ and $N$ respectively. Then $t$ is also 
    bounded by $N$ from our construction. Thus 
    \begin{equation*}
        \abs{E\sbrc{XY} - \int stdP} \leq M\epsilon + N\epsilon.
    \end{equation*}
    Combine the results above, we arrive at
    \begin{equation*}
        \begin{split}
            \abs{E\sbrc{XY} - E\sbrc{X}E\sbrc{Y}} 
            &\leq \abs{E\sbrc{XY} - \int stdP} + \abs{E\sbrc{X}E\sbrc{Y} - \int s dP \int t dP} \\ 
            &\leq (M+N)\epsilon + \abs{E\sbrc{X} - \int sdP}\abs{\int t dP} + \abs{E\sbrc{Y} - \int tdP}\abs{E\sbrc{X}} \\ 
            &\leq (M+N)\epsilon + \epsilon N + \epsilon M.
        \end{split}
    \end{equation*}
    Since $\epsilon$ is arbitrary, we conclude that $E\sbrc{XY} = E\sbrc{X}E\sbrc{Y}$.
\end{solution}

\begin{exercise}{2.6}\label{ex:2.6}
    Let $(\Omega,\F,P)$ be a probability space and $A_1,\ldots\in\F$ be sets such that 
    \begin{equation*}
        \sum_{i=1}^\infty P(A_i) < \infty.
    \end{equation*}
    Prove the Borel-Cantelli lemma:
    \begin{equation*}
        P\pth{\bigcap_{m=1}^\infty\bigcup_{i=m}^\infty A_i} = 0.
    \end{equation*}
\end{exercise}
\begin{solution}
    Set $B_m = \bigcup_{i=m}^\infty A_i$ be measurable. Then 
    \begin{equation*}
        P(B_m) \leq \sum_{i=m}^\infty P(A_i) \to 0
    \end{equation*}
    as $m\to\infty$ by the assumption. Thus 
    \begin{equation*}
        P\pth{\bigcap_{m=1}^\infty B_m} \leq \lim_{n\to\infty} P\pth{\bigcap_{m=1}^n B_m}
        \leq \lim_{n\to\infty} P(B_n) = 0.
    \end{equation*}
\end{solution}

\begin{exercise}{2.7}\label{ex:2.7}$ $\vspace{-1.5em}
    \begin{thmenum}
        \item Suppose $G_1,\ldots,G_n$ are disjoint sets in $\F$ such that $\bigcup_{i=1}^n G_i = \Omega$.
        Prove that the family 
        \begin{equation*}
            \G = \Set{G}{\text{$G$ is a union of some $G_i$}}\cup\set{\varnothing}
        \end{equation*}
        is a $\sigma$-algebra.
        \item Prove that every finite $\sigma$-algebra is of type $\G$ as in (a).
        \item Let $\F$ be a finite $\sigma$-algebra on $\Omega$ and $X:\Omega\to\R$ 
        be $\F$-measurable. Prove that $X$ is simple. 
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), first, $\varnothing\in\G$ by definition. Let $G\in\G$. Then $G = 
    \cup_{i\in\I}G_i$ for some $\I\subset\set{1,\ldots,n}$, with the convention that 
    $\cup_{i\in\varnothing}G_i = \varnothing$. Then $G^c = \bigcup_{i\notin\I}G_i\in\G$. 
    Lastly, for countably many $G_i\in\G$, since $\G$ is finite, there are in fact 
    finitely many distinct $G_i$ and the union must lie in $\G$ by the definition. 
    Hence $\G$ is a $\sigma$-algebra. 

    For (b), let $\F$ be a finite $\sigma$-algebra. Consider the collection 
    \begin{equation*}
        \S = \Set{S\in\F}{\text{$S\cap F = \varnothing$ or $S$ for all $F\in\F$}}.
    \end{equation*}
    Since $\F$ is finite, $\S$ is also finite. We first check that every distinct 
    sets in $\S$ are disjoint. Suppose not. There are $S_1, S_2\in\S$ such that 
    $S_1\cap S_2$ is non-empty. Then $S_1\cap S_2 = S_1 = S_2$, contradicting the 
    assumption that $S_1$ and $S_2$ are distinct. Thus every distinct sets in 
    $\S$ are disjoint. Next, we check that $\bigcup_{S\in\S} S = \Omega$. If not, 
    let $A = \Omega\setminus\bigcup_{S\in\S} S$ be non-empty and $A\cap F$ is a 
    non-empty proper subset of $A$ for some $F\in\F$. But then $A\cap F$ or 
    $A\cap F^c$ must satisfy the condition that there is some $F'\in\F$ such that
    $A\cap F\cap F'$ or $A\cap F^c\cap F'$ is non-empty, proper subset of 
    $A\cap F$ or $A\cap F^c$ respectively. Note that $F'\neq F$ and the process 
    continues. In the end, we can find a infinite sequence of distinct sets lying 
    in $\F$, contradicting the finiteness of $\F$. Thus $\bigcup_{S\in\S} S = \Omega$.
    Finally, by (a),
    \begin{equation*}
        \G = \Set{G}{G\text{ is a union of some } S\in\S}\cup\set{\varnothing}
    \end{equation*}
    is a $\sigma$-algebra. It remains to show that $\G = \F$. Clearly, $\G\subset\F$ since 
    $\S\subset\F$. For any $F\in\F$, we can write $F = \bigcup_{i=1}^n S_i$ for some 
    $S_i\in\S$. Thus $F\in\G$. We end up with $\G = \F$. 
    
    For (c), suppose that $X$ can take infinitely many values $\set{a_i}_{i\in\I}$. 
    Since $X$ is $\F$-measurable, $X^{-1}(\set{a_i})\in\F$ for all $i\in\I$. In particular, 
    $X^{-1}(\set{a_i})$ and $X^{-1}(\set{a_j})$ are disjoint for all $i\neq j$. This 
    implies that $\F$ contains infinitely many disjoint sets, contradicting the 
    finiteness of $\F$. Thus $X$ can only take finitely many values and is simple. 
\end{solution}

\begin{exercise}{2.8}\label{ex:2.8}
    Let $B_t$ be Brownian motion on $\R$, $B_0 = 0$. Put $E = E^0$. 
    \begin{thmenum}
        \item Prove that 
        \begin{equation*}
            E\sbrc{e^{iuB_t}} = e^{-\frac{u^2t}{2}} \text{ for all } u\in\R.
        \end{equation*}
        \item Use the power series expansion of the exponential function to show that
        \begin{equation*}
            E\sbrc{B_t^{2k}} = \frac{(2k)!}{2^k k!} t^k \text{ for all } k\in\N.
        \end{equation*}
        \item Prove that 
        \begin{equation*}
            E\sbrc{f(B_t)} = \int_{\R} f(x) \frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}} dx
        \end{equation*}
        for all measurable functions $f$ on $\R$ such that the integral is finite. Deduce (b) by 
        setting $f(x) = x^{2k}$.
        \item Now suppose that $B_t$ is a $n$-dimensional Brownian motion. Prove that 
        \begin{equation*}
            E^x\sbrc{\abs{B_t - B_s}^4} = n(n+2)\abs{t-s}^2
        \end{equation*}
        for all $n\in\N$ and $0\leq s,t\leq T$.
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), directly compute the expectation:
    \begin{equation*}
        \begin{split}
            E\sbrc{e^{iuB_t}} &= \int_{-\infty}^\infty e^{iux}\frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}dx 
            = e^{-\frac{u^2t}{2}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi t}}e^{-\frac{(x - iut)^2}{2t}}dx
            = e^{-\frac{u^2t}{2}}. 
        \end{split}
    \end{equation*}

    For (b), note that the power series expansion of the exponential function 
    gives 
    \begin{equation*}
        e^{iuB_t} = \sum_{k=0}^\infty \frac{(iuB_t)^k}{k!} 
        \quad\Rightarrow\quad
        E\sbrc{e^{iuB_t}} = \sum_{k=0}^\infty \frac{(iu)^k}{k!} E\sbrc{B_t^k}.
    \end{equation*}
    The right-hand side is 
    \begin{equation*}
        e^{-\frac{u^2t}{2}} = \sum_{k=0}^\infty \frac{1}{k!}\pth{-\frac{u^2t}{2}}^k 
        = \sum_{k=0}^\infty \frac{(-1)^{k}u^{2k}}{2^k k!} t^k.
    \end{equation*}
    For those two expressions to be equal, as a function of $u$, $E\sbrc{B_t^k} = 0$ 
    for odd $k$ and we may rewrite the first expression as 
    \begin{equation*}
        E\sbrc{e^{iuB_t}} = \sum_{k=0}^\infty \frac{(-1)^ku^{2k}}{(2k)!}E\sbrc{B_t^{2k}}. 
    \end{equation*}
    By comparing the coefficients, 
    \begin{equation*}
        E\sbrc{B_t^{2k}} = \frac{(-1)^kt^k}{2^k k!}\cdot\frac{(2k)!}{(-1)^k} 
        = \frac{(2k)!}{2^k k!}t^k.
    \end{equation*}

    For (c), it is clear that $B_t$ has the density $p(x) = \frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}}$.
    It follows that 
    \begin{equation*}
        E\sbrc{f(B_t)} = \int f(x)\frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}}dx. 
    \end{equation*}
    By setting $f:x\mapsto x^{2k}$, we have
    \begin{equation*}
        \begin{split}
            E\sbrc{B_t^{2k}} &= \int x^{2k}\frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}}dx \\
            &= x^{2k+1}\frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}\Big|_{-\infty}^\infty 
            - \int 2kx^{2k}\frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}} - x^{2k}\frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}\frac{x^2}{t}dx \\
            &= \frac{1}{t}E\sbrc{B_t^{2(k+1)}} - 2kE\sbrc{B_t^{2k}}
            \quad\Rightarrow\quad 
            E\sbrc{B_t^{2(k+1)}} = (2k+1)tE\sbrc{B_t^{2k}}.
        \end{split}
    \end{equation*}
    And also when $k = 1$, $E\sbrc{B_t^2} = t$. Suppose $E\sbrc{B_t^{2k}} = 
    \frac{(2k)!}{2^k k!}t^k$. Then 
    \begin{equation*}
        E\sbrc{B_t^{2(k+1)}} = (2k+1)t\frac{(2k)!}{2^k k!}t^k 
        = \frac{(2(k+1))!}{2^{k+1}(k+1)!}t^{k+1}. 
    \end{equation*}
    The conclusion follows by induction. 

    For (d), if $n = 1$, $B_t - B_s\sim N(0, \abs{t-s})$ and $E^x\sbrc{\abs{B_t - B_s}^4} = 3\abs{t-s}^2$.  
    Now suppose that for $n$-dimensional $B_t$, $E^x\sbrc{\abs{B_t-B_s}^4} = n(n+2)\abs{t-s}^2$. 
    Then for $n+1$-dimensional $B_t$,
    \begin{equation*}
        \begin{split}
            E^x\sbrc{\abs{B_t-B_s}^4} &= \int_{\R^n}\int_{\R}(\abs{y}^2 + z^2)^2\frac{1}{\sqrt{(2\pi)^{n+1}\abs{t-s}^{n+1}}}\exp\pth{-\frac{\abs{y}^2 + z^2}{2\abs{t-s}}} dy dz \\ 
            &= \int_{\R^n}\abs{y}^4\frac{1}{\sqrt{(2\pi)^n\abs{t-s}^n}}\exp\pth{-\frac{\abs{y}^2}{2\abs{t-s}}} dy \\
            &\quad+ 2\abs{t-s}\int_{\R^n} \abs{y}^2\frac{1}{\sqrt{(2\pi)^n\abs{t-s}^n}}\exp\pth{-\frac{\abs{y}^2}{2\abs{t-s}}} dy 
            + 3\abs{t-s}^2 \\ 
            &= n(n+2)\abs{t-s}^2 + 2n\abs{t-s}^2 + 3\abs{t-s}^2 = (n+1)(n+3)\abs{t-s}^2.
        \end{split}
    \end{equation*}
    Thus by induction, the conclusion holds for all $n\in\N$. It follows from 
    the Kolmogorov's continuity theorem that we can always set $B_t$ to be a 
    continuous process. 
\end{solution}

\begin{exercise}{2.9}\label{ex:2.9}
    Let $(\Omega, \F, P) = ([0,\infty),\B,\mu)$ be a probability space where 
    $\mu$ is a probability measure such that there is no mass at single points. 
    Define 
    \begin{equation*}
        X_t(\omega) = \begin{cases*}
            1 & if $t = \omega$, \\ 
            0 & if $t\neq \omega$.
        \end{cases*}\quad\text{and}\quad 
        Y_t(\omega) = 0\text{ for all } (t,\omega)\in[0,\infty)\times[0,\infty).
    \end{equation*} 
    Prove that $\set{X_t}$ and $\set{Y_t}$ have the same distributions and $X_t$ 
    is a version of $Y_t$. And yet $t\mapsto Y_t(\omega)$ is continuous for     all $\omega$, while $t\mapsto X_t(\omega)$ is discontinuous for all $\omega$. 
\end{exercise}
\begin{solution}
    First, given any $t$, $X_t$ and $Y_t$ are both random variables. For $t_1,\ldots,t_k\in[0,\infty)$, 
    consider the sets $F_1,\ldots,F_k\in\B$. Since $Y_t$ is constant, 
    \begin{equation*}
        P(Y_{t_1}\in F_1,\ldots,Y_{t_k}\in F_k) = \mathds{1}\set{0\in \cap_{i=1}^k F_i}.
    \end{equation*}
    Also, since $\mu$ has no mass at single points, 
    \begin{equation*}
        P(X_{t_1}\in F_1,\ldots,X_{t_k}\in F_k) = \mathds{1}\set{0\in \cap_{i=1}^k F_i}.
    \end{equation*}
    Thus, $X_t$ and $Y_t$ have the same distributions. Furthermore, for any 
    $t\in[0,\infty)$, $\set{X_t = Y_t} = \Omega\setminus\set{t}$, which has 
    zero measure and hence $X_t$ is a version of $Y_t$. Now since $Y_t$ is 
    constant, $t\mapsto Y_t(\omega)$ is continuous for all $\omega$. On the other 
    hand, for any $\omega\in[0,\infty)$, $X(t, \omega)$ is discontinuous at $t = \omega$, 
    proving that $t\mapsto X_t(\omega)$ is discontinuous for all $\omega$.
\end{solution}

\begin{exercise}{2.10}\label{ex:2.10}
    Prove that the Brownian motion $B_t$ has stationary increments, i.e., 
    given $h>0$, the process $\set{B_{t+h}-B_t}$ has the same distribtuions 
    for all $t$. 
\end{exercise}
\begin{solution}
    For any $h>0$, $B_{t+h}-B_t\sim N(0, h)$. The stationarity follows immediately.
\end{solution}

\begin{exercise}{2.11}\label{ex:2.11}
    If $B_t = (B_t^{(1)},\ldots,B_t^{(n)})$ is an $n$-dimensional Brownian motion, 
    then the component processes $B_t^{(i)}$, $1\leq i\leq n$, are 
    independent Brownian motions. 
\end{exercise}
\begin{solution}
    Since the $B_t$ is continuous almost surely, the component processes 
    $B_t^{(i)}$ are continuous almost surely as well. Now we may regard 
    the component processes as projections of $B_t$ and hence they are 
    normally distributed. $E\sbrc{B_t^{(i)}} = 0$ since $E\sbrc{B_t} = 0$. 
    Also, $\Cov(B_t^{(i)}, B_t^{(j)}) = t\delta_{ij}$ as $\Var(B_t) = tI$. 
    Since for $i\neq j$ the covariance is zero, the component processes 
    are independent. 
\end{solution}

\begin{exercise}{2.12}\label{ex:2.12}
    Let $B_t$ be a Brownian motion and fix $t_0\geq 0$. Prove that the process 
    $\tilde{B}_t = B_{t+t_0} - B_{t_0}$ is a Brownian motion. 
\end{exercise}
\begin{solution}
    First, it is clear that $\tilde{B}_0 = B_{t_0}-B_{t_0} = 0$. Since $B_t$ 
    is almost surely continuous, $B_{t+t_0} - B_{t_0}$ is also almost surely 
    continuous. Also, $B_{t+t_0} - B_{t_0}\sim N(0, t)$. For 
    $s<t<u$, 
    \begin{equation*}
        \Cov(\tilde{B}_u - \tilde{B}_t, \tilde{B}_t - \tilde{B}_s) 
        = \Cov(B_{u+t_0}-B_{t+t_0}, B_{t+t_0}-B_{s+t_0}) = 0. 
    \end{equation*}
    Thus $\tilde{B}_t$ has independent increments. This shows that 
    $\tilde{B}_t$ is a Brownian motion.
\end{solution}

\begin{exercise}{2.13}\label{ex:2.13}
    Let $B_t$ be $2$-dimensional Brownian motion and put 
    \begin{equation*}
        D_\rho = \Set{x\in\R^2}{\abs{x}<\rho}
    \end{equation*}
    for $\rho > 0$. Compute $P^0(B_t\in D_\rho)$. 
\end{exercise}
\begin{solution}
    Since $B_t\sim N(0, tI)$, 
    \begin{equation*}
        P^0(B_t\in D_\rho) = \int_{D_\rho} \frac{1}{2\pi t}\exp\pth{-\frac{\abs{x}^2}{2t}}dx 
        = \int_0^{2\pi}\int_0^\rho \frac{1}{2\pi t}\exp\pth{-\frac{r^2}{2t}}rdrd\theta 
        = 1 - \exp\pth{-\frac{\rho^2}{2t}}.
    \end{equation*}
\end{solution}

\begin{exercise}{2.14}\label{ex:2.14}
    Let $B_t$ be $n$-dimensional Brownian motion and $K\subset\R^n$ be a measure 
    zero set under the Lebesgue measure. Prove that the expected total length 
    of time that $B_t$ spends in $K$ is zero. 
\end{exercise}
\begin{solution}
    Given $\omega\in\Omega$, the process $B_t(\omega)$ spends 
    \begin{equation*}
        \int_0^\infty \mathds{1}\set{B_t(\omega)\in K}dt
    \end{equation*} 
    amount of time in $K$. The expected total length of time that $B_t$ spends 
    in $K$ is 
    \begin{equation*}
        \int_{\Omega}\int_0^\infty\mathds{1}\set{B_t(\omega)\in K}dtdP(\omega) 
        = \int_0^\infty\int_{\Omega}\mathds{1}\set{B_t(\omega)\in K}dP(\omega)dt 
        = 0
    \end{equation*}
    by the Fubini-Tonelli theorem, since $K$ has measure zero under the Lebesgue measure 
    and $B_t^{-1}(K)$ is measure zero under the probability measure $P$ for all $t\geq 0$.
\end{solution}

\begin{exercise}{2.15}\label{ex:2.15}
    Let $B_t$ be an $n$-dimensional Brownian motion starting at $0$ and let 
    $U\in\R^{n\times n}$ be a orthogonal matrix, i.e., $U^TU = I$. Prove that 
    $\tilde{B}_t = UB_t$ is also a Brownian motion. 
\end{exercise}
\begin{solution}
    Since $B_t$ is a Brownian motion and $U$ is a linear transformation on a 
    finite-dimensional space, $\tilde{B}_t$ must be contiuous almost surely 
    and $\tilde{B}_0 = UB_0 = 0$. Also, since $B_t\sim N(0, tI)$, we have 
    $\tilde{B}_t\sim N(0, tU^TU) = N(0, tI)$. For $s<t<u$, 
    \begin{equation*}
        \Cov(\tilde{B}_u - \tilde{B}_t, \tilde{B}_t - \tilde{B}_s) 
        = \Cov(UB_u - UB_t, UB_t - UB_s) 
        = U\Cov(B_u - B_t, B_t - B_s)U^T = 0.
    \end{equation*}
    Thus $\tilde{B}_t$ has independent increments. Therefore, $\tilde{B}_t$ 
    is a Brownian motion.
\end{solution}

\begin{exercise}{2.16}\label{ex:2.16}
    Let $B_t$ be a Brownian motion on $\R$ and $c>0$. Prove that the process 
    $X_t = \frac{1}{c}B_{c^2t}$ is a Brownian motion. 
\end{exercise}
\begin{solution}
    First, $X_0 = \frac{1}{c}B_0 = 0$. Suppose $\omega\in\Omega$ is such that 
    $B_t(\omega)$ is continuous. For $\epsilon>0$, there exists $\delta>0$ such 
    that $\abs{B_{t}(\omega)-B_{s}(\omega)}<c\epsilon$ as long as $\abs{t-s}<\delta$. 
    Then we may set $\delta' = \delta/c^2$ and see that 
    \begin{equation*}
        \abs{X_t(\omega) - X_s(\omega)} = \frac{1}{c}\abs{B_{c^2t}(\omega) - B_{c^2s}(\omega)} 
        < \epsilon
    \end{equation*}
    whenever $\abs{t-s}<\delta'\Leftrightarrow \abs{c^2t-c^2s}<\delta$. 
    Thus $X_t$ is continuous almost surely. Also, since $B_{c^2t}\sim N(0, c^2tI)$, 
    $X_t\sim N(0, tI)$. For $s<t<u$, 
    \begin{equation*}
        \Cov(X_u - X_t, X_t - X_s) = \Cov\pth{\frac{1}{c}B_{c^2u} - \frac{1}{c}B_{c^2t}, 
        \frac{1}{c}B_{c^2t} - \frac{1}{c}B_{c^2s}} 
        = \frac{1}{c^2}\Cov(B_{c^2u} - B_{c^2t}, B_{c^2t} - B_{c^2s}) = 0.
    \end{equation*}
    We conclude that $X_t$ is a Brownian motion.
\end{solution}

\begin{exercise}{2.17}\label{ex:2.17}
    Let $B_t$ be a Brownian motion on $\R$. Show that the quadratic variation 
    process is $\inp{B}{B}_t^2(\omega) = t$ almost surely by the following steps: 
    \begin{thmenum}
        \item Define $\Delta B_k = B_{t_{k+1}} - B_{t_k}$ and put $Y(t,\omega) 
        = \sum_{t_k\leq t} (\Delta B_k)^2$ for a partition $\set{t_k}_{k=1}^n$ of $[0,t]$. 
        Show that 
        \begin{equation*}
            E\sbrc{(Y_t - t)^2} = 2\sum_{t_k\leq t} (\Delta t_k)^2
        \end{equation*}
        and deduce that $Y(t,\cdot)\to t$ in $L^2$ as $\Delta t_k\to 0$. 
        \item Use (a) to show that almost every paths of $B_t$ does not have a bounded 
        total variation on $[0,t]$.
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), note that $\Delta B_k$ are independent and $\Delta B_k\sim N(0, \Delta t_k)$. 
    \begin{equation*}
        \begin{split}
            E\sbrc{(Y_t - t)^2} &= E\sbrc{\pth{\sum_{t_k\leq t} (\Delta B_k)^2 - \Delta t_k}^2} 
            = E\sbrc{\sum_{t_k\leq t} ((\Delta B_k)^2 - \Delta t_k)^2} \\ 
            &= \sum_{t_k\leq t} E\sbrc{(\Delta B_k)^4} - 2E\sbrc{(\Delta B_k)^2}\Delta t_k + (\Delta t_k)^2 \\
            &= \sum_{t_k\leq t} 3(\Delta t_k)^2 - 2(\Delta t_k)^2 + (\Delta t_k)^2  \\
            &= 2\sum_{t_k\leq t} (\Delta t_k)^2\leq 2\max_{k}\Delta t_k\sum_{t_k\leq t} \Delta t_k 
            = 2t\max_{k}\Delta t_k\to 0
        \end{split}
    \end{equation*}
    as $\max_{k}\Delta t_k\to 0$, the right-hand side converges to $0$ and hence $Y(t, \cdot)\to t$ 
    in $L^2(P)$. Hence $\inp{B}{B}_t^2(\omega) = t$ almost surely. 

    For (b), let $\P$ be a partition of $[0,t]$. With respect to $\P$, define 
    the total variation of $B_t$ as 
    \begin{equation*}
        Z^\P_t(\omega) = \sum_{t_k\leq t} \abs{B_{t_{k+1}}(\omega) - B_{t_k}(\omega)} 
        \leq N(\P)\sum_{t_k\leq t} \abs{B_{t_{k+1}}(\omega) - B_{t_k}(\omega)}^2\to\infty
    \end{equation*}
    as $\norm{\P}\to 0$ by the Cauchy inequality and (a) that $Y(t, \cdot)\to t$ in $L^2(P)$.
\end{solution}

\begin{exercise}{2.18}\label{ex:2.18}
    Let $\Omega = \set{1,2,3,4,5}$ and $\U = \set{\set{1,2,3}, \set{3,4,5}}$ 
    be a collection of subsets of $\Omega$.
    \begin{thmenum}
        \item Find $\sigma(\U)$. 
        \item Define $X:\Omega\to\R$ by
        \begin{equation*}
            X(1) = X(2) = 0,\quad X(3) = 10,\quad X(4) = X(5) = 1. 
        \end{equation*}
        Is $X$ $\sigma(\U)$-measurable?
        \item Define $Y:\Omega\to\R$ by
        \begin{equation*}
            Y(1) = 0,\quad Y(2) = Y(3) = Y(4) = Y(5) = 1.
        \end{equation*}
        Find $\sigma(Y)$. 
    \end{thmenum}
\end{exercise}
\begin{solution}
    For (a), 
    \begin{equation*}
        \sigma(\U) = \set{\varnothing, \set{1,2}, \set{3}, \set{4,5}, \set{1,2,3}, \set{3,4,5}, \set{1,2,4,5}, \Omega}.
    \end{equation*}
    
    For (b), it is not hard to verify that $X$ is $\sigma(\U)$-measurable. 

    For (c), 
    \begin{equation*}
        \sigma(Y) = \set{\varnothing, \set{1}, \set{2,3,4,5}, \Omega}. 
    \end{equation*}
\end{solution}

\begin{exercise}{2.19}\label{ex:2.19}
    Prove that every convergent sequence is a Cauchy sequence.
\end{exercise}
\begin{solution}
    Let $\set{x_n}$ be a convergent sequence with limit $x$. For any $\epsilon > 0$, 
    we may find $N\in\N$ such that for all $n\geq N$, $d(x_n, x)<\epsilon/2$. 
    Then for all $m,n\geq N$,
    \begin{equation*}
        d(x_n,x_m)\leq d(x_n, x) + d(x_m, x) < \epsilon/2 + \epsilon/2 = \epsilon.
    \end{equation*}
    Hence $\set{x_n}$ is a Cauchy sequence.
\end{solution}

\begin{exercise}{2.20}\label{ex:2.20}
    Let $B_t$ be a $1$-dimensional Brownian motion, $\sigma\in\R$ and 
    $0\leq s < t$. Prove that
    \begin{equation*}
        E\sbrc{\exp\pth{\sigma(B_s - B_t)}} = \exp\pth{\frac{\sigma^2(s-t)}{2}}.
    \end{equation*}
\end{exercise}
\begin{solution}
    Since $B_s-B_t\sim N(0, t-s)$, 
    \begin{equation*}
        \begin{split}
            E\sbrc{\exp\pth{\sigma(B_s - B_t)}} &= \int_{-\infty}^\infty e^{\sigma x}\frac{1}{\sqrt{2\pi(t-s)}}e^{-\frac{x^2}{2(t-s)}}dx \\
            &= e^{\frac{\sigma^2(t-s)}{2}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi(t-s)}}e^{-\frac{(x - \sigma(t-s))^2}{2(t-s)}}dx 
            = e^{\frac{\sigma^2(t-s)}{2}}.
        \end{split}
    \end{equation*}
\end{solution}