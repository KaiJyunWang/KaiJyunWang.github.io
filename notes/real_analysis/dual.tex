\begin{theorem}[Dualities of $\ell^p$ Spaces]
    Let $1< p < \infty$. Then $(\ell^p)' \cong \ell^{p'}$, where 
    $p'$ is the conjugate exponent of $p$.
\end{theorem}
\begin{proof}
    We need to prove that there exists an isometric isomorphism 
    $\psi:\ell^{p'}\to\pth{\ell^{p}}'$ such that $\psi gf = \sum_i f_ig_i$ 
    for all $g\in\ell^{p'}$ and $f\in\ell^p$. We show that $\psi$ 
    is well-defined, linear, bounded, bijective, and isometric. 

    First, we show that $\psi$ is well-defined. For $f\in\ell^p$ and 
    $g\in\ell^{p'}$, 
    \begin{equation*}
        \abs{\psi gf} \leq \sum_i \abs{f_ig_i} \leq \norm{f}_p\norm{g}_{p'} < \infty
    \end{equation*}
    by the H\"older's inequality. Thus $\psi g\in\pth{\ell^{p}}'$ is 
    well-defined. 

    Next, $\psi$ is linear since for $g_1,g_2\in\ell^{p'}$ and $c\in\R$, 
    \begin{equation*}
        \psi(cg_1 + g_2)(f) = \sum_i f_i(cg_{1i}+g_{2i}) = c\sum_i f_ig_{1i} + \sum_i f_ig_{2i} = c\psi g_1(f) + \psi g_2(f)
    \end{equation*}
    for all $f\in\ell^p$. Hence $\psi(cg_1 + g_2) = c\psi g_1 + \psi g_2$. 

    Now, to show that $\psi$ is bounded,
    \begin{equation*}
        \begin{split}
            \norm{\psi g} &= \sup\Set{\abs{\psi gf}}{\norm{f}_p = 1} 
            = \sup\Set{\abs{\sum_i f_ig_i}}{\norm{f}_p = 1} \\
            &\leq \sup_{\norm{f}_p = 1}\set{\norm{g}_{p'}} \leq \norm{g}_{p'}.
        \end{split}
    \end{equation*}
    We see that $\norm{\psi}\leq 1$. Next, let 
    $h\in\pth{\ell^p}'$ and define $g$ by $g_i = h(e_i)$. Then 
    \begin{equation*}
        \begin{split}
            \norm{g}_{p'} &= \pth{\sum_i \abs{g_i}^{p'}}^{1/p'} 
            = \pth{\sum_i \abs{h(e_i)}^{p'}}^{1/p'} 
            \leq \pth{\sum_i \norm{h}^{p'}}^{1/p'} 
            = \norm{h}.
        \end{split}
    \end{equation*}
    Then $g\in\ell^{p'}$. Furthermore, for such $g$,
    \begin{equation*}
        \psi g(f) = \sum_i f_ig_i = \sum_i f_ih(e_i) = h\pth{\sum_i f_ie_i} = h(f)
    \end{equation*} 
    for every $f\in\ell^p$. Hence $\psi$ is surjective and $\norm{\psi g} 
    = \norm{h}$. The isometry of $\psi$ is immediate from that 
    \begin{equation*}
        \norm{\psi g} \leq \norm{g}_{p'} \leq \norm{h} = \norm{\psi g}.
    \end{equation*} 
    Finally, $\psi$ is injective since otherwise there exists 
    $g\neq 0$ such that $\psi g = 0$. Then $\norm{g}_{p'} = 0$ 
    by the isometry of $\psi$, which implies that $g = 0$, 
    a contradiction. We conclude that $\psi$ is an isometric 
    isomorphism and the proof is complete.
\end{proof}

\begin{proposition}\label{prop:dual_isometry}
    $1\leq p<\infty$. $1/p + 1/p' = 1$. Let $g\in\L^{p'}(X,\mu)$. Then the 
    mapping $Tg:\L^p(X,\mu)\to\R$ defined by 
    \begin{equation*}
        Tg(f) = \int_X fg d\mu
    \end{equation*}
    is a bounded linear functional. Furthermore, $\norm{Tg}_{\L^p\to\R} 
    = \norm{g}_{p'}$.
\end{proposition}
\begin{proof}
    We start by checking that $Tg$ is well-defined. For $f\in\L^p$, 
    \begin{equation*}
        \abs{Tg(f)} = \abs{\int fgd\mu} \leq \int \abs{fg}d\mu \leq \norm{f}_p\norm{g}_{p'}
    \end{equation*}
    by H\"older's inequality. Thus $Tg(f)\in\R$. Also, we obtain that 
    $\norm{Tg}_{\L^p\to\R}\leq\norm{g}_{p'}$. For the linearity, let $c\in\R$ 
    and $f_1,f_2\in\L^p$.
    \begin{equation*}
        Tg(cf_1+f_2) = \int (cf_1+f_2)gd\mu = c\int f_1gd\mu + \int f_2gd\mu 
        = cTg(f_1) + Tg(f_2).
    \end{equation*}
    Lastly, to furnish the isometry, let $g\neq 0$ and define 
    \begin{equation*}
        f = \sgn(g)\pth{\frac{\abs{g}}{\norm{g}_{p'}}}^{p'/p}
        \implies \int \abs{f}^pd\mu 
        = \int \pth{\frac{\abs{g}}{\norm{g}_{p'}}}^{p'}d\mu<\infty.
    \end{equation*}
    Then $f\in\L^p$ and $\norm{f}_p = 1$. Also, 
    \begin{equation*}
        Tg(f) = \int \sgn(g)\pth{\frac{\abs{g}}{\norm{g}_{p'}}}^{p'/p}gd\mu 
        = \norm{g}_{p'}.
    \end{equation*}
    It follows that $\norm{Tg}_{\L^p\to\R} = \norm{g}_{p'}$.
\end{proof}

\begin{theorem}[Riesz Representation]
    Let $(X,\A,\mu)$ be a $\sigma$-finite measure space and $1\leq p<\infty$. 
    Then the mapping $T:\L^{p'}(X,\mu)\to(\L^p(X,\mu))'$ defined by 
    $Tg\in\L^p(X,\mu)$, 
    \begin{equation*}
        Tg(f) = \int fg d\mu,
    \end{equation*}
    is an isometric isomorphism.
\end{theorem}
\begin{proof}
    By \cref{prop:dual_isometry}, $Tg$ is a bounded linear functional. 
    Besides, let $c\in\R$ and $g_1,g_2\in\L^{p'}$,
    \begin{equation*}
        T(cg_1+g_2)(f) = \int (cg_1+g_2)fd\mu = c\int g_1fd\mu + \int g_2fd\mu 
        = cTg_1(f) + Tg_2(f) = (cTg_1+Tg_2)(f)
    \end{equation*}
    for all $f\in\L^p$. Thus $T$ is linear. It remains to show 
    that $T$ is a bijection. We first verify that $T$ is surjective. 

    Consider the case where $p>1$ and $\mu(X)<\infty$. Let $h\in(\L^p)'$. 
    Define $\nu:\A\to\R$ by $\nu(A) = h(\chi_A)$. We claim that $\nu$ 
    is a finite measure and $\nu\ll\mu$. Since 
    \begin{equation*}
        \abs{\nu(A)} = \abs{h(\chi(A))} \leq \norm{h}_{\L^p\to\R}\norm{\chi_A}_p 
        = \norm{h}_{\L^p\to\R}\mu(A)^{1/p},
    \end{equation*}
    we see that $\nu$ is finite since so is $\mu$. Also, if $\mu(A) = 0$, 
    then $\abs{\nu(A)} = 0$ and hence $\nu(A) = 0$. Thus $\nu\ll\mu$. 
    For finite additivity, let $A_1,A_2\in\A$ be disjoint. 
    \begin{equation*}
        \nu(A_1\cup A_2) = h(\chi_{A_1\cup A_2}) = h(\chi_{A_1}+\chi_{A_2}) 
        = h(\chi_{A_1}) + h(\chi_{A_2}) = \nu(A_1) + \nu(A_2).
    \end{equation*}
    To show the $\sigma$-additivity, let $A_j\in\A$ be countably many 
    disjoint sets. Put $A = \cup_j A_j$, $A = B_n + C_n$ where 
    $B_n = \cup_{j=1}^{n} A_j$ and $C_n = \cup_{j=n+1}^{\infty} A_j$. 
    Then since $B_n\cap C_n = \varnothing$, 
    \begin{equation*}
        \nu(A) = \nu(B_n + C_n) = \nu(B_n) + \nu(C_n) 
        = \sum_{j=1}^{n}\nu(A_j) + \nu(C_n)
    \end{equation*}
    for all $n$. Since $\mu(X)<\infty$, $\sum_j \mu(A_j)<\infty$ and 
    $\mu(C_n)\to 0$ as $n\to\infty$. Thus 
    \begin{equation*}
        \abs{\nu(C_n)} = \abs{h(C_n)} 
        \leq \norm{h}_{\L^p\to\R}\mu(C_n)^{1/p}\to\infty.
    \end{equation*} 
    We conclude that $\nu(A) = \sum_j \nu(A_j)$ and $\nu$ is a measure. 

    Next, since $\nu\ll\lambda$, by the Radon-Nikodym theorem, 
    there exists a unique $g\in\L^1(X,\mu)$ such that 
    \begin{equation*}
        h(\chi_A) = \nu(A) = \int_A gd\mu = \int_X \chi_A gd\mu 
        = Tg(\chi_A).
    \end{equation*} 
    for arbitrary $A\in\A$. Extend by linearity to $p$-integrable 
    simple functions, say $s = \sum_{i=1}^n c_i\chi_{A_i}$. 
    \begin{equation*}
        h(s) = \sum_{i=1}^n c_ih(\chi_{A_i}) 
        = \sum_{i=1}^n c_i\int_X \chi_{A_i}gd\mu 
        = \int_X \sum_{i=1}^n c_i\chi_{A_i}gd\mu 
        = \int_X sgd\mu = Tg(s).
    \end{equation*}
    For a general $f\in\L^p$, by separating $f = f^+ - f^-$ 
    if necessary, we may assume that $f\geq 0$. By 
    \cref{lem:simple_approx}, there exists a sequence of simple 
    functions $s_n\nearrow f$. Then by Lebesgue's monotone 
    convergence theorem, $\norm{f-s_n}_p\to 0$. Since $h$ is 
    a bounded linear functional, it is continuous, and hence 
    $h(s_n)\to h(f)$ as $n\to\infty$. We obtain that 
    \begin{equation*}
        h(f) = \lim_{n\to\infty} h(s_n) = \lim_{n\to\infty} \int_X s_ngd\mu 
        = \int_X fgd\mu = Tg(f)
    \end{equation*}
    for all $f\in\L^p$. Thus $Tg = h$. It remains to check 
    that $g\in\L^{p'}$. Let 
    \begin{equation*}
        f_n = \begin{cases}
            \abs{g}^{p'-1}\sgn(g) & \text{if } \abs{g(x)}^{p'-1} \leq n, \\
            n\sgn(g) & \text{otherwise}.
        \end{cases}
    \end{equation*}
    Then $f_n\in\L^p$ and $f_ng\nearrow\abs{g}^{p'}$. 
    \begin{equation*}
        \abs{Tg(f_n)} = \abs{\int f_ngd\mu} \leq \norm{Tg}_{\L^p\to\R}\norm{f_n}_p.
    \end{equation*}
    Also, $f_ng = \abs{f_n}\abs{g} \geq \abs{f_n}\abs{f_n}^{1/(p'-1)} = \abs{f_n}^p$ and 
    \begin{equation*}
        \norm{f_n}_p^p = \int \abs{f_n}^pd\mu \leq \int f_ngd\mu \leq \norm{Tg}_{\L^p\to\R}\norm{f_n}_p.
    \end{equation*}
    As a result, 
    \begin{equation*}
        \norm{g}_{p'}^{p'} = \int \abs{g}^{p'}d\mu = \lim_{n\to\infty} \int f_ngd\mu 
        \leq \norm{Tg}_{\L^p\to\R}\norm{f_n}_p < \infty.
    \end{equation*}
    Hence $g\in\L^{p'}$ and $T$ is indeed surjective. Furthermore, 
    such $g$ is unique by the uniqueness of the Radon-Nikodym 
    derivative. We also conclude that $T$ is injective. 

    For the case where $p=1$ and $\mu(X)<\infty$, $p'=\infty$. 
    We consider the same mapping $T$ with $Tg(f) = \int fg d\mu$. 
    We claim that $g\in\L^\infty$. Suppose $g\not\in\L^\infty$. 
    Then for every $K$, the set $A_K = \Set{x\in X}{\abs{g(x)}>K}$ 
    has positive measure. Define $f_K = \sgn(g)\chi_{A_K}/\mu(A_K)$. 
    Note that $\norm{f_K}_1 = 1$. If $g\geq 0$, then 
    \begin{equation*}
        \abs{Tg(f_K)} = \int f_Kgd\mu > K
    \end{equation*}
    for all $K$. But $Tg$ is a bounded linear functional, 
    which is a contradiction. Thus $g\in\L^\infty$. 

    Finally, we prove the case where $X$ is $\sigma$-finite. 
    Write $X = \cup_n X_n$ where $\mu(X_n)<\infty$ and $X_n\subset 
    X_{n+1}$. For every $f\in\L^p(X_k,\mu)$, consider 
    $\hat{f}\in\L^p(X,\mu)$ defined by $\hat{f} = f$ on $X_k$ 
    and $\hat{f} = 0$ on $X-X_k$. Then $\norm{f}_{\L^p(X_k)} 
    = \norm{f}_{\L^p(X)}$. Let $h\in(\L^p(X))'$ and consider 
    $h_k\in (\L^p(X_k))'$ by $h_k(f) = h(\hat{f})$. Then 
    $\norm{h_k}\leq\norm{h}$. By the previous result, we can 
    find a unique $g_k\in\L^{p'}(X_k,\mu)$ such that 
    \begin{equation*}
        h_k(f) = \int f g_k d\mu, \norm{g_k}_{\L^{p'}(X_k)} 
        \leq \norm{h_k} \leq \norm{h}.
    \end{equation*}
    Since $X_n\subset X_{n+1}$, for $f\in\L^p(X_k)$, we have 
    $h_k(f) = h(\hat{f}) = h_{k+1}(f)$ and $g_k = g_{k+1}$ 
    $\mu$-a.e.\ in $X_k$. Define $g = g_k$ on $X_k$ with 
    $\norm{g}_{\L^{p'}(X)}\leq \norm{h}$. Let $f\in\L^p(X,\mu)$. 
    H\"older's inequality implies that $fg\in\L^1(X,\mu)$ and 
    \begin{equation*}
        h(f\chi_{X_k}) = h_k(f) = \int f\chi_{X_k}g_kd\mu
    \end{equation*}
    Since $f\chi_{X_k}\leq \abs{f}$, $f\chi_k\to f\in \L^p(X,\mu)$ 
    by Lebesgue's dominated convergence theorem. Also, 
    \begin{equation*}
        h_k(f) = \int f\chi_{X_k}g_kd\mu \to \int fgd\mu = Tg(f)
    \end{equation*}
    by Lebesgue's dominated convergence theorem. Thus $T$ is 
    indeed the desired isometric isomorphism.
\end{proof}
\begin{remark}
    $(\L^\infty)'\not\cong\L^1$. Consider $C^\infty([-1,1])$, 
    a subspace of $\L^\infty$. Define a linear functional 
    $\delta:\C^\infty([-1,1])\to\R$ by $\delta(f) = f(0)$. 
    Clearly $\delta\in(\L^\infty)'$. Now suppose there exists 
    $g\in\L^1$ such that $\delta(f) = \int_{-1}^{1} fgdx$. 
    Let $f = \chi_A$ where $A$ is measurable. Then 
    $f\in\L^\infty$ and by definition, 
    \begin{equation*}
        0 = f(0) = \delta(f) = \int_{-1}^{1} fgdx = \int_A gdx.
    \end{equation*}
    Thus $g = 0$ a.e.\ and $\delta = 0$, a contradiction.
\end{remark}

\begin{definition}
    $M(X)$ is a space consisting of all finite signed measures. 
    For $\nu\in M(X)$, the total variation norm of $\nu$ is 
    defined by $\norm{\nu} = \nu^+(X) + \nu^-(X)$, where $\nu^+$ 
    and $\nu^-$ are the Hahn-Jordan decompositions of $\nu$. 
\end{definition}

\begin{proposition}
    $M(X)$ with the total variation norm forms a Banach space.
\end{proposition}
\begin{proof}
    Clearly, $M(X)$ forms a vector space. We check that $\norm{\cdot}$ 
    is indeed a norm. For $\nu\in M(X)$, clearly $\norm{\nu}\geq 0$. If 
    $\norm{\nu} = 0$, then $\nu^+(X) = \nu^-(X) = 0$, $\nu^+(A)$ and 
    $\nu^-(A)$ are zero for all $A\in\A$, and hence $\nu = 0$. Conversely, 
    if $\nu = 0$, then so are $\nu^+$ and $\nu^-$ and hence $\norm{\nu} = 0$. 
    For $c\in\R$, 
    \begin{equation*}
        \norm{c\nu} = \abs{c}\nu^+(X) + \abs{c}\nu^-(X) 
        = \abs{c}(\nu^+(X) + \nu^-(X)) = \abs{c}\norm{\nu}.
    \end{equation*}
    Lastly, let $\nu,\mu\in M(X)$. Notice that $(\nu+\mu)^+\leq \nu^+ + \mu^+$ 
    and $(\nu+\mu)^-\leq \nu^- + \mu^-$. Thus 
    \begin{equation*}
        \norm{\nu+\mu} = (\nu+\mu)^+(X) + (\nu+\mu)^-(X) 
        \leq \nu^+(X) + \mu^+(X) + \nu^-(X) + \mu^-(X) 
        = \norm{\nu} + \norm{\mu},
    \end{equation*}
    proving that $\norm{\cdot}$ is indeed a norm. 

    For the completeness, let $\nu_n$ be a Cauchy sequence in $M(X)$. We 
    define a measure $\nu$ by $\nu(A) = \lim_{n\to\infty} \nu_n(A)$ for 
    all $A\in\A$. We claim that the limit exists and $\nu$ is indeed a 
    finite signed measure. Since the sequence is Cauchy, for every 
    $\epsilon>0$, there exists $N$ such that
    \begin{equation*}
        (\nu_m-\nu_n)^+(X) + (\nu_m-\nu_n)^-(X) = \norm{\nu_m-\nu_n} \leq \epsilon
    \end{equation*}
    for all $m,n\geq N$. Since both $(\nu_m-\nu_n)^+$ and $(\nu_m-\nu_n)^-$ 
    are positive measures, we have 
    \begin{equation*}
        (\nu_m-\nu_n)^+(A) \leq (\nu_m-\nu_n)^+(X) \leq \epsilon, 
        \quad \text{and} \quad 
        (\nu_m-\nu_n)^-(A) \leq (\nu_m-\nu_n)^-(X) \leq \epsilon
    \end{equation*}
    for every $A\in\A$. Thus 
    \begin{equation*}
        \abs{\nu_m(A)-\nu_n(A)} = \abs{(\nu_m-\nu_n)^+(A)-(\nu_m-\nu_n)^-(A)} \leq \epsilon.
    \end{equation*}
    It follows that for any fixed $A\in\A$, $\nu_n(A)$ is a Cauchy 
    sequence in $\R$ and hence the limit exists. Also, taking 
    $A = X$, we see that $\nu(X)$ is finite. To show that $\nu$ is 
    a measure, first note that $\nu(\varnothing) = 0$. For finite 
    additivity, let $A_1,A_2\in\A$ be disjoint. Then 
    \begin{equation*}
        \nu(A_1\cup A_2) = \lim_{n\to\infty} \nu_n(A_1\cup A_2) 
        = \lim_{n\to\infty} \nu_n(A_1) + \nu_n(A_2) = \nu(A_1) + \nu(A_2).
    \end{equation*}
    For the $\sigma$-additivity, let $A_n\in\A$ be countably many 
    disjoint sets. Put $A = \cup_n A_n$, $A = B_n\cup C_n$ where 
    $B_n = \cup_{j=1}^{n} A_j$ and $C_n = \cup_{j=n+1}^{\infty} A_j$. 
    Since $\nu(X)<\infty$, $\sum_j \nu(A_j)<\infty$ and hence 
    $\nu(C_n)\to 0$ as $n\to\infty$. Thus 
    \begin{equation*}
        \nu(A) = \nu(B_n) + \nu(C_n) = \sum_{j=1}^{n}\nu(A_j) + \nu(C_n) 
    \end{equation*}
    for every $n$ and by letting $n\to\infty$, we obtain
    $\nu(A) = \sum_j \nu(A_j)$. Finally, fix $n$ and let $m\to\infty$, 
    \begin{equation*}
        \norm{\nu-\nu_n} = \lim_{m\to\infty} \norm{\nu_m-\nu_n} 
        = \lim_{m\to\infty} \abs{\nu_m(X) - \nu_n(X)} = \abs{\nu(X) - \nu_n(X)} 
        \leq \epsilon
    \end{equation*}
    for all $n\geq N$. Thus $\nu_n\to\nu$ in norm and $M(X)$ is complete.
\end{proof}

\begin{definition}
    Let $f:[a,b]\to\R$. The \textbf{variation} of $f$ is defined by 
    \begin{equation*}
        V_{\mathcal{P}}(f) = \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)},
    \end{equation*}
    where $\mathcal{P} = \set{a=t_0<t_1<\cdots<t_n=b}$ is a partition 
    of $[a,b]$. The \textbf{total variation} of $f$ on $[a,b]$ is defined by 
    \begin{equation*}
        V(f) = \sup_{\mathcal{P}} V_{\mathcal{P}}(f).
    \end{equation*}
\end{definition}

\begin{definition}
    The \textbf{bounded variation space} $BV([a,b])$ consists of all 
    functions $f:[a,b]\to\R$ such that $V(f)<\infty$. For $f\in BV([a,b])$, 
    the \textbf{total variation norm} is defined by $\norm{f}_{TV} 
    = \abs{f(a)} + V(f)$.
\end{definition}

\begin{proposition}
    $BV([a,b])$ with the total variation norm forms a Banach space.
\end{proposition}
\begin{proof}
    It clearly forms a vector space. We check that $\norm{\cdot}_{TV}$ 
    is indeed a norm. First, clearly $\norm{f}_{TV}\geq 0$. If 
    $\norm{f}_{TV} = 0$, then $f(a) = 0$ and $f(t) = f(t')$ for all 
    $t,t'\in[a,b]$. Hence $f = 0$; if $f = 0$, then $V(f) = 0$ and 
    $f(a) = 0$ and $\norm{f}_{TV} = 0$. Next, for $c\in\R$, 
    \begin{equation*}
        \norm{cf}_{TV} = \abs{cf(a)} + \sum_{i=0}^{n-1} \abs{cf(t_{i+1})-cf(t_i)} 
        = \abs{c}\pth{\abs{f(a)} + \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)}}
        = \abs{c}\norm{f}_{TV}.
    \end{equation*}
    Lastly, let $f,g\in BV([a,b])$. Then 
    \begin{equation*}
        \begin{split}
            \norm{f+g}_{TV} 
            &= \sup_{\mathcal{P}} \abs{(f+g)(a)} + \sum_{i=0}^{n-1} \abs{(f+g)(t_{i+1})-(f+g)(t_i)} \\ 
            &\leq \sup_{\mathcal{P}} \abs{f(a)} + \abs{g(a)} + \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)} + \sum_{i=0}^{n-1} \abs{g(t_{i+1})-g(t_i)} \\
            &\leq \sup_{\mathcal{P}} \abs{f(a)} + \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)} + \sup_{\mathcal{P}} \abs{g(a)} + \sum_{i=0}^{n-1} \abs{g(t_{i+1})-g(t_i)}  
            = \norm{f}_{TV} + \norm{g}_{TV}.
        \end{split}
    \end{equation*}
    Thus $\norm{\cdot}_{TV}$ is indeed a norm. 

    For the completeness, let $f_n$ be a Cauchy sequence in 
    $BV([a,b])$. For $\epsilon>0$, there exists $N$ such that 
    $\norm{f_m-f_n}_{TV}<\epsilon$ for all $m,n\geq N$. Given 
    any $x\in[a,b]$, consider the partition $\mathcal{P} 
    = \set{a<x<b}$. 
    \begin{equation*}
        \begin{split}
            \abs{f_m(x)-f_n(x)} 
            &= \abs{f_m(x)-f_m(a)+f_m(a)-f_n(a)+f_n(a)-f_n(x)} \\ 
            &\leq \abs{(\pth{f_m(x)-f_n(x)}) - (f_m(a) - f_n(a))} + \abs{f_m(a)-f_n(a)} \\
            &\leq V(f_m-f_n) + \abs{f_m(a)-f_n(a)} = \epsilon.
        \end{split}
    \end{equation*}
    Thus $\set{f_n(x)}$ is a Cauchy sequence in $\R$ and hence 
    converges pointwisely to, say $f(x)$. Furthermore, observe that 
    the choice of $N$ does not depend on $x$, and thus the convergence 
    is uniform. We claim that $f\in BV([a,b])$. Indeed, for any 
    partition $\mathcal{P} = \set{a = t_0<\cdots<t_n = b}$, 
    \begin{equation*}
        \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)} 
        \leq \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f_N(t_{i+1})} + \sum_{i=0}^{n-1} \abs{f(t_i)-f_N(t_i)} + V(f_N).
    \end{equation*}
    Since the convergence is uniform, we can choose $N$ such that 
    $\abs{f(t)-f_N(t)}\leq \epsilon/(2n)$. Thus
    \begin{equation*}
        \sum_{i=0}^{n-1} \abs{f(t_{i+1})-f(t_i)} \leq \epsilon + V(f_N).
    \end{equation*}
    Since $f_N$ is of bounded variation, we see that $f\in BV([a,b])$ 
    as well. Lastly, to show that $\norm{f-f_n}_{TV}\to 0$, 
    first note that by definition we have $\abs{f_n(a)-f(a)}\to 0$. 
    It remains to show that $V(f_n-f)\to 0$. For any $\epsilon>0$, 
    there exists $N$ such that $V_{\mathcal{P}}(f_m-f_n)<\epsilon$ 
    for all $m,n\geq N$ and some partition $\mathcal{P}$. Taking 
    $m\to\infty$, we obtain $V_{\mathcal{P}}(f-f_n)<\epsilon$ for 
    all $n\geq N$. Since the partition is arbitrary, we have 
    $V(f-f_n)<\epsilon$ for all $n\geq N$. Thus $f_n\to f$ in 
    $BV([a,b])$ and $BV([a,b])$ is complete.
\end{proof}

\begin{theorem}\label{thm:M_BV}
    $M([a,b])$ is isometrically isomorphic to $BV([a,b])$. 
\end{theorem}
\begin{proof}
    We define the mapping $\phi:M([a,b])\to BV([a,b])$ by 
    \begin{equation*}
        \rho(t) = \phi\nu(t) = \nu([a,t]).
    \end{equation*}
    First, we show that $\rho\in BV([a,b])$. For any partition 
    $\mathcal{P} = \set{a=t_0<\cdots<t_n=b}$, 
    \begin{equation*}
        \begin{split}
            \sum_{i=0}^{n-1} \abs{\rho(t_{i+1})-\rho(t_i)} + \abs{\rho(a)} 
            &= \sum_{i=0}^{n-1} \abs{\nu([a,t_{i+1}])-\nu([a,t_i])} + \abs{\nu(\set{a})} \\
            &= \sum_{i=0}^{n-1} \abs{\nu((t_i,t_{i+1}])} + \abs{\nu(\set{a})} \\
            &= \sum_{i=0}^{n-1} \abs{\nu}((t_i,t_{i+1}]) + \abs{\nu}(\set{a}) 
            = \abs{\nu}([a,b]) = \norm{\nu}.
        \end{split}
    \end{equation*}
    Since $\nu$ is a finite signed measure, $\rho\in BV([a,b])$. 
    Furthermore, taking supremum over all partitions, we obtain 
    that $\norm{\rho}_{TV} = \norm{\nu}$. It remains to show that 
    $\phi$ is an isomorphism. Suppose $\nu,\mu\in M([a,b])$ and 
    $\phi\nu = \phi\mu$. Then $\nu([a,t]) = \mu([a,t])$ for all 
    $t\in[a,b]$. Since $[a,t]$ generates the Borel $\sigma$-algebra 
    on $[a,b]$, we have $\nu = \mu$. Thus $\phi$ is injective. 
    For surjectivity, let $\rho\in BV([a,b])$. Consider the 
    signed measure $\nu$ defined by $\nu([a,t]) = \rho(t)$ and 
    $\nu(\varnothing) = 0$. Then $\nu$ is a finite signed measure 
    and $\phi\nu = \rho$. The proof is complete.
\end{proof}

\begin{lemma}\label{lem:h_b}
    Let $X$ be a normed vector space and $M\subset X$ be a 
    proper subspace. Suppose $S:M\to\R$ is a bounded linear 
    functional. Then for every $x\in X\setminus M$, there 
    exists a linear $U:M'\to\R$ such that $\norm{U}_{M'\to\R} 
    = \norm{S}_{M\to\R}$, where $M' = M + \R x$.
\end{lemma}
\begin{proof}
    Clearly $M'$ is a subspace; furthermore, $M' = M\bigoplus\R x$ 
    since if $v = w + cx = w' + c'x$ for some $w,w'\in M$ and 
    $c,c'\in\R$, then $(c-c')x = w-w' \in M$. Since $x\not\in M$, 
    this implies that $c = c'$, $w = w'$ and hence the 
    representation is unique. 

    Now we can define $U$ on $M'$ by $U(w+cx) = Sw + c\lambda$ 
    for any $w+cx\in M'$ and some $\lambda\in\R$ to be determined. 
    To make $U$ have the same norm as $U$, we need to find $\lambda$ 
    such that $\abs{Sw+c\lambda}\leq\norm{S}\norm{w+cx}$ holds for all 
    $w\in M$ and $c\in\R$. Clearly if $c=0$, the inequality is 
    already satisfied. For $c\neq 0$, by deviding both sides by 
    $\abs{c}$, we see that the condition is equivalent to 
    $\abs{Sw + \lambda}\leq\norm{S}\norm{w+x}$ for all $w\in M$. Now 
    for any $w,v\in M$, 
    \begin{equation*}
        Sw - Sv = S(w-v) \leq \abs{S(w-v)} \leq \norm{S}\norm{w-v} 
        = \norm{S}\norm{w+x-(v+x)} \leq \norm{S}(\norm{w+x}+\norm{v+x}).
    \end{equation*}
    Thus 
    \begin{equation*}
        Sw - \norm{S}\norm{w+x} \leq Sv + \norm{S}\norm{v+x}.
    \end{equation*} 
    Fix $v$ and taking supremum over all $w\in M$ on the left, 
    \begin{equation*}
        \sup_{w\in M} Sw - \norm{S}\norm{w+x} \leq Sv + \norm{S}\norm{v+x}.
    \end{equation*}
    Taking infimum over all $v\in M$ on the right, 
    \begin{equation*}
        \sup_{w\in M} Sw - \norm{S}\norm{w+x} \leq \inf_{v\in M} Sv + \norm{S}\norm{v+x}.
    \end{equation*}
    Hence there exists $\lambda\in\R$ such that 
    \begin{equation*}
        S(w) - \norm{S}\norm{w+x} \leq -\lambda \leq S(w) + \norm{S}\norm{w+x}
    \end{equation*}
    for all $w\in M$. Picking this $\lambda$, we see that 
    \begin{equation*}
        \abs{Sw + \lambda} \leq \norm{S}\norm{w+x}
    \end{equation*}
    as desired. Thus $U$ is a bounded linear functional on $M'$ 
    with $\norm{U}_{M'\to\R} = \norm{S}_{M\to\R}$. Also, on $M$, 
    $U = S$ and hence $U$ is an extension of $S$.  
\end{proof}

\begin{theorem}[Hahn-Banach]
    Let $X$ be a normed vector space and $M\subset X$ be a 
    subspace. Suppose $S:M\to\R$ is a bounded linear 
    functional on $M$. Then there exists a bounded linear 
    functional $T:X\to\R$ such that $T|_M = S$ and 
    $\norm{T}_{X\to\R} = \norm{S}_{M\to\R}$.
\end{theorem}
\begin{proof}
    The proof relies on Zorn's lemma.\footnote{Zorn's lemma 
    states that if every chain in a partially ordered set has an 
    upper bound, then the set has a maximal element. It is a 
    direct consequence of the axiom of choice.} We start by 
    constructing a partial order space. Let $(P,\preceq)$ be a 
    partial order space with 
    \begin{equation*}
        P = \Set{(U,Y)}{M\subset Y\subset X, 
        Y\text{ is a subspace of }X, 
        U\text{ is a bounded extension of }S\text{ on }V}
    \end{equation*}
    and the partial order: $(U_1,Y_1)\preceq (U_2,Y_2)$ if 
    $Y_1\subset Y_2$ and $U_2$ is a bounded extension of 
    $U_1$ on $Y_2$. Clearly the pair indeed forms a partial 
    order space. We now check the assumptions of Zorn's lemma. 
    Let $C = \Set{(U_\alpha,Y_\alpha)}{\alpha\in A}$ with an 
    arbitrary index set $A$ be a chain in $P$. Put $Y = 
    \cup_{\alpha\in A}Y_\alpha$. We claim that $Y$ is a subspace 
    of $X$. Indeed, for $y_1,y_2\in Y$ and $c_1,c_2\in\R$, there exist
    $\alpha_1,\alpha_2\in A$ such that $y_1\in Y_{\alpha_1}$ and 
    $y_2\in Y_{\alpha_2}$. Since $Y$ is a chain, one of them is 
    a subspace of the other, say $Y_{\alpha_1}$ is a subspace of 
    $Y_{\alpha_2}$. Then $y_1,y_2\in Y_{\alpha_2}$ and hence 
    $c_1y_1+c_2y_2\in Y_2\subset Y$. Thus $Y$ is a subspace. 

    Next we need to define a bounded linear functional $U$ on $Y$ 
    so that $U$ is a bounded extension of $S$ on $Y$. For $y\in Y$, 
    we can find an $\alpha\in A$ such that $y\in Y_\alpha$ and set 
    $U(y) = U_\alpha(y)$. Such $U$ is well-defined since if $\alpha_1$ 
    and $\alpha_2$ are two indices satisfying $y\in Y_{\alpha_1}\cap 
    Y_{\alpha_2}$, then $U_{\alpha_1}(y) = U_{\alpha_2}(y)$ since 
    one of them is an extension of the other. Also, $U$ is linear 
    since $U_\alpha$ is linear for every $\alpha\in A$. Lastly, $U$ 
    is a bounded extension of $U_\alpha$ on $Y$ for any $\alpha\in A$ 
    because every $U_{\alpha'}$ with $(U_{\alpha},Y_{\alpha})\preceq 
    (U_{\alpha'},Y_{\alpha'})$ is a bounded extension of $U_\alpha$. 
    We conclude that $(U,Y)\in P$ is an upper bound of $C$. 

    By Zorn's lemma, there exists a maximal element $(T,Z)\in P$. 
    We claim that $Z = X$. Suppose $Z\subsetneq X$. Then there exists 
    $x\in X\setminus Z$ and also a bounded extension $T'$ of $T$ 
    on $Z+\R x\supsetneq Z$ by \cref{lem:h_b}. But then $(T',Z+\R x)\in P$ 
    and $(T,Z)\preceq (T',Z+\R x)$, contradicting the maximality of 
    $(T,Z)$. Thus $Z = X$ and $T$ is a bounded extension of $S$ on $X$.
\end{proof}

\begin{theorem}[Riesz Representation of {$C([a,b])$}]
    $C([a,b])'\cong BV([a,b])\cong M([a,b])$ isometrically.
\end{theorem}
\begin{proof}
    In \cref{thm:M_BV}, we have shown that $M([a,b])\cong BV([a,b])$. 
    We are going to show this by constructing an isometric 
    isomorphism between $C([a,b])'$ and $BV([a,b])$. 

    Let $X = C([a,b])$ and $\ell\in X'$. $\ell:X\to\R$ is a bounded 
    linear functional. We need to find a $\nu\in M([a,b])$ such that 
    \begin{equation*}
        \ell(f) = \int_{[a,b]} f d\nu
    \end{equation*}
    for $f\in C([a,b])$. Let $Y = B([a,b]) = 
    \Set{f:[a,b]\to\R}{\text{$f$ is bounded}}$. By Hahn-Banach theorem, 
    there exists a bounded linear extension $L:Y\to\R$ of $\ell$. Now if 
    $f = \chi_{[a,t]}\in Y$, then 
    \begin{equation*}
        L(f) = \int_{[a,b]} \chi_{[a,t]} d\nu = \nu([a,t]) = \rho(t).
    \end{equation*}
    We claim that $\rho\in BV([a,b])$. For any partition $\mathcal{P} 
    = \set{a = t_0 < \cdots < t_n = b}$, 
    \begin{equation*}
        \begin{split}
            V_{\mathcal{P}}(\rho) &= \sum_{i=0}^{n-1} \abs{\rho(t_{i+1})-\rho(t_i)} 
            = \sum_{i=0}^{n-1} \abs{L(\chi_{[a,t_{i+1}]})-L(\chi_{[a,t_i]})} \\
            &= \sum_{i=0}^{n-1} L(\chi_{(t_i,t_{i+1}]})s_i = L\pth{\sum_{i=0}^{n-1} \chi_{(t_i,t_{i+1}]}s_i} 
            \leq \norm{L}\norm{\sum_{i=0}^{n-1} \chi_{(t_i,t_{i+1}]}s_i}_{\infty} \leq \norm{L}
        \end{split}
    \end{equation*}
    by letting $s_i = \sgn(\rho(t_{i+1})-\rho(t_i))$. Thus $\rho\in BV([a,b])$ 
    and $\norm{\rho}_{TV}\leq\norm{L} = \norm{\ell}$. To extend to $f\in C([a,b])$ 
    so that 
    \begin{equation*}
        \ell(f) = L(f) = \int_{[a,b]} f d\nu,
    \end{equation*}
    we first note that by our established result, $f = \chi_{[a,t]}\in Y$ 
    holds. By linearity so does simple functions. For $f\in C([a,b])$, 
    consider 
    \begin{equation*}
        h_{\mathcal{P}}(t) = f(a) + \sum_{i=0}^{n-1} f(t_i)\chi_{(t_i,t_{i+1}]}(t).
    \end{equation*}
    Since $L$ is continuous and $h_{\mathcal{P}}\to f$ uniformly as 
    $\norm{\mathcal{P}}\to 0$, we have 
    \begin{equation*}
        L(f) = \lim_{\norm{\mathcal{P}}\to 0} L(h_{\mathcal{P}}) 
        = \int_{a}^{b} fd\rho.
    \end{equation*} 
    $L$ is an extension of $\ell$ and hence  
    \begin{equation*}
        \ell(f) = \int_{a}^{b} fd\rho = f(a)\rho(a) + \int_{a}^{b} fd\rho.
    \end{equation*}
    Finally, we claim that $\norm{\ell}\leq\norm{\rho}_{TV}\leq\norm{L}
    =\norm{\ell}$. Take $f\in X$. 
    \begin{equation*}
        \abs{\ell(f)} = \abs{\int_{a}^{b} fd\rho} \leq \norm{f}_{\infty}\norm{\rho}_{TV} 
        \leq \norm{f}_{\infty}\norm{L} = \norm{\ell}\norm{f}_{\infty}.
    \end{equation*}
    Hence $\norm{\ell}\leq\norm{\rho}_{TV}\leq\norm{L} = \norm{\ell}$. 
    It follows that the mapping $\ell\mapsto\rho$ is isometric. Conversely, 
    if $\rho\in BV([a,b])$, define 
    \begin{equation*}
        \ell_{\rho}(f) = f(a)\rho(a) + \int_{a}^{b} fd\rho.
    \end{equation*} 
    We need to check that $\ell_{\rho}$ is linear and 
    $\norm{\rho}_{TV}\leq\norm{\ell}\leq\norm{\rho}_{TV}$. $\ell_\rho$ 
    has an extension $L_\rho:Y\to\R$. Define 
    $\lambda(t) = L_\rho(\chi_{[a,t]})$. Then $\norm{\rho}_{TV}=\norm{\lambda}
    \leq\norm{L_{\rho}}=\norm{\ell_\rho}$.
\end{proof}
\begin{remark}
    If $\ell\in C([a,b])'$, there exists $\rho\in BV([a,b])$ such that 
    \begin{equation*}
        \ell(f) = \int_{a}^{b} fd\rho;
    \end{equation*}
    if $\rho\in BV([a,b])$, 
    \begin{equation*}
        \ell_{\rho}(f) = f(a)\rho(a) + \int_{a}^{b} fd\rho
    \end{equation*}
    and $\norm{\ell_\rho} = \norm{\rho}_{TV}$.
\end{remark}

\begin{definition}
    Let $X$ be a Banach space and $J:X\to X''$, the canonical mapping 
    defined by $J:x\mapsto(T\mapsto Tx)$ for $T\in X'$. $X$ is called 
    \textbf{reflexive} if $J$ is surjective. 
\end{definition}
\begin{remark}
    Intuitively, $X$ is reflexive meaning that $X\cong X''$. Such canonical 
    mapping is well-defined since $\hat{x} = T\mapsto T(x)$ is indeed a bounded linear 
    functional on $X'$, which we verify here. This is linear because 
    \begin{equation*}
        \hat{x}(cT + S) = (cT + S)(x) = cT(x) + S(x) = c\hat{x}(T) + \hat{x}(S)
    \end{equation*}
    for $c\in\R$ and $S\in X'$. To show that $\hat{x}$ is bounded, we have 
    \begin{equation*}
        \abs{\hat{x}(T)} = \abs{T(x)} \leq \norm{T}\norm{x} = \norm{T}_{X'}\norm{x}_{X}
    \end{equation*}
\end{remark}

\begin{definition}
    A Banach space $X$ is said to be \textbf{uniformly convex} if for 
    all $\epsilon>0$, $x,y\in X$ with $\norm{x-y}\geq \epsilon>0$ and 
    $\norm{x},\norm{y}\leq 1$, we have 
    \begin{equation*}
        \norm{\frac{x+y}{2}} \leq 1-\delta
    \end{equation*}
    for some $\delta>0$ depending on $\epsilon$.
\end{definition}

\begin{theorem}[Clarkson]
    $\L^p(\Omega,\mu)$ is uniformly convex for $1< p <\infty$.
\end{theorem}
\begin{proof}
    Consider the function $\alpha:\R^2\to[0,\infty)$ defined by 
    \begin{equation*}
        \alpha(x) = \frac{\abs{x_1}^p + \abs{x_2}^p}{2} - \abs{\frac{x_1 + x_2}{2}}^p
    \end{equation*}
    for $x = (x_1,x_2)\in\R^2$. Observe that $\alpha(x)\geq 0$ for all
    $x\in\R^2$ and $\alpha(x) = 0$ if and only if $x_1 = x_2$ by the strict 
    convexity of $\abs{\cdot}^p$. Now given $\epsilon>0$, choose $\eta<\epsilon^p/2$. 
    Consider the set $D = \Set{x\in\R^2}{\abs{x_1}^p + \abs{x_2}^p = 1,\, \abs{x_1-x_2}^p\geq \eta}$. 
    Then $D$ is a compact set. By the compactness, we can set 
    $\theta = \inf_{x\in D}\alpha(x) > 0$. We now claim that if $x\in\R^2$ 
    satisfies $\abs{x_1-x_2}^p\geq \eta(\abs{x_1}^p + \abs{x_2}^p)$, then 
    \begin{equation*}
        \abs{x_1}^p + \abs{x_2}^p \leq \frac{\alpha(x)}{\theta}.
    \end{equation*}

    By the assumption, we may assume that $x\neq (0,0)$. Set 
    $t = (\abs{x_1}^p+\abs{x_2}^p)^{1/p}$. Then 
    \begin{equation*}
        \abs{\frac{x_1}{t}}^p + \abs{\frac{x_2}{t}}^p = 1, 
        \quad \text{and} \quad 
        \abs{\frac{x_1}{t}-\frac{x_2}{t}}^p \geq \eta.
    \end{equation*} 
    Thus 
    \begin{equation*}
        \theta \leq \alpha\pth{\frac{x}{t}} = \frac{\alpha(x)}{t^p}
        \Rightarrow \abs{x_1}^p + \abs{x_2}^p = t^p \leq \frac{\alpha(x)}{\theta}.
    \end{equation*}
    The claim follows. 

    Now let $f,g\in\L^p(\Omega,\mu)$ with $\norm{f}_p,\norm{g}_p\leq 1$ 
    and $\norm{f-g}_p\geq \epsilon$. Put 
    \begin{equation*}
        E = \Set{x\in\Omega}{\abs{f(x)-g(x)}^p \geq \eta(\abs{f(x)}^p + \abs{g(x)}^p)}.
    \end{equation*}
    Using the claim and $\alpha(x)\geq 0$,
    \begin{equation*}
        \begin{split}
            \epsilon^p &\leq \int_\Omega \abs{f-g}^pd\mu 
            = \int_E \abs{f-g}^pd\mu + \int_{E^c} \abs{f-g}^pd\mu \\ 
            &\leq 2^p\int_E \abs{\frac{f-g}{2}}^pd\mu + \eta\int_{E^c} \abs{f}^pd\mu + \eta\int_{E^c} \abs{g}^pd\mu \\
            &\leq 2^p\int_E \frac{\abs{f}^p+\abs{g}^p}{2}d\mu + 2\eta \\
            &\leq 2^{p-1}\int_E \frac{\alpha(f,g)}{\theta}d\mu + 2\eta \\
            &\leq \frac{2^{p-1}}{\theta}\int_\Omega \frac{\abs{f}^p + \abs{g}^p}{2} - \abs{\frac{f+g}{2}}^pd\mu + 2\eta 
            \leq \frac{2^{p-1}}{\theta} + 2\eta - \frac{2^{p-1}}{\theta}\norm{\frac{f+g}{2}}_p^p.
        \end{split}
    \end{equation*}
    Hence 
    \begin{equation*}
        \norm{\frac{f+g}{2}}_p^p \leq 1 - \theta\frac{\epsilon^p-2\eta}{2^{p-1}} 
        \Rightarrow \norm{\frac{f+g}{2}}_p \leq 1 - \delta
    \end{equation*}
    for some $\delta>0$. We conclude that $\L^p(\Omega,\mu)$ is uniformly convex 
    for $1<p<\infty$.
\end{proof}

\begin{theorem}[Milman-Pettis]
    Every uniformly convex Banach space is reflexive.
\end{theorem}
\begin{proof}
    
\end{proof}

\begin{corollary}
    $\L^p(\Omega,\mu)$ is reflexive for any $1< p <\infty$.
\end{corollary}
\begin{proof}
    This is a direct consequence of the Clarkson theorem and 
    Milman-Pettis theorem. Since for every $1<p<\infty$, 
    $\L^p(\Omega,\mu)$ is uniformly convex, and every uniformly 
    convex Banach space is reflexive, we conclude that 
    $\L^p(\Omega,\mu)$ is reflexive.
\end{proof}
\begin{remark}
    This corollary can also be inferred from the Riesz 
    representation theorem twice. 
\end{remark}

\begin{example}
    Fix $x\in X$ and define the functional $L_x:X'\to\R$ by 
    $L_x(\ell) = \ell(x)$. Then $L_x$ is a bounded linear functional. 
    To see this, let $c\in\R$ and $\ell_1,\ell_2\in X'$. 
    \begin{equation*}
        L_x(c\ell_1 + \ell_2) = (c\ell_1 + \ell_2)(x) = c\ell_1(x) + \ell_2(x) 
        = cL_x(\ell_1) + L_x(\ell_2).
    \end{equation*} 
    And also
    \begin{equation*}
        \norm{L_x} = \sup_{\norm{\ell}=1}\abs{L_x(\ell)} 
        = \sup_{\norm{\ell}=1}\abs{\ell(x)} \leq \sup_{\norm{\ell}=1}\norm{\ell}\norm{x} 
        = \norm{x}.
    \end{equation*}
    In fact, we have $\norm{L_x} = \norm{x}$. To see this, consider 
    the one-dimensional subspace $Y = \R x$ and the functional 
    $s:Y\to\R$ defined by $s(\lambda x) = \lambda\norm{x}$ for some 
    $\lambda\neq 0$. Then by the Hahn-Banach theorem, there exists a 
    bounded linear functional $s':X\to\R$ such that $s'|_Y = s$ with 
    $\norm{s'}_X = \norm{s}_Y$. Then $s'\in X'$ and 
    \begin{equation*}
        \norm{L_x} \geq \abs{L_x(s')} = \abs{s'(x)} = \norm{x}.
    \end{equation*} 
    Hence $\norm{L_x} = \norm{x}$. 
\end{example}

\begin{proposition}
    If $X$ is a finite-dimensional Banach space, then $X$ is reflexive.
\end{proposition}
\begin{proof}
    Let $X$ be a finite-dimensional Banach space. Then $X$ is 
    isomorphic to $\R^n$ for some $n\in\N$. Since the dual of 
    $\R^n$ is also $\R^n$, we have $X'\cong X$. Thus $X''\cong X'\cong X$. 
    Hence $X$ is reflexive.
\end{proof}

\begin{example}
    Consider the space $C[-1,1]$ with $\norm{f} = \sup_{x\in[-1,1]}\abs{f(x)}$. 
    Then $C[-1,1]$ is a Banach space, but not reflexive. Suppose 
    $C[-1,1]$ is reflexive. Write $(C[-1,1])'' \cong C[-1,1]$. Then 
    the mapping $L\mapsto L_x$ is an isomorphism, where 
    $L:(C[-1,1])'\to\R$, $L\in(C[-1,1])''$. Then, for all $\ell\in C[-1,1]'$, 
    there exists an $x\in C[-1,1]$ and $L_x\in C[-1,1]''$ such that 
    \begin{equation*}
        \begin{cases}
            \norm{L_x} = 1, &\text{ and } \norm{\ell} = L_x(\ell), \\
            \norm{x} = 1, &\text{ and } \norm{\ell} = \ell(x).
        \end{cases}
    \end{equation*}
    
    Now consider the functional 
    \begin{equation*}
        \ell(g) = \int_{-1}^0 g(x)dx - \int_0^1 g(x)dx.
    \end{equation*}
    Clearly $\ell$ is a bounded linear functional on $C[-1,1]$. 
    \begin{equation*}
        \abs{\ell(g)} \leq \int_{-1}^0 \abs{g(x)}dx + \int_0^1 \abs{g(x)}dx 
        \leq 2\sup_{x\in[-1,1]}\abs{g(x)} = 2\norm{g}.
    \end{equation*}
    Thus $\norm{\ell} \leq 2$. In fact, we have $\norm{\ell} = 2$ by considering 
    the continuous functions 
    \begin{equation*}
        g_\epsilon(x) = \begin{cases}
            1, & x\in[-1,-\epsilon], \\
            -\frac{x}{\epsilon}, & x\in[-\epsilon,\epsilon], \\ 
            -1, & x\in[\epsilon,1].
        \end{cases}
    \end{equation*} 
    Then $\norm{g_\epsilon} = 1$ and $\ell(g_\epsilon) = 2-\epsilon\to 2$ 
    as $\epsilon\to 0$. 

    However, if $f\in C[-1,1]$ has $\norm{f} = 1$, then there exists 
    an interval $I\subset[-1,1]$ with $\mu(I)>0$ such that $\sup_I \abs{f}< 1-\delta$, 
    where $\delta>0$. Then 
    \begin{equation*}
        \begin{split}
            \abs{\ell(f)} &= \abs{\int_{[-1,0]-I} f(x)dx + \int_{[-1,0]\cap I} f(x)dx - \int_{[0,1]\cap I} f(x)dx - \int_{[0,1]-I} f(x)dx} \\
            &\leq \mu([-1,0]-I) + \mu([-1,0]\cap I)(1-\delta) + \mu([0,1]\cap I)(1-\delta) + \mu([0,1]-I) \\
            &= 2-\delta\mu(I). 
        \end{split}
    \end{equation*}
    This contradicts the fact that there is an $x\in C[-1,1]$ 
    such that $\norm{x} = 1$ and $\norm{\ell} = \ell(x)$. Thus 
    $C[-1,1]$ is not reflexive.
\end{example}

\begin{theorem}\label{thm:dual_separable}
    Let $X$ be a Banach space. If $X'$ is separable, then $X$ 
    is also separable.
\end{theorem}
\begin{proof}
    By the assumption, let $\set{\ell_n}\subset X'$ be a countable 
    dense subset. By definition, since $\norm{\ell_n} = \sup_{\norm{z}=1}\abs{\ell_n(z)}$, 
    for each $n\in\N$, we can find a $z_n\in X$ such that $\norm{z_n} = 1$ and 
    $\abs{\ell_n(z_n)} \geq \frac{1}{2}\norm{\ell_n}$. 
    
    Now we claim that $Y = \spanby\set{z_n}$ is dense in $X$. 
    Suppose not. Then there is an $x\in X\setminus Y$. Consider 
    the space $W = \Set{cx+y}{c\in\R,\, y\in Y}$. Define a linear 
    functional $\ell(v) = c\ell(x)\neq 0$ for $v = cx+y$ with $c\neq 0$ 
    on $W$. By the Hahn-Banach theorem, we can extend $\ell$ to $X$. 
    For such $\ell$ on $X$, we have $\ell(z_n) = 0$ for all $n$ since 
    $z_n\in Y$. Assume without loss of generality that $\norm{\ell} = 1$ 
    and $\norm{\ell_n-\ell}\leq\epsilon$ by the density of $\set{\ell_n}$ 
    in $X'$. Hence 
    \begin{equation*}
        \norm{\ell_n} \geq \norm{\ell} - \norm{\ell_n-\ell} \geq 1-\epsilon.
    \end{equation*}
    On the other hand, 
    \begin{equation*}
        \norm{\ell_n}\leq 2\abs{\ell_n(z_n)} = 2\abs{\ell_n(z_n) - \ell(z_n)} 
        \leq 2\norm{\ell_n-\ell}\norm{z_n} \leq 2\epsilon.
    \end{equation*}
    This implies that $1\leq 3\epsilon$. Picking $\epsilon<1/3$ leads 
    to a contradiction. Hence $Y$ is dense in $X$. 

    Finally, write $X = \overline{Y} = \overline{\Set{\sum_{k=1}^{M}\alpha_kz_k}{M\geq 1, \alpha_k\in\R}} 
    = \overline{\Set{\sum_{k=1}^{M}\alpha_kz_k}{M\geq 1, \alpha_k\in\Q}}$. 
    Thus $X$ is separable. 
\end{proof}

\begin{theorem}\label{thm:subspace_reflexive}
    Let $X$ be a reflexive Banach space. If $Y\subset X$ is a 
    closed subspace, then $Y$ is reflexive.
\end{theorem}
\begin{proof}
    Fix a bounded lineal functional $L:Y'\to\R$. We want to show 
    that there exists a unique $z\in Y$ such that $L(\ell) = L_z(\ell) 
    = \ell(z)$ for all $\ell\in Y'$. Suppose $\ell:X\to\R$ is a 
    bounded linear functional on $X$. Consider its restriction 
    on $Y$, $\ell|_Y$. Note that $\norm{\ell|_Y} \leq \norm{\ell}$. 

    Now for $L$, we cna extend by Hahn-Banach theorem to $L_0:X'\to\R$. 
    For $m\in X'$, $m:X\to\R$, consider its restriction on $Y$, 
    $m|_Y$. Then $L_0(m) = L(m|_Y)$. We check that $L_0$ is 
    linear and bounded. For $c\in\R$ and $m,\ell\in X'$,
    \begin{equation*}
        L_0(cm + \ell) = L\pth{(cm+\ell)|_Y} = L(cm|_Y + \ell|_Y)
        = cL(m|_Y) + L(\ell|_Y) = cL_0(m) + L_0(\ell).
    \end{equation*}
    And also 
    \begin{equation*}
        \abs{L_0(m)} = \abs{L(m|_Y)} \leq \norm{L}\norm{m|_Y} 
        \leq \norm{L}\norm{m}. \Rightarrow \norm{L_0}\leq\norm{L}.
    \end{equation*}
    Thus $L_0$ is a bounded linear functional on $X'$. We now use 
    the reflexivity of $X$. Since $X''\cong X$, there exists a 
    $z\in X$ such that $L_0(m) = L_z(m)$ for all $m\in X'$. 

    We claim that $z\in Y$. Suppose not. Then there exists a 
    bounded linear functional $m:\Set{cz+y}{c\in\R,\, y\in Y}\to\R$ 
    such that $m(z)\neq 0$ and $m(y) = 0$ for all $y\in Y$. 
    Extend $m$ to $m_0:X\to\R$ by Hahn-Banach theorem. Then 
    \begin{equation*}
        L_0(m_0) = L(m_0|_Y) = L(0) = 0 \neq m_0(z) = L_0(m_0),
    \end{equation*} 
    which is absurd. Hence $z\in Y$ and we see that $L(m) = m(z)$ 
    for all $m\in Y'$. Take $m\in Y'$ and its extension $m_0\in X'$. 
    If $m_0,m_0'\in X'$ are two extensions of $m$, then $L_0(m_0) 
    = L_0(m_0')$ and hence $L(m) = L_0(m_0) = m_0(z) = m(z)$. 
    Thus the extension is unique. We conclude that $Y$ is 
    a reflexive Banach space.
\end{proof}