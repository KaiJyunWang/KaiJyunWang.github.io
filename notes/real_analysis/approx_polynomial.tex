\begin{proposition}
    Let $X$ be a finite-dimensional vector space. Then every norm on 
    $X$ is equivalent.
\end{proposition}
\begin{proof}
    This can be seen as a special case of \cref{prop:equivalent_norm}, as any 
    finite-dimensional vector space is a Banach space. However, we also have a
    simple proof here. 

    Let $\set{e_1,\ldots,e_n}$ be a basis for $X$. For any $x\in X$, we can write 
    $x = \sum_{i=1}^n x_ie_i$. For any norm $\norm{\cdot}$ on $X$, 
    \begin{equation*}
        \norm{x} = \norm{\sum_{i=1}^n x_ie_i} \leq \sum_{i=1}^n \abs{x_i}\norm{e_i} 
        \leq \pth{\max_{1\leq i\leq n}\norm{e_i}}\sum_{i=1}^n\abs{x_i} = C_1\norm{x}_1.
    \end{equation*}
    where $\norm{\cdot}_1$ is the $\ell^1$ norm. Also, this implies that 
    $\norm{\cdot}:X\to\R$ is continuous with respect to the $\ell^1$ norm since 
    \begin{equation*}
        \abs{\norm{x} - \norm{y}} \leq \norm{x-y} \leq C_1\norm{x-y}_1.
    \end{equation*}
    Now for any $x\neq 0$, the function $f(x) = \norm{\frac{x}{\norm{x}_1}}$ is 
    continuous on $S = \Set{x\in X}{\norm{x}_1 = 1}$, which is compact. 
    By extreme value theorem, $f$ attains its minimum on $S$, which leads to 
    \begin{equation*}
        \frac{\norm{x}}{\norm{x}_1} = \norm{\frac{x}{\norm{x}_1}} \geq C_2 > 0
    \end{equation*}
    for some $C_2 > 0$ since $x\neq 0$. Thus $\norm{x} \geq C_2\norm{x}_1$ and 
    the norms are equivalent.
\end{proof}

\begin{remark}
    Every finite-dimensional normed vector space is complete.
\end{remark}

\begin{remark}
    Every closed ball in a finite-dimensional normed vector space is compact.
\end{remark}

\begin{theorem}
    Let $X$ be a Banach space and $Y$ be a finite-dimensional subspace. For any 
    $x\in X$, there exists a $y^*\in Y$ such that 
    \begin{equation*}
        \norm{x-y^*} = \inf_{y\in Y}\norm{x-y}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Since $Y$ is a subspace, $0\in Y\subset X$. Then $\norm{x-y^*}\leq\norm{x}$. 
    Consider the closed ball $B = \Set{y\in Y}{\norm{x-y}\leq \norm{x}}$. Let 
    $f(y) = \norm{x-y}$. Observe that 
    \begin{equation*}
        \abs{f(y) - f(z)} = \abs{\norm{x-y} - \norm{x-z}} \leq \norm{y-z},
    \end{equation*}
    and $f$ is continuous. Since $B$ is compact, $f$ attains its minimum at some 
    point $y^*\in B\subset Y$. Thus $\norm{x-y^*} = \inf_{y\in Y}\norm{x-y}$.
\end{proof}

\begin{proposition}
    Let $X$ be a Banach space and $Y\subset X$ be a finite-dimensional subspace. 
    Suppose that for each $x\in X$, there corresponds a unique $y_x\in Y$ such 
    that $\norm{x-y_x} = \inf_{y\in Y}\norm{x-y}$. Then the map $P:X\to Y$ defined 
    by $P: x\mapsto y_x$ is continuous.
\end{proposition}
\begin{proof}
    Let $x_n\to x$ in $X$. Since 
    \begin{equation*}
        \norm{P(x_n)} = \norm{P(x_n) - x_n + x_n} \leq \norm{P(x_n) - x_n} + \norm{x_n}
        \leq 2\norm{x_n},
    \end{equation*}
    $P(x_n)$ is bounded. By Bolzano-Weierstrass theorem, there is a subsequence 
    $x_{n_k}$ such that $P(x_{n_k})\to P(\hat{x})$ for some $\hat{x}\in X$. It 
    remains to show that $P(x) = P(\hat{x})$. Indeed, 
    \begin{equation*}
        \norm{P(x_{n_k}) - x_{n_k}} \leq \norm{P(x) - x_{n_k}} 
    \end{equation*}
    for all $k$. Letting $k\to\infty$ gives $\norm{P(\hat{x}) - x} \leq \norm{P(x) - x}$. 
    Since the minimizer is unique, $P(x) = P(\hat{x})$ and $x = \hat{x}$. Hence 
    $P$ is continuous.
\end{proof}

\begin{theorem}
    Let $X$ be a Banach space, $Y\subset X$ be a subspace, and $x\in X$. Then the 
    set 
    \begin{equation*}
        Y_x = \Set{y\in Y}{y = \argmin_{y\in Y}\norm{x-y}}
    \end{equation*} 
    is a bounded convex set. 
\end{theorem}
\begin{proof}
    Since $0\in Y$, for all $y\in Y_x$, $\norm{y} \leq \norm{x-y} + \norm{x} \leq 2\norm{x}$. 
    Thus $Y_x$ is bounded. To see the convexity, let $y_1,y_2\in Y_x$ and $t\in[0,1]$. 
    Then $ty_1 + (1-t)y_2\in Y$ and
    \begin{equation*}
        \begin{split}
            \norm{x-\pth{ty_1 + (1-t)y_2}} &= \norm{t(x-y_1) + (1-t)(y_2-x)} \\
            &\leq t\norm{x-y_1} + (1-t)\norm{x-y_2} \\
            &= \norm{x-y_1} = \norm{x-y_2}
        \end{split}
    \end{equation*}
    since $y_1,y_2\in Y_x$. Thus $ty_1 + (1-t)y_2\in Y_x$ and $Y_x$ is convex.
\end{proof}

\begin{definition}
    A Banach space $(X,\norm{\cdot})$ has a strictly convex norm if 
    \begin{equation*}
        \norm{x + y} < \norm{x} + \norm{y}
    \end{equation*}
    for all $x,y\in X$ such that $\alpha x \neq \beta y$ for all $\alpha,\beta\in\R$.
\end{definition}

\begin{remark}
    $\L^p$ spaces are strictly convex for $1<p<\infty$.
\end{remark}

\begin{definition}
    For any bounded function $f:[0,1]\to\R$, the Bernstein polynomial of 
    degree $n$ for $f$ is defined as 
    \begin{equation*}
        B_n(f)(x) = \sum_{k=0}^n f\pth{\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k}.
    \end{equation*}
\end{definition}

\begin{theorem}[Weierstrass]
    Let $f\in C([a,b])$. Then for any $\epsilon>0$, there exists a polynomial 
    $p$ such that $\norm{f-p}_\infty < \epsilon$.
\end{theorem}
\begin{proof}
    First consider the mapping $\sigma: x\mapsto a + (b-a)x$ for $x\in[a,b]$. 
    Then by replacing $f$ with $f\circ\sigma$, we can assume that $a=0$ and $b=1$. 
    
    Now consider the Bernstein polynomial $B_n(f)$. Since $f$ is continuous on 
    $[0,1]$, which is compact, $f$ is uniformly continuous. Thus for any 
    $\epsilon>0$, there exists a $\delta>0$ such that $\abs{x-y}<\delta$ implies 
    $\abs{f(x)-f(y)}<\epsilon$. Let $F = \Set{k\in\set{0,\ldots,n}}{\abs{x - \frac{k}{n}}<\delta}$. 
    We can compute that 
    \begin{equation*}
        \begin{split}
            \abs{B_n(f)(x) - f(x)} &= \abs{f(x) - \sum_{k=0}^n f\pth{\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k}} \\
            &\leq \sum_{k\in F}\abs{f(x) - f\pth{\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} + \sum_{k\notin F}\abs{f(x) - f\pth{\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} \\ 
            &\leq \sum_{k=0}^n \epsilon\binom{n}{k}x^k(1-x)^{n-k} + 2\norm{f}_\infty\sum_{k\notin F}\binom{n}{k}x^k(1-x)^{n-k} \\
            &\leq \epsilon + 2\norm{f}_\infty\sum_{k=0}^n \mathds{1}\set{\abs{x-\frac{k}{n}}\geq\delta}\binom{n}{k}x^k(1-x)^{n-k} \\
            &\leq \epsilon + 2\norm{f}_\infty\sum_{k=0}^n \frac{1}{\delta^2}\pth{x-\frac{k}{n}}^2\binom{n}{k}x^k(1-x)^{n-k} \\ 
            &= \epsilon + 2\frac{\norm{f}_\infty}{\delta^2}\sum_{k=0}^{n}\pth{x^2 - \frac{2k}{n}x + \frac{k^2}{n^2}}\binom{n}{k}x^k(1-x)^{n-k}.
        \end{split}
    \end{equation*}
    Now let 
    \begin{equation*}
        S(x,y) = \sum_{k=0}^{n}\binom{n}{k}x^ky^{n-k} = (x+y)^n.
    \end{equation*}
    Then 
    \begin{align*}
        nx(x+y)^{n-1} &= x\pd{S}{x} = \sum_{k=0}^{n}k\binom{n}{k}x^ky^{n-k}, \\
        n(n-1)x^2(x+y)^{n-2} &= x^2\pdd{S}{x} = \sum_{k=0}^{n}k(k-1)\binom{n}{k}x^ky^{n-k}.
    \end{align*}
    Taking $y = 1-x$ gives 
    \begin{equation*}
        \sum_{k=0}^{n}\pth{x^2 - \frac{2k}{n}x + \frac{k^2}{n^2}}\binom{n}{k}x^k(1-x)^{n-k} 
        = x^2 - \frac{2}{n}x\cdot nx + \frac{1}{n^2}\pth{n(n-1)x^2 + nx} 
        = \frac{x(1-x)}{n}\leq \frac{1}{4n}.
    \end{equation*}
    Hence we obtain the estimate
    \begin{equation*}
        \abs{B_n(f)(x) - f(x)} \leq \epsilon + \frac{\norm{f}_\infty}{2n\delta^2}.
    \end{equation*}
    Letting $n\to\infty$ and by the arbitrariness of $\epsilon$, we see that 
    $B_n(f)\to f$ uniformly.
\end{proof}

\begin{remark}
    An alternative expression for the Weierstrass theorem is that for such $f$, 
    there exists a sequence of polynomials $p_n$ such that $p_n\to f$ uniformly.
\end{remark}

\begin{remark}
    As a direct consequence of the Weierstrass theorem, the polynomial space is
    dense in $C([0,1])$.
\end{remark}

\begin{definition}
    A map $T:C([a,b])\to C([a,b])$ is said to be \textbf{positive} if 
    $T(f)\geq 0$ for all $f\geq 0$.
\end{definition}

\begin{proposition}
    The map $U:C([0,1])\to C([0,1])$ defined by $f\mapsto B_n(f)$ is linear, positive, and continuous.
\end{proposition}
\begin{proof}
    To show the linearity, let $c\in\R$ and $f,g\in C([0,1])$. Then 
    \begin{equation*}
        \begin{split}
            U(cf + g)(x) &= \sum_{k=0}^n \pth{cf\pth{\frac{k}{n}} + g\pth{\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} \\
            &= c\sum_{k=0}^{n}f\pth{\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k} + \sum_{k=0}^{n}g\pth{\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k} 
            = cU(f)(x) + U(g)(x).
        \end{split}
    \end{equation*}

    To show the positivity, let $f\geq 0$. Then $f\pth{\frac{k}{n}}x^k(1-x)^{n-k}\geq 0$ for all $k$ 
    and $x\in[0,1]$. Then the sum is nonnegative and $U(f)\geq 0$. 

    To show the continuity, it is enough to show the boundedness of $U$. 
    \begin{equation*}
        \begin{split}
            \norm{U(f)}_\infty &= \sup_{x\in[0,1]}\abs{\sum_{k=0}^n f\pth{\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k}} \\
            &\leq \sup_{x\in[0,1]}\sum_{k=0}^n \abs{f\pth{\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} \\ 
            &\leq \sup_{x\in[0,1]}\sum_{k=0}^n \norm{f}_\infty\binom{n}{k}x^k(1-x)^{n-k} = \norm{f}_\infty.
        \end{split}
    \end{equation*}
    Hence $U$ is a bounded linear operator and thus continuous.
\end{proof}

\begin{theorem}[Korovkin]
    Let $T_n:C([0,1])\to C([0,1])$ be positive linear maps. Suppose that 
    $T_n(f_i)\to f_i$ uniformly for $i = 0,1,2$ with $f_i(x) = x^i$. Then 
    $T_n(f)\to f$ uniformly for all $f\in C([0,1])$.
\end{theorem}
\begin{proof}
    Let $\epsilon > 0$. Since $f$ is continuous on a compact 
    set, we can assume that it is Lipschitz with constant $L$. Now observe 
    that 
    \begin{equation*}
        \abs{f(x)-f(a)} \leq L\abs{x-a} \leq L\epsilon + L\frac{(x-a)^2}{\epsilon}.
    \end{equation*}
    This can be verify as follows, 
    \begin{equation*}
        \begin{cases*}
            L\abs{x-a} \leq L\epsilon \leq L\epsilon + L\frac{(x-a)^2}{\epsilon}  & if $\abs{x-a} \leq \epsilon$, \\
            L\abs{x-a} \leq L\frac{(x-a)^2}{\epsilon} \leq L\epsilon + L\frac{(x-a)^2}{\epsilon} & if $\abs{x-a} > \epsilon$.
        \end{cases*}
    \end{equation*}
    Next, we apply $T_n$ and note that we have $\abs{T_n(f)} = T_n(\abs{f})$. 
    Then,
    \begin{equation*}
        \begin{split}
            \abs{T_n(f)(x) - f(a)} &\leq \abs{T_n(f)(x) - f(a)T_n(f_0)(x)} + \abs{f(a)T_n(f_0)(x) - f(a)} \\ 
            &= T_n(\abs{f-f(a)})(x) + \abs{f(a)}\abs{T_n(f_0)(x) - f_0} \\
            &\leq L\pth{\epsilon T_n(f_0)(x) + \frac{1}{\epsilon}T_n(f_2 - 2af_1 + a^2f_0)(x)} + \norm{f}_\infty\norm{T_n(f_0) - f_0}_\infty \\
            &\leq L\epsilon(T_n(f_0)(x) - f_0(x)) + L\epsilon f_0(x) + \frac{L}{\epsilon}(T_n(f_2)(x) - f_2(x)) + \frac{L}{\epsilon}f_2(x) \\
            &\qquad - \frac{2aL}{\epsilon}(T_n(f_1)(x) - f_1(x)) - \frac{2aL}{\epsilon}f_1(x) + \frac{a^2L}{\epsilon}(T_n(f_0)(x) - f_0(x)) \\
            &\qquad + \frac{a^2L}{\epsilon}f_0(x) + \norm{f}_\infty\norm{T_n(f_0) - f_0}_\infty \\
            &\leq L\epsilon\norm{T_n(f_0) - f_0}_\infty + \frac{L}{\epsilon}\norm{T_n(f_2) - f_2}_\infty + \frac{2aL}{\epsilon}\norm{T_n(f_1) - f_1}_\infty \\
            &\qquad + \frac{a^2L}{\epsilon}\norm{T_n(f_0) - f_0}_\infty + \norm{f}_\infty\norm{T_n(f_0) - f_0}_\infty + L\epsilon + \frac{L}{\epsilon}(x-a)^2.
        \end{split}
    \end{equation*}
    Now taking $a = x$ and then taking supremum over $x\in[0,1]$ gives
    \begin{equation*}
        \begin{split}
            \norm{T_n(f) - f}_\infty &\leq L\epsilon\norm{T_n(f_0) - f_0}_\infty + \frac{L}{\epsilon}\norm{T_n(f_2) - f_2}_\infty \\
            &\qquad + \frac{2L}{\epsilon}\norm{T_n(f_1) - f_1}_\infty + \frac{L}{\epsilon}\norm{T_n(f_0) - f_0}_\infty + \norm{f}_\infty\norm{T_n(f_0) - f_0}_\infty + L\epsilon.
        \end{split}
    \end{equation*}
    By the assumptions, there is $N$ such that $n>N$ implies that 
    \begin{equation*}
        \norm{T_n(f_i) - f_i}_\infty < \epsilon^2, \quad i = 0,1,2.
    \end{equation*}
    Thus, 
    \begin{equation*}
        \norm{T_n(f) - f}_\infty < L\epsilon^3 + 5L\epsilon + \norm{f}_\infty\epsilon^2.
    \end{equation*}
    Since $\epsilon$ is arbitrary, we obtain that $T_n(f)\to f$ uniformly.
\end{proof}

\begin{example}
    Let $f\in C([0,1])$ and $L_n(f)$ be the polygonal approximation of $f$ with 
    nodes at $k/n$ for $k = 0,\ldots,n$, i.e., 
    \begin{equation*}
        L_n(f)(x) = f\pth{\frac{k}{n}} + n\pth{f\pth{\frac{k+1}{n}} - f\pth{\frac{k}{n}}}\pth{x - \frac{k}{n}}, \quad x\in\sbrc{\frac{k}{n},\frac{k+1}{n}}.
    \end{equation*}
    Now $L_n(1) = 1$, $L_n(x) = x$, and $\norm{L_n(x^2) - x^2}_\infty \leq \max_{0\leq k\leq n-1} \frac{(k+1)^2}{n^2} - \frac{k^2}{n^2} \leq \frac{1}{n}\to 0$. 
    By the Korovkin theorem, we can conclude that $L_n(f)\to f$ uniformly for 
    all $f\in C([0,1])$.
\end{example}

\begin{definition}
    Let $f$ be a bounded function on $[a,b]$. The \textbf{modulus of continuity} 
    of $f$ is defined as 
    \begin{equation*}
        \omega_f(\delta) = \sup_{\substack{\abs{x-y}\leq\delta \\ x,y\in[a,b]}}\abs{f(x) - f(y)}.
    \end{equation*}
\end{definition}

\begin{definition}
    A function $f$ is said to be \textbf{Lipschitz of order $\alpha$} if 
    \begin{equation*}
        \abs{f(x) - f(y)}\leq M\abs{x-y}^\alpha
    \end{equation*}
    for some $M>0$ and all $x,y\in[a,b]$.
\end{definition}

\begin{proposition}
    Let $f$ be a bounded function on $[a,b]$. Then 
    \begin{thmenum}
        \item $\omega_f(\delta_1)\leq \omega_f(\delta_2)$ for all $\delta_1\leq\delta_2$. 
        \item If $f'$ exists and is bounded, then $\omega_f(\delta)\leq M\delta$ for some $M$. 
        \item If $f$ is Lipschitz of order $\alpha$, then $\omega_f(\delta)\leq M\delta^\alpha$ for some $M$ and all $\delta>0$.
    \end{thmenum}
\end{proposition}
\begin{proof}
    For (a), note that we have $\abs{x-y}\leq\delta_1\leq\delta_2$ for all $x,y\in[a,b]$.
    
    For (b), from the mean value theorem, we have that if $\abs{x-y}\leq\delta$, then
    \begin{equation*}
        \abs{f(x) - f(y)} = \abs{f'(c)(x-y)}\leq M\abs{x-y}\leq M\delta
    \end{equation*}
    for some $c\in[a,b]$ and some $M>0$. 
    
    For (c), we have that 
    \begin{equation*}
        \abs{f(x) - f(y)}\leq M\abs{x-y}^\alpha\leq M\delta^\alpha
    \end{equation*}
    for $\abs{x-y}\leq\delta$.
\end{proof}

\begin{lemma}\label{lem:omega}
    Let $f$ be a bounded function on $[a,b]$ and $\delta>0$. Then 
    \begin{thmenum}
        \item $\omega_f(n\delta)\leq n\omega_f(\delta)$ for all $n\in\N$. 
        \item $\omega_f(\lambda\delta)\leq (1+\lambda)\omega_f(\delta)$ for all $\lambda>0$.
    \end{thmenum}
\end{lemma}
\begin{proof}
    For (a), let $x<y$ be such that $\abs{x-y}\leq n\delta$. We can split $[x,y]$ 
    into $n$ intervals of length at most $\delta$, say $[z_0,z_1],\ldots,[z_{n-1},z_n]$. 
    Then $\abs{z_i - z_{i-1}}\leq\delta$ for all $i$ and 
    \begin{equation*}
        \abs{f(x) - f(y)} \leq \sum_{i=1}^n\abs{f(z_i) - f(z_{i-1})} \leq n\omega_f(\delta).
    \end{equation*}

    For (b), let $n\in\N$ be such that $n-1\leq\lambda\leq n$. Then 
    \begin{equation*}
        \omega_f(\lambda\delta) \leq \omega_f(n\delta) \leq n\omega_f(\delta) \leq (1+\lambda)\omega_f(\delta).
    \end{equation*}
\end{proof}

\begin{theorem}
    For any $f\in C([0,1])$, the Bernstein polynomial $B_n(f)$ satisfies 
    \begin{equation*}
        \norm{B_n(f) - f}_\infty \leq \frac{3}{2}\omega_f\pth{\frac{1}{\sqrt{n}}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    By \cref{lem:omega}, setting $\delta = 1/\sqrt{n}$ and 
    $\lambda = \sqrt{n}\abs{x-\frac{k}{n}}$, then 
    \begin{equation*}
        \begin{split}
            \abs{f(x) - B_nf(x)} &\leq \sum_{k=0}^{n}\abs{f(x) - f\pth{\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} 
            \leq \sum_{k=0}^{n}\omega_f\pth{\abs{x-\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} \\
            &\leq \sum_{k=0}^{n}\omega_f\pth{\frac{1}{\sqrt{n}}}\pth{1 + \sqrt{n}\abs{x-\frac{k}{n}}}\binom{n}{k}x^k(1-x)^{n-k} \\
            &\leq \omega_f\pth{\frac{1}{\sqrt{n}}}\brc{1 + \sqrt{n}\sum_{k=0}^{n}\abs{x-\frac{k}{n}}\binom{n}{k}x^k(1-x)^{n-k}} \\
            &\leq \omega_f\pth{\frac{1}{\sqrt{n}}}\brc{1 + \sqrt{n}\pth{\sum_{k=0}^{n}\abs{x-\frac{k}{n}}^2\binom{n}{k}x^k(1-x)^{n-k}}^{1/2}\pth{\sum_{k=0}^{n}\binom{n}{k}x^k(1-x)^{n-k}}^{1/2}} \\
            &\leq \omega_f\pth{\frac{1}{\sqrt{n}}}\brc{1 + \sqrt{n}\frac{1}{2\sqrt{n}}}
            = \frac{3}{2}\omega_f\pth{\frac{1}{\sqrt{n}}}.
        \end{split}
    \end{equation*}
    The fourth inequality follows from the Cauchy-Schwarz inequality.
\end{proof}

\begin{theorem}\label{thm:cpt_approx}
    Let $X$ be a metric space and $Y\subset X$ be a compact subset. Then 
    for any $f\in X$, there exists a $p^*\in Y$ such that $d(f,p^*)\leq d(f,q)$ 
    for all $q\in Y$.
\end{theorem}
\begin{proof}
    Let $d^*$ be the shortest distance from $f$ to $Y$, i.e., $d^* = \inf_{q\in Y}d(f,q)$. 
    Then there exists a sequence $q_n\in Y$ such that $d(f,q_n)\to d^*$. From 
    the compactness of $Y$, there exists a subsequence $q_{n_k}$ such that
    $q_{n_k}\to p^*\in Y$. We claim that $p^*$ is the desired point. Indeed, 
    for any $\epsilon > 0$, there is an $N$ such that $d(f,q_{n_k})\leq d^* + \epsilon$ 
    and $d(q_{n_k},p^*)\leq\epsilon$ for all $k\geq N$. Then
    \begin{equation*}
        d(f,p^*) \leq d(f,q_{n_k}) + d(q_{n_k},p^*) \leq d^* + 2\epsilon.
    \end{equation*}
    Since $\epsilon$ is arbitrary, we have $d(f,p^*)\leq d^*$, 
    which completes the proof.
\end{proof}

\begin{theorem}
    If $X$ is a strictly convex Banach space, and $Y$ is a convex compact
    subset of $X$, then for any $f\in X$, there exists a unique $p^*\in Y$ 
    such that $\norm{f-p^*} = \inf_{q\in Y}\norm{f-q}$.
\end{theorem}
\begin{proof}
    The exsistence of such $p^*$ follows from \cref{thm:cpt_approx}. Denote 
    the shortest distance from $f$ to $Y$ by $d^*$. To show that $p^*$ is unique, 
    suppose that there are two such points $p_1^*$ and $p_2^*$. Then by the 
    convexity of $Y$, $\frac{1}{2}p_1^* + \frac{1}{2}p_2^*\in Y$ and from the 
    strict convexity of $X$, 
    \begin{equation*}
        \norm{f - \frac{1}{2}p_1^* - \frac{1}{2}p_2^*} < \frac{1}{2}\norm{f-p_1^*} + \frac{1}{2}\norm{f-p_2^*} = d^*.
    \end{equation*}
    This contradicts the minimality of $d^*$ and thus $p^*$ is unique.
\end{proof}

\begin{definition}
    A function $g$ on $[a,b]$ satisfies the \textbf{equioscillation condition} of 
    degree $n$ if there are $n+2$ points $a\leq x_0 < x_1 \ldots< x_{n+1}\leq b$ 
    such that $g(x_i) = (-1)^i\norm{g}$ for $i = 0,\ldots,n+1$.
\end{definition}

\begin{theorem}[Chebyshev Equioscillation theorem]
    Let $f\in C[a,b]$ and $p\in P_n$ be the polynomial of degree $n$. Let 
    $r = f - p$. Then $r$ satisfies the equioscillation condition of degree $n$ 
    if and only if $\norm{f-p}_\infty \leq \norm{f-q}_\infty$ for all $q\in P_n$.
\end{theorem}
\begin{proof}
    First assume that $r$ satisfies the equioscillation condition of degree $n$. 
    If $p$ is not the best approximation to $f$ in $P_n$, then there is $q\in P_n$ 
    such that $\norm{f-(p+q)}_\infty < \norm{f-p}_\infty$. This implies that 
    $\norm{r-q}_\infty < \norm{r}_\infty$. By the equioscillation condition, 
    $\abs{r(x_i) - q(x_i)} < \abs{r(x_i)}$ for all $i = 0,\ldots,n+1$. 
    This means that $q$ has the same sign with $r$ at each $x_i$, so $q$ must 
    change sign $n+1$ times. This contradicts the fact that $q\in P_n$.

    Conversely, suppose that $p\in P_n$ is the best approximation to $f$ in $P_n$ 
    in uniform norm. Let $R = \norm{r}_\infty$. Since $r$ is uniform continuous 
    on $[a,b]$, we can split $[a,b]$ into subintervals $[t_i, t_{i+1}]$ such that 
    $\abs{r(x) - r(y)} < R/2$ for all $x,y\in[t_i,t_{i+1}]$. Now observe that if 
    $[t_i,t_{i+1}]$ contains a local extremum of $r$, then $r$ must have same 
    sign in $[t_i,t_{i+1}]$. Denote the intervals by $I_k$ and rearrange them so 
    that $r$ has maximum in $I_1,\ldots,I_{k_1}$ and minimum in 
    $I_{k_1+1},\ldots,I_{k_1+k_2}$. The rest intervals are denumerated by 
    $I_{k_1+k_2+1},\ldots,I_{k_1+k_2+k_3}$. By construction we see that 
    the intervals with extremum points are disjoint. 
    
    We claim that $k_1 + k_2 \geq n+2$. Assume that $k_1 + k_2 \leq n+1$. 
    Consider the polynomial 
    \begin{equation*}
        q(x) = \pm\prod_{i=1}^{k_1 + k_2 - 1} (x - z_i),
    \end{equation*}
    where $z_i$ are the points chosen with $\max I_i < z_i < \min I_{i+1}$ 
    for $i = 1,\ldots,k_1+k_2-1$. Notice that $q(x)\neq 0$ for all $x$ 
    lying in $I_i$ for $i = 1,\ldots,k_1+k_2$. We select the sign of $q$ 
    such that $q$ has the same sign as $r$ in $I_i$ for $i = 1,\ldots,k_1+k_2$. 
    We show that $p + \lambda q$ gives a better approximation than $p$ for 
    some $\lambda > 0$. Let $S = \cup_{i=1}^{k_1+k_2}I_i$ and 
    $N = \cup_{i=k_1+k_2+1}^{k_1+k_2+k_3}I_i$. Then for $x\in S$, 
    \begin{equation*}
        \abs{f(x) - (p(x) + \lambda q(x))} = \abs{r(x) - \lambda q(x)} \leq R - \lambda \min\abs{q(x)} < R.
    \end{equation*}
    And for $x\in N$, 
    \begin{equation*}
        \abs{f(x) - (p(x) + \lambda q(x))} = \abs{r(x) - \lambda q(x)} \leq R + \lambda \max\abs{q(x)} 
        < \frac{R}{2} + \lambda \norm{q}_\infty < R
    \end{equation*}
    by taking $\lambda = \frac{R}{2\norm{q}_\infty}$. This contradicts the 
    assumption that $p$ is the best approximation in $P_n$. Hence $k_1 + k_2 \geq n+2$. 
    Since $p$ lies in $P_n$, $p$ has at most $n$ interior extremum plus 
    the two endpoints; we have $n+2$ extremum points, yielding the equioscillation 
    condition.
\end{proof}
\begin{remark}
    The Chebyshev equioscillation theorem gives us a way to compute the best 
    polynomial approximation to a function in uniform norm. The approach is 
    as follows. Consider the approximation polynomial $p(x) = \sum_{k=0}^{n}a_kx^k$ 
    and the error $h = \norm{f-p}_\infty$. Our goal is to find the coefficients 
    $a_k$ and the error $h$ as well. The Chebyshev equioscillation theorem 
    gives the extremum points $x_0,\ldots,x_{n+1}$ such that $f(x_i) = (-1)^ih$. 
    Then we have the system of equations 
    \begin{equation*}
        \begin{pmatrix}
            1 & x_0 & \cdots & x_0^n & -1\\
            1 & x_1 & \cdots & x_1^n & 1 \\
            \vdots & \vdots & \ddots & \vdots &\vdots \\
            1 & x_{n+1} & \cdots & x_{n+1}^n & (-1)^n
        \end{pmatrix}
        \begin{pmatrix}
            a_0 \\ a_1 \\ \vdots \\ a_n \\ h
        \end{pmatrix}
        =
        \begin{pmatrix}
            f(x_0) \\ f(x_1) \\ \vdots \\ f(x_{n+1})
        \end{pmatrix}.
    \end{equation*}
    Since $x_i$ are unknown, we need to guess a set of $x_i$ and 
    solve the system of equations. The iteration continues until 
    $\norm{f-p}_\infty = h$.
\end{remark}