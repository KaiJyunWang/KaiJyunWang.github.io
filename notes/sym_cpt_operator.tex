\begin{definition}
    $\D(A)\subset\H$ is dense in $\H$. A linear operator $A:\D(A)\to\H$ is 
    said to be \textbf{symmetric} if $\inp{Ax}{y} = \inp{x}{Ay}$ for all 
    $x,y\in\D(A)$.
\end{definition}
\begin{remark}
    Note that the domain $\D(A)$ is dense in $\H$. It follows that by density, 
    the domain can often be extended to $\H$. For simplicity, we consider 
    the domain to be $\H$, but the domain can be any dense subset of $\H$.
\end{remark}

\begin{definition}
    $\lambda\in\mathbb{F}$ is an \textbf{eigenvalue} of a linear operator 
    $A:\H\to\H$ if there exists a non-zero vector $x\in\H$ such that 
    $Ax = \lambda x$. The vector $x$ is called the \textbf{eigenvector} 
    corresponding to the eigenvalue $\lambda$.
\end{definition} 

\begin{proposition}\label{prop:sym_eval}
    Let $A:\H\to\H$ be a symmetric operator. The followings are true. 
    \begin{thmenum}
        \item $\inp{Ax}{x}\in\R$ for all $x\in\H$.
        \item If $\lambda\in\mathbb{F}$ is an eigenvalue of $A$, then $\lambda\in\R$. 
        \item If $\lambda_1,\lambda_2\in\mathbb{F}$ are two distinct eigenvalues 
        with respect to eigenvectors $x_1,x_2\in\H$, then $\inp{x_1}{x_2} = 0$. 
        \item Suppose $\set{x_\alpha}$ is an orthonormal basis of $\H$ with 
        the property that each $x_\alpha$ is an eigenvector of $A$ corresponding 
        to the eigenvalue $\lambda_\alpha$. Then if $\mu\in\mathbb{F}$ is also 
        an eigenvalue of $A$, then $\mu = \lambda_\alpha$ for some $\alpha$.
    \end{thmenum}
\end{proposition}
\begin{proof}
    For (a), $\inp{Ax}{x} = \inp{x}{Ax} = \overline{\inp{Ax}{x}}$. Then 
    $\Im\pth{\inp{Ax}{x}} = 0$ and $\inp{Ax}{x}\in\R$.

    For (b), let $x\in\H$ be the corresponding eigenvector to $\lambda$. Then 
    \begin{equation*}
        \inp{Ax}{x} = \inp{\lambda x}{x} = \lambda\inp{x}{x} = \lambda\norm{x}^2
        \implies \lambda = \frac{\inp{Ax}{x}}{\norm{x}^2}\in\R.
    \end{equation*}

    For (c), by symmetry and (b), we have 
    \begin{equation*}
        \lambda_1\inp{x_1}{x_2} = \inp{Ax_1}{x_2} = \inp{x_1}{Ax_2} 
        = \overline{\lambda_2}\inp{x_1}{x_2} = \lambda_2\inp{x_1}{x_2}.
    \end{equation*}
    Since $\lambda_1\neq\lambda_2$, $\inp{x_1}{x_2} = 0$. 

    For (d), let $\mu\in\mathbb{F}$ be an eigenvalue of $A$ with eigenvector 
    $y\in\H$, $y\neq 0$. We claim that $\mu = \lambda_\alpha$ for some $\alpha$. 
    Suppose not. Then write $y = \sum_{j} c_jx_{\alpha_j}$, where $c_j\in\mathbb{F}$. 
    We see that
    \begin{equation*}
        \norm{y}^2 = \lim_{M\to\infty}\inp{y}{\sum_{j=1}^{M} c_jx_{\alpha_j}} 
        = \lim_{M\to\infty} \overline{c_j}\inp{y}{x_{\alpha_j}} = 0
    \end{equation*}
    by (c), but this is a contradiction since $y\neq 0$. Thus $\mu = \lambda_\alpha$ 
    for some $\alpha$.
\end{proof}

\begin{definition}
    A linear operator $A:\H\to\H$ is called \textbf{bounded} if 
    \begin{equation*}
        \norm{A} = \sup_{\norm{x}=1}\norm{Ax} < \infty.
    \end{equation*}
\end{definition}

\begin{proposition}\label{prop:sym_norm}
    $A:\H\to\H$ is a symmetric bounded linear operator. Then 
    \begin{equation*}
        \norm{A} = \sup_{\norm{x} = 1} \abs{\inp{Ax}{x}}.
    \end{equation*}
\end{proposition}
\begin{proof}
    Assume $\norm{x} = 1$. By Cauchy-Schwarz inequality, $\abs{\inp{Ax}{x}} 
    \leq \norm{Ax}\norm{x} = \norm{Ax}$. Taking supremum, 
    \begin{equation*}
        \sup_{\norm{x} = 1}\abs{\inp{Ax}{x}} \leq \sup_{\norm{x} = 1}\norm{Ax} 
        = \norm{A}.
    \end{equation*}
    To see the reverse inequality, note that $\norm{Ax}^2 = \inp{Ax}{Ax} 
    = \inp{A^2x}{x}$. For any nonzero $\lambda\in\R$, define $x^+ 
    = \lambda x + \frac{1}{\lambda}Ax$ and $x^- = \lambda x - \frac{1}{\lambda}Ax$. 
    Then $x = \frac{1}{2\lambda}(x^++x^-)$ and $Ax = \frac{\lambda}{2}(x^+-x^-)$. 
    Now, 
    \begin{equation*}
        \begin{split}
            \inp{A^2x}{x} &= \inp{A\pth{\frac{\lambda}{2}(x^+-x^-)}}{\frac{1}{2\lambda}(x^++x^-)} \\
            &= \frac{1}{4}\inp{Ax^+-Ax^-}{x^++x^-} \\ 
            &= \frac{1}{4}\pth{\inp{Ax^+}{x^+} + \inp{Ax^+}{x^-} - \inp{Ax^-}{x^+} - \inp{Ax^-}{x^-}} 
        \end{split}
    \end{equation*}
    Notice that $\inp{A^2x}{x}$, $\inp{Ax^+}{x^+}$ and $\inp{Ax^-}{x^-}$ are 
    real numbers by \cref{prop:sym_eval}; hence 
    $\Im\pth{\inp{Ax^+}{x^-} - \inp{Ax^-}{x^+}} = 0$. Also, $\inp{Ax^-}{x^+} 
    = \overline{\inp{x^+}{Ax^-}} = \overline{\inp{Ax^+}{x^-}}$. We have 
    $\Im\pth{\inp{Ax^+}{x^-}} = 0$ and $\inp{Ax^+}{x^-} - \inp{Ax^-}{x^+} = 0$. 
    Thus, letting $C = \sup_{\norm{x} = 1}\abs{\inp{Ax}{x}}$, 
    \begin{equation*}
        \begin{split}
            \inp{A^2x}{x} &= \frac{1}{4}\pth{\inp{Ax^+}{x^+} + \inp{Ax^-}{x^-}} \\
            &\leq \frac{1}{4}C\pth{\norm{x^+}^2 + \norm{x^-}^2} \\
            &= \frac{1}{4}C\pth{\inp{x^+}{x^+} + \inp{x^-}{x^-}} \\
            &= \frac{1}{4}C\pth{2\lambda^2\norm{x^2} + \frac{2}{\lambda^2}\norm{Ax}^2}
            = \frac{1}{2}C\pth{\lambda^2\norm{x}^2 + \frac{1}{\lambda^2}\norm{Ax}^2}.
        \end{split}
    \end{equation*}
    Notice that for $a,b\in\R$, $(a-b)^2\geq 0$ and thus $a^2 + b^2\geq 2ab$. Hence 
    \begin{equation*}
        \lambda^2\norm{x}^2 + \frac{1}{\lambda^2}\norm{Ax}^2 \geq 2\lambda\norm{x}\frac{1}{\lambda}\norm{Ax} 
        = 2\norm{Ax}\norm{x}.
    \end{equation*}
    We see that 
    \begin{equation*}
        \norm{Ax}^2 = \inp{A^2x}{x} \leq \frac{1}{2}C\inf_{\lambda\neq 0} \lambda^2\norm{x}^2 + \frac{1}{\lambda^2}\norm{Ax}^2 
        \leq C\norm{Ax}\norm{x}.
    \end{equation*}
    Clearly if $Ax=0$ the inequality holds. Suppose $\norm{Ax} \neq 0$. Then 
    deviding both sides by $\norm{Ax}$ and taking supremum gives 
    \begin{equation*}
        \norm{A} = \sup_{\norm{x} = 1}\norm{Ax} \leq C\sup_{\norm{x} = 1}\norm{x} 
        = C = \sup_{\norm{x} = 1}\abs{\inp{Ax}{x}}.
    \end{equation*}
    We conclude that $\norm{A} = \sup_{\norm{x} = 1} \abs{\inp{Ax}{x}}$.
\end{proof}

\begin{definition}
    $X$ and $Y$ are normed spaces. $M\subset X$. $A:M\to Y$ is an operator. We 
    say that $A$ is \textbf{compact} if $A$ is continuous and for every bounded 
    sequence $x_n\in M$, the sequence $Ax_n\in Y$ has a convergent subsequence.
\end{definition}
\begin{remark}
    A compact operator $A$ transfers bounded sets in $X$ to relatively compact 
    sets in $Y$.
\end{remark}

\begin{example}
    Consider the integral operator $A:C([0,1])\to C([0,1])$ equipped with the 
    supremum norms. Define 
    \begin{equation*}
        Au(x) = \int_0^1 K(x,y)f(y)dy,
    \end{equation*}
    where $K\in C([0,1]^2)$. We verify that $A$ is well-defined, i.e., 
    $Au\in C([0,1])$. Let $x_n\to x\in [0,1]$. 
    \begin{equation*}
        \begin{split}
            \abs{Au(x_n) - Au(x)} &= \abs{\int_0^1 K(x_n,y)u(y)dy - \int_0^1 K(x,y)u(y)dy} \\
            &\leq \int_0^1 \abs{K(x_n,y) - K(x,y)}\abs{u(y)}dy 
            \leq \norm{K(x_n,\cdot) - K(x,\cdot)}_\infty\norm{u}_\infty.
        \end{split}
    \end{equation*}
    Since $K$ is continuous, $(x_n,y)\to (x,y)$ implies $K(x_n,y)\to K(x,y)$. 
    It follows that $Au(x_n)\to Au(x)$. Hence $Au\in C([0,1])$.

    We claim that $A$ is compact. Let $\set{u_n}$ be 
    a bounded sequence in $C([0,1])$. By Arzel\`a-Ascoli theorem, it suffices 
    to show that $\set{Au_n}$ is bounded and uniformly equicontinuous. To see 
    the boundedness, note that by assumption we have $\norm{u_n}_\infty\leq M$ 
    for all $n$. Also, since $K$ is continuous on a compact set, we have 
    $K(x,y)\leq C$ for all $x,y\in [0,1]$. Then 
    \begin{equation*}
        \begin{split}
            \norm{Au_n}_\infty &= \sup_{x\in [0,1]}\abs{\int_0^1 K(x,y)u_n(y)dy} \\
            &\leq \sup_{x\in [0,1]}\int_0^1 \abs{K(x,y)}\abs{u_n(y)}dy \\
            &\leq \sup_{x\in [0,1]}\int_0^1 C\norm{u_n}_\infty dy = C\norm{u_n}_\infty \leq CM.
        \end{split}
    \end{equation*} 
    Thus $\set{Au_n}$ is bounded. To see the uniform equicontinuity, let 
    $\epsilon>0$ be given. By continuity of $K$, we can find $\delta>0$ such that 
    whenever $\abs{x-z}<\delta$, $\abs{K(x,y) - K(z,y)}<\epsilon/M$ for all $y\in [0,1]$. 
    Then for each $n\in\N$, 
    \begin{equation*}
        \begin{split}
            \abs{Au_n(x) - Au_n(z)} &= \abs{\int_0^1 K(x,y)u_n(y)dy - \int_0^1 K(z,y)u_n(y)dy} \\
            &\leq \int_0^1 \abs{K(x,y) - K(z,y)}\abs{u_n(y)}dy 
            \leq \frac{\epsilon}{M}\norm{u_n}_\infty \leq \epsilon.
        \end{split}
    \end{equation*}
    Thus $\set{Au_n}$ is uniformly equicontinuous. By Arzel\`a-Ascoli theorem, 
    $\set{Au_n}$ has a convergent subsequence, i.e., $A$ is compact.
\end{example}

\begin{definition}
    Let $A:\H\to\H$ be a linear operator. If $\lambda\in\mathbb{F}$ 
    is an eigenvalue of $A$, then the corresponding \textbf{eigenspace} 
    is defined as 
    \begin{equation*}
        E_\lambda = \set{x\in\H: Ax = \lambda x}.
    \end{equation*}
\end{definition}
\begin{remark}
    Clearly $E_\lambda$ is a subspace of $\H$.
\end{remark}

\begin{theorem}[Spectral Theorem for Compact Symmetric Operators]
    Let $\H$ be a separable Hilbert space. Suppose that $A:\H\to\H$ is a symmetric 
    compact linear operator. Then the followings are true.
    \begin{thmenum}
        \item There exists an at most countable orthobormal basis $\set{x_j}$, in 
        which each $x_j$ is an eigenvector of $A$ corresponding to an eigenvalue $\lambda_j$. 
        \item If $\lambda_i\neq\lambda_j$, then $\inp{x_i}{x_j} = 0$.
        \item For any $\lambda\neq 0$, $\dim(E_\lambda)<\infty$. 
        \item If $\dim(\H) = \infty$, then either $\lambda_j\to 0$ or there is only 
        finitely many $\lambda_j\neq 0$.
    \end{thmenum}
\end{theorem}
\begin{proof}
    First note that if $\H = \set{0}$, then the statements are vacuously true. 
    We assume that $\H\neq\set{0}$. Assume first that $\dim(\H) = \infty$ and 
    $\ker(A) = \set{0}$. By the assumptions we can find $x\in\H$ such that 
    $\norm{Ax}>0$ and thus $\norm{A} > 0$. By \cref{prop:sym_norm}, 
    $\norm{A} = \sup_{\norm{x} = 1}\abs{\inp{Ax}{x}}$. Hence there exists 
    a sequence $z_n\in\H$ such that $\abs{\inp{Az_n}{z_n}}\to\norm{A}$ 
    with $\norm{z_n} = 1$. Let $\lambda_1\in\R$ satisfying that $\lambda_1 
    = \sgn(\inp{Az_n}{z_n})\norm{A}$ for $n$ greater than some $N$ so that the 
    sign of $\inp{Az_n}{z_n}$ does not alternate. Now notice that 
    \begin{equation*}
        \begin{split}
            0 &\leq \norm{\lambda_1z_n-Az_n}^2 \\
            &= \abs{\lambda_1}^2\norm{z_n}^2 + \norm{Az_n}^2 - 2\abs{\lambda_1}\inp{Az_n}{z_n} \\
            &\leq 2\abs{\lambda_1}^2 - 2\lambda_1\inp{Az_n}{z_n} 
            \leq 2\abs{\lambda_1}^2 - 2\abs{\lambda_1}\abs{\inp{Az_n}{z_n}} \to 0.
        \end{split}
    \end{equation*}
    Hence $\lambda_1z_n - Az_n\to 0$. Since $A$ is compact, $\set{Az_n}$ has a 
    convergent subsequence, say $A(z_{n_k})\to y$. Then $\lambda_1z_{n_k}\to \lambda_1x_1$ 
    for some $x_1\in\H$ with $Ax_1 = y$ and then $z_{n_k}\to x_1$. Since $A$ 
    is continuous, $A(z_{n_k})\to Ax_1$ implies that
    \begin{equation*}
        Ax_1 = \lim_{k\to\infty} A(z_{n_k}) = \lim_{k\to\infty} \lambda_1z_{n_k} = \lambda_1x_1.
    \end{equation*}
    Note that $\norm{z_{n_k}} = 1$ and thus $\norm{x_1} = 1$. We have shown that
    there exists an eigenvector $x_1$ corresponding to an eigenvalue $\lambda_1$, 
    with $\norm{x_1} = 1$ and $\abs{\lambda_1} = \norm{A}$. 

    Next, define $W_1 = \spanby(\set{x_1})$ and $W_1^\perp = \Set{y\in\H}{\inp{y}{x_1} = 0}$. 
    Consider $A_1 = A\restrict{W_1^\perp}$. We verify that $A_1:W_1^\perp\to W_1^\perp$ 
    is well-defined. For any $y\in W_1^\perp$, 
    \begin{equation*}
        \inp{A_1y}{x_1} = \inp{Ay}{x_1} = \inp{y}{Ax_1} = \inp{y}{\lambda_1x_1} = \lambda_1\inp{y}{x_1} = 0.
    \end{equation*}
    Hence $A_1y\in W_1^\perp$. Observe that $A_1$ is also symmetric since for every 
    $y_1,y_2\in W_1^\perp$, 
    \begin{equation*}
        \inp{A_1y_1}{y_2} = \inp{Ay_1}{y_2} = \inp{y_1}{Ay_2} = \inp{y_1}{A_1y_2}.
    \end{equation*}
    We show that $A_1$ is compact. Suppose $y_n\in w_1^\perp$ and $y_n\to y\in W_1^\perp$. 
    Then $A_1y_n = Ay_n\to Ay = A_1y$ by the continuity of $A$. Also, if $\set{y_n}$ 
    is a bounded sequence in $W_1^\perp$, then $\set{A_1y_n} = \set{Ay_n}\subset W_1^\perp$ 
    has a convergent subsequence. Since $W_1$ is finite-dimensional, $W_1$ is itself 
    closed and thus so does $W_1^\perp$ by \cref{prop:orthogonal_complement}. It follows 
    that the subsequence converges in $W_1^\perp$. Hence $A_1$ is compact. 

    Now by similar argument as above, we can find $x_2\in W_1^\perp$ such that 
    $\norm{x_2} = 1$, $\abs{\lambda_2} = \norm{A_1}\leq \norm{A} = \abs{\lambda_1}$ 
    and $Ax_2 = A_1x_2 = \lambda_2x_2$ for some $\lambda_2\in\R$. Continue the 
    process. We obtain a sequence $\set{x_j}$ such that each $x_j$ is an eigenvector 
    of $A$ corresponding to an eigenvalue $\lambda_j$ with $\abs{\lambda_j}\leq \abs{\lambda_{j-1}}$ 
    and $\norm{x_j} = 1$. Furthermore, observe that $\inp{x_i}{x_j} = 0$ for all $i\neq j$ 
    and $\abs{\lambda_{i+1}} = \norm{A_i}$.
    
    We verify (d) first. Notice that $\abs{\lambda_j}$ decreases and bounded below by $0$. 
    Hence $\abs{\lambda_j}\to\alpha$. By the compactness of $A$, there exists a subsequence 
    $x_{n_i}$ such that $Ax_{n_i}$ converges. Thus $Ax_{n_i}$ is Cauchy and 
    \begin{equation*}
        \norm{Ax_{n_i} - Ax_{n_j}}^2 = \norm{\lambda_{n_i}x_{n_i} - \lambda_{n_j}x_{n_j}}^2 
        = \abs{\lambda_{n_i}}^2 + \abs{\lambda_{n_j}}^2.
    \end{equation*}
    Note that the left hand side converges to $0$ and the right hand side converges 
    to $2\alpha^2$. Hence $\alpha = 0$. 

    Next, we show that $\dim(E_\lambda) < \infty$ for $\lambda\neq 0$. Suppose 
    not. Then we can find a countable orthonormal basis $\set{x_i}$ of $E_\lambda$ 
    with each $x_i$ corresponding to $\lambda$. By the compactness of $A$, there 
    exists a subsequence $x_{n_i}$ such that $Ax_{n_i}$ converges and thus Cauchy. 
    \begin{equation*}
        2\abs{\lambda}^2 = \abs{\lambda}^2\norm{x_{n_i}}^2 + \abs{\lambda}^2\norm{x_{n_j}}^2 
        = \norm{\lambda x_{n_i} - \lambda x_{n_j}}^2 = \norm{Ax_{n_i} - Ax_{n_j}}^2 \to 0.
    \end{equation*}
    Hence $\abs{\lambda} = 0$, a contradiction. Thus $\dim(E_\lambda) < \infty$. 

    Lastly, we show that $\overline{\spanby\pth{\set{x_j}}} = \H$. For every $x\in\H$, 
    consider the partial sum $z_n = \sum_{i=1}^{n}\inp{x}{x_i}x_i$. We want to 
    show that $z_n\to x$. $Az_n = \sum_{i=1}^{n}\lambda_i\inp{x}{x_i}x_i$. 
    Notice that 
    \begin{equation*}
        \inp{x-z_n}{x_j} = \inp{x - \sum_{i=1}^{n}\inp{x}{x_i}x_i}{x_j} 
        = \inp{x}{x_j} - \inp{x}{x_j} = 0.
    \end{equation*}
    Hence $x-z_n\in W_n^\perp$. Thus since $\norm{x-z_n}$ is bounded,
    \begin{equation*}
        \norm{Ax-Az_n} = \norm{A(x-z_n)} = \norm{A_n(x-z_n)} 
        \leq \norm{A_n}\norm{x-z_n} = \abs{\lambda_{n+1}}\norm{x-z_n}\to 0
    \end{equation*}
    Hence $Az_n\to Ax$ and thus we can write $Ax = \sum_j\lambda_j\inp{x}{x_j}x_j$. 
    If $y = \sum_j\inp{x}{x_j}x_j$, then $Ay = \sum_j\lambda_j\inp{x}{x_j}x_j = Ax$. 
    Because $A$ has zero kernel, $x = y = \sum_j\inp{x}{x_j}x_j$. Thus we have 
    $\overline{\spanby\pth{\set{x_j}}} = \H$. 

    Now we drop the assumption that $\ker(A) = \set{0}$. Note that $\ker(A)$ is a 
    closed subspace of $\H$ since if $x_n\in\ker(A)$ and $x_n\to x\in\H$, by continuity 
    of $A$, 
    \begin{equation*}
        Ax = \lim_{n\to\infty} Ax_n = 0
        \implies x\in\ker(A).
    \end{equation*} 
    It follows that by \cref{prop:orthogonal_complement}, $\H = \ker(A)\oplus\ker(A)^\perp$. 
    For $\ker(A)$, we apply \cref{thm:orthonormal_basis_exist} to find an orthonormal 
    basis $\set{w_k}$ of $\ker(A)$. Note that since $\set{w_k}\subset \ker(A)$, each 
    $w_k$ is an eigenvector of $A$ corresponding to the eigenvalue $0$. Also, 
    define $A^\perp:\ker(A)^\perp\to\ker(A)^\perp$ by $A^\perp y = Ay$. We verify 
    that such definition is well-defined, i.e., $A^\perp y\in\ker(A)^\perp$. For 
    each $y\in\ker(A)^\perp$,
    \begin{equation*}
        \inp{A^\perp y}{w} = \inp{Ay}{w} = \inp{y}{Aw} = 0
    \end{equation*}
    for all $w\in\ker(A)$. Hence $A^\perp y\in\ker(A)^\perp$. Also, $A^\perp$ inherits 
    the compactness and symmetry of $A$. By the previous argument, we can find an 
    orthonormal basis $\set{x_j}$ of $\ker(A)^\perp$ such that each $x_j$ is an 
    eigenvector of $A^\perp$ and thus $A$, corresponding to an eigenvalue $\lambda_j$. 
    Notice that $\inp{w_k}{x_j} = 0$. Thus $\set{w_k}\cup\set{x_j}$ forms the desired 
    orthonormal basis of $\H$. 

    Finally, if $\dim(\H) = \infty$, then (c) and (d) are vacuously true. 
    (a) follows by applying the above construction with the process 
    being terminated in finite steps. Once (a) is established, (b) follows 
    by observing that $x_i$ and $x_j$ are distinct eigenvectors in the orthonormal 
    basis and thus must be orthogonal.
\end{proof}

\begin{definition}
    Let $\H$ be a separable Hilbert space and $A:\H\to\H$ be a symmetric compact 
    linear operator. Then for each $x\in\H$, we can find $\set{x_j}$ and $\set{w_k}$ 
    are orthonormal bases for $\ker(A)^\perp$ and $\ker(A)$ respectively such that 
    \begin{equation*}
        x = \sum_j\inp{x}{x_j}x_j + \sum_k\inp{x}{w_k}w_k.
    \end{equation*}
    Such decomposition is called the \textbf{spectral decomposition} of $A$.
\end{definition}

\begin{theorem}[Fredholm Alternative]
    Let $\H$ be separable. Suppose $A:\H\to\H$ is a symmetric compact 
    linear operator, $\lambda\neq 0$. Let $N_\lambda = \Set{x\in\H}{Ax = \lambda x}$. 
    Then the equation 
    \begin{equation*}
        \lambda x - Ax = z
    \end{equation*}
    has a solution if and only if $z\in N_\lambda^\perp$. Furthermore, if 
    $\lambda$ is not an eigenvalue of $A$, then the solution is unique.
\end{theorem}
\begin{proof}
    Consider the orthonormal eigenbasis $\set{x_j}\bigcup\set{w_k}$ of $A$ with nonzero 
    eigenvalues $\lambda_j$ for $x_j$ and zeros for $w_k$. Suppose first that 
    $\lambda\neq\lambda_j$ for all $j$. This is equivalent to that $N_\lambda = \set{0}$ and 
    $N_\lambda^\perp = \H$. For every $z\in\H$, by setting 
    \begin{equation*}
        x = \sum_j\frac{1}{\lambda-\lambda_j}\inp{z}{x_j}x_j + \sum_k\frac{1}{\lambda}\inp{z}{w_k}w_k,
    \end{equation*} 
    we see that 
    \begin{equation*}
        \begin{split}
            \lambda x - Ax 
            &= \lambda\sum_j\frac{1}{\lambda-\lambda_j}\inp{z}{x_j}x_j + \lambda\sum_k\frac{1}{\lambda}\inp{z}{w_k}w_k - \sum_j\frac{\lambda_j}{\lambda-\lambda_j}\inp{z}{x_j}x_j \\
            &= \sum_j\inp{z}{x_j}x_j + \sum_k\inp{z}{w_k}w_k = z.
        \end{split}
    \end{equation*}
    We verify that such $x$ indeed belongs to $\H$. 
    \begin{equation*}
        \norm{x}^2 = \sum_j\abs{\frac{1}{\lambda-\lambda_j}\inp{z}{x_j}}^2 + \sum_k\abs{\frac{1}{\lambda}\inp{z}{w_k}}^2 
        \leq \sup_j\frac{1}{\abs{\lambda-\lambda_j}^2}\norm{z}^2 + \sup_k\frac{1}{\abs{\lambda}^2}\norm{z}^2 < C_\lambda\norm{z}^2
    \end{equation*}
    for some $C_\lambda$ by the Parseval's identity. Since $\lambda\neq 0$, $C_\lambda$ is 
    finite and thus $x\in\H$. To check the uniqueness, it suffices to show that the 
    homogeneous equation $\lambda x - Ax = 0$ implies $x = 0$. Indeed, if $x\neq 0$ satisfies 
    $\lambda x - Ax = 0$, then $\lambda$ becomes an eigenvalue of $A$ with eigenvector $x$, 
    which contradicts to our assumption. Hence the solution is unique. The converse is trvial 
    since $N_\lambda^\perp = \H$.

    Now suppose that $\lambda = \lambda_{j}$ for some $j$, say $j=1$. Then $N_\lambda = E_{\lambda_1}$. 
    If $z\in E_{\lambda_1}^\perp$, then since $\dim(E_{\lambda_1})<\infty$ by the spectral theorem, 
    $E_{\lambda_1}$ is a closed subspace of $\H$ and hence $E_{\lambda_1}^\perp$ by 
    \cref{prop:orthogonal_complement}. Thus for such $z$, we can write $z = 
    \sum_{j:\lambda_j\neq\lambda_1} \inp{z}{x_j}x_j + \sum_k\inp{z}{w_k}w_k$. 
    Set 
    \begin{equation*}
        x = \sum_{j:\lambda_j\neq\lambda_1}\frac{1}{\lambda-\lambda_j}\inp{z}{x_j}x_j + \sum_k\frac{1}{\lambda}\inp{z}{w_k}w_k.
    \end{equation*}
    Then 
    \begin{equation*}
        \begin{split}
            \lambda x - Ax 
            &= \lambda\sum_{j:\lambda_j\neq\lambda_1}\frac{1}{\lambda-\lambda_j}\inp{z}{x_j}x_j + \lambda\sum_k\frac{1}{\lambda}\inp{z}{w_k}w_k - \sum_{j:\lambda_j\neq\lambda_1}\frac{\lambda_j}{\lambda-\lambda_j}\inp{z}{x_j}x_j \\
            &= \sum_{j:\lambda_j\neq\lambda_1}\inp{z}{x_j}x_j + \sum_k\inp{z}{w_k}w_k = z.
        \end{split}
    \end{equation*}
    We verify that such $x$ indeed belongs to $\H$. By exactly the same argument as above, 
    \begin{equation*}
        \norm{x}^2 \leq \sup_{j:\lambda_j\neq\lambda}\frac{1}{\abs{\lambda-\lambda_j}^2}\norm{z}^2 + \sup_k\frac{1}{\abs{\lambda}^2}\norm{z}^2 < C_\lambda\norm{z}^2.
    \end{equation*}
    Since by the spectral theorem we have $\lambda_j\to 0$ as $j\to\infty$, $C_\lambda<\infty$.
    We conclude that the equation $\lambda x - Ax = z$ has a solution if $z\in N_\lambda^\perp$. 
    Conversely, if $x$ is a solution, then for every $x_{j_i}$ with $\lambda_{j_i} = \lambda_1 = \lambda$, 
    \begin{equation*}
        \begin{split}
            \inp{z}{x_{j_i}} &= \inp{\lambda x - Ax}{x_{j_i}} = \inp{\lambda x}{x_{j_i}} - \inp{Ax}{x_{j_i}} \\
            &= \lambda\inp{x}{x_{j_i}} - \inp{x}{Ax_{j_i}} = 0.
            = \lambda\inp{x}{x_{j_i}} - \lambda_{j_i}\inp{x}{x_{j_i}} = 0.
        \end{split}
    \end{equation*}
    We see that $z\in N_\lambda^\perp$. This completes the proof.
\end{proof}
\begin{remark}
    If $\lambda$ is an eigenvalue of $A$, we can actually find infinitely 
    many solutions. Since every eigenvector $x$ corresponding to $\lambda$ 
    would become a homogeneous solution, for any solution $x_0$ such that 
    $\lambda x_0 - Ax_0 = z$, $x_0 + x$ forms another solution for any 
    $x\in E_\lambda$. In other words, the set of solutions is $x_0 + E_\lambda$.
\end{remark}