\begin{definition}
    Let $(\Omega,\F,\P)$ be a probability space. Suppose $\F_\beta\subset\F$, 
    $\beta\in B$ are a collection of sub-$\sigma$-algebras. Then $\set{\F_\beta}$ 
    are \textbf{independent} if for all finite $\set{\F_i}_{i=1}^n\subset\set{\F_\beta}$, 
    \begin{equation*}
        \P(\cap_{i=1}^n A_i) = \prod_{i=1}^{n}\P(A_i) 
    \end{equation*}
    where $A_i\in\F_i$. 
\end{definition}

\begin{definition}
    A collection of random variables $\Set{X_\beta}{\beta\in B}$ on $(\Omega,\F,\P)$ 
    is \textbf{independent} if the collection of the generating $\sigma$-algebras 
    $\Set{\sigma(X_\beta)}{\beta\in B}$ is. 
\end{definition}
\begin{remark}
    In other words, 
    \begin{equation*}
        \P(\cap_i \set{X_{\beta_i}\in A_i}) = \prod_i \P(X_{\beta_i}\in A). 
    \end{equation*}
    Note that these random variables can map into different measurable space. 
\end{remark}

\begin{definition}
    A collection of events $\S$ is \textbf{independent} if $\Set{\one_A}{A\in\S}$ is. 
\end{definition}

\begin{proposition}
    Let $X_1,\dots,X_n$ be independent random variables and $g_1,\ldots g_n$ 
    are measurable functions. Then $g_1(X_1),\ldots,g_n(X_n)$ are independent. 
\end{proposition}
\begin{proof}
    Suppose $g_i:(S_i,\S_i)\to (T_i,\T_i)$. For $A_i\in\T_i$, $g^{-1}(A_i)\in\S_i$ 
    and 
    \begin{equation*}
        \P(\cap_i \set{g_i(X_i)\in A_i}) = \P(\cap_i \set{X_i\in g^{-1}(A_i)}) 
        = \prod_i \P(X_i\in g^{-1}(A_i)) = \prod_i \P(g_i(X_i)\in A_i).
    \end{equation*}
    $g_1(X_1),\ldots,g_n(X_n)$ are independent. 
\end{proof}

\begin{theorem}\label{thm:pi-system_independence}
    Let $\S_1,\ldots\S_n$ be a collection of $\pi$-system. If $\Omega\in \S_i$ 
    for all $i = 1,\ldots,n$ and for all $A_i\in\S_i$, 
    \begin{equation*}
        \P(\cap_i A_i) = \prod_i \P(A_i), 
    \end{equation*}
    then $\sigma(\S_1),\ldots,\sigma(\S_n)$ are independent. 
\end{theorem}
\begin{proof}
    Fix $\S_2,\ldots,\S_n$. Put 
    \begin{equation*}
        \L = \Set{A\in\F}{\P(A\cap (\cap_{i=2}^n A_i)) = \P(A)\prod_{i=2}^n \P(A_i), A_i\in\S_i\text{ for $i = 2,\ldots,n$}}.
    \end{equation*}
    We claim that $\L$ forms a $\lambda$-system. First, by assumption we can 
    pick $A_i = \Omega$ for $i=2,\ldots,n$ to see that $\Omega\in\L$. Suppose 
    that $A\subset B$, $A,B\in\L$, 
    \begin{equation*}
        \begin{split}
            \P((B-A)\cap(\cap_{i=2}^n A_i)) 
            &= \P((B\cap(\cap_{i=2}^n A_i)) - (A\cap(\cap_{i=2}^n A_i))) \\ 
            &= \P(B)\prod_{i=2}^n\P(A_i) - \P(A)\prod_{i=2}^n\P(A_i) 
            = \P(B-A)\prod_{i=2}^n\P(A_i).
        \end{split}
    \end{equation*}
    Hence $B-A\in\L$. Let $S_j\nearrow S$, $S_j\in\L$. Then 
    \begin{equation*}
        \P(S\cap(\cap_{i=2}^nA_i)) = \lim_{j\to\infty} \P(S_j\cap(\cap_{i=2}^nA_i)) 
        = \lim_{j\to\infty}\P(S_j)\prod_{i=2}^n\P(A_i) 
        = \P(S)\prod_{i=2}^n\P(A_i).
    \end{equation*} 
    Thus $S\in\L$ and $\L$ is a $\lambda$-system. By Dynkin's $\pi$-$\lambda$, 
    $\sigma(\S_1),\S_2,\ldots,\S_n$ satisfies the product property. Repeat the 
    procedure for $\S_2,\ldots,\S_n$. We have that $\sigma(\S_1),\ldots,\sigma(\S_n)$ 
    satisfies the product property. That is, they are independent. 
\end{proof}

\begin{corollary}
    Let $X_1,\ldots,X_n$ be $\R$-valued random variables. Then they are independent 
    if and only if 
    \begin{equation*}
        \P(X_1\leq s_1,\ldots, X_n\leq s_n) = \prod_{i=1}^n \P(X_i\leq s_i)
    \end{equation*}
    for all $s_i\in\R$, $1\leq i\leq n$.
\end{corollary}
\begin{proof}
    The sufficient part is trivial. For the converse, put 
    $\S_i = \Set{\set{X_i\leq t}}{t\in\R}\cup\set{\Omega}$. Clearly $\S_i$ 
    are $\pi$-system and $\Omega\in\S_i$ for all $i$. $\sigma(\S_i)$ are 
    independent and $\S_i$ generates $\sigma(X_i)$. Applying \cref{thm:pi-system_independence} 
    shows that $X_i$ are independent.  
\end{proof}

\begin{corollary}\label{cor:group_independence}
    If $\F_{ij}$, $1\leq i\leq n, 1\leq j\leq m(i)$ are independent $\sigma$-algebras, 
    then $\G_i = \sigma(\cup_j \F_{ij})$ are independent. 
\end{corollary}
\begin{proof}
    Put $\H_i = \Set{\cap_j A_j}{A_j\in\F_{{ij}}}$. We claim that $\sigma(\H_i) = \G_i$. 
    Indeed, by choosing sets of the form 
    \begin{equation*}
        (\Omega,\ldots,\Omega,A_j,\Omega,\ldots,\Omega)\in\F_{i1}\times\cdots\times\F_{im(i)}, 
    \end{equation*}
    it is clear that $\cup_j\F_{ij} \subset \H_i$. Also, if $A\in\H_i$, then 
    \begin{equation*}
        A = \cap_j A_j = (\cup_j(A_j^c))^c \in\sigma(\cup_j\F_{ij}). 
    \end{equation*}
    Thus $\cup_j \F_{ij}\subset\H_i\subset\sigma(\cup_j \F_{ij})$ and 
    $\sigma(\H_i) = \sigma(\cup_j \F_{ij}) = \G_i$. Also notice that $\H_i$ 
    contain $\Omega$ and form $\pi$-systems. For $A_i\in\H_i$, write $A_i = \cap_j A_{ij}$. 
    Then 
    \begin{equation*}
        \P(\cap_i A_i) = \P(\cap_{ij}A_{ij}) = \prod_{ij}\P(A_{ij}) = \prod_i\P(\cap_j A_{ij}) 
        = \prod_i \P(A_i).
    \end{equation*}
    From \cref{thm:pi-system_independence} we know that $\G_i = \sigma(\H_i)$ are 
    independent. 
\end{proof}

\begin{corollary}
    If $X_{ij}, 1\leq i\leq n, 1\leq j\leq m(i)$ are independent random variables, then 
    $Y_i = h_i(X_{i1},\ldots,X_{im(i)})$ are independent provided that $h_i$ are measurable. 
\end{corollary}
\begin{proof}
    Write $\F_{ij} = \sigma(X_{ij})$. We claim that $\sigma(Y_i) \subset \sigma(\cup_j \F_{ij})$. 
    Indeed, if $B_i$ is a measurable set, $h_i^{-1}(B_i)$ is measurable. Write 
    $h_i^{-1}(B_i) = C_{i1}\times\cdots\times C_{im(i)}$ and since each 
    $X_{ij}^{-1}(C_{ij})\in\F_{ij}$, we see that $\sigma(Y_i)\subset\sigma(\cup_{j}\F_{ij})$. 
    It then follows from \cref{cor:group_independence} that $\sigma(Y_i)$ are independent 
    and $Y_i$ are independent. 
\end{proof}

\begin{theorem}
    If $X_1,\ldots X_n$ are independent $\R$-valued random variables 
    and the distribution of $X_i$ is $\mu_i$. Then the joint distribution 
    of $(X_1,\ldots,X_n)$ is $\mu_1\times\cdots\times\mu_n$. 
\end{theorem}
\begin{proof}
    Let $\mu$ be the distribution of $(X_1,\ldots,X_n)$. By definition, 
    \begin{equation*}
        \begin{split}
            \mu((X_1,\ldots)\in A_1\times\cdots\times A_n)
            &= \mu(X_1\in A_1,\ldots, X_n\in A_n) \\
            &= \prod_{i=1}^n \mu_i(X_i\in A_i) 
            = (\mu_1\times\cdots\times\mu_n)(A_1\times\cdots\times A_n).
        \end{split} 
    \end{equation*}
    Now the sets of the forms $A = A_1\times\cdots\times A_n$ is a $\pi$-system 
    generating the product $\sigma$-algebra. By \cref{cor:measure_agree}, 
    the joint distribution is exactly $\mu_1\times\cdots\times\mu_n$. 
\end{proof}

\begin{theorem}\label{thm:mean_independence}
    Let $X,Y$ be two independent random variables. If 
    $h(x,y)$ satisfies either 
    \begin{thmenum}
        \item $\E\sbrc{\abs{h(X, Y)}}<\infty$, or 
        \item $h$ is non-negative, 
    \end{thmenum}
    then 
    \begin{equation*}
        \E\sbrc{h(X,Y)} = \int\int hd\mu_Xd\mu_Y,
    \end{equation*}
    where $\mu_X,\mu_Y$ are the distributions of $X$ and $Y$, 
    respectively.
\end{theorem}
\begin{proof}
    The proof follows directly from Fubini-Tonelli theorem. If 
    one of the assumptions is true, then 
    \begin{equation*}
        \E\sbrc{h(X,Y)} = \int_{\R^2}hd(\mu_X\times\mu_Y) 
        = \int\int hd\mu_Xd\mu_Y. 
    \end{equation*}
\end{proof}
\begin{remark}
    If $h(x,y) = h_1(x)h_2(y)$, then 
    \begin{equation*}
        \E\sbrc{h_1(X)h_2(Y)} = \E\sbrc{h(X,Y)} 
        = \int\int h_1h_2d\mu_Xd\mu_Y 
        = \E\sbrc{h_1(X)}\E\sbrc{h_2(Y)}. 
    \end{equation*}
\end{remark}

\begin{corollary}
    If $X_1,\ldots X_n$ are independent random variables 
    and 
    \begin{thmenum}
        \item $\E\sbrc{\abs{X_1\cdots X_n}} < \infty$ or 
        \item $X_i\geq 0$ for all $i$,  
    \end{thmenum}
    then 
    \begin{equation*}
        \E\sbrc{X_1\cdots X_n} = \prod_{i=1}^n\E\sbrc{X_i}. 
    \end{equation*}
\end{corollary}
\begin{proof}
    Let $h(x,y) = xy$. By assumptions, we have either 
    $\E\sbrc{\abs{h(X_1,X_2)}}<\infty$ or $h(X_1,X_2)\geq 0$. 
    By \cref{thm:mean_independence}, $\E\sbrc{X_1X_2} = \E\sbrc{X_1}\E\sbrc{X_2}$. 
    Substitute $X_1$ by $X_1X_2$ and $X_2$ by $X_3$, we see that 
    $\E\sbrc{X_1X_2X_3} = \E\sbrc{X_1}\E\sbrc{X_2}\E\sbrc{X_3}$. 
    Repeat the procedure $n$ times and the result follows. 
\end{proof}

\begin{definition}
    Let $X, Y$ be independent random variables with CDF $F$ and $G$, respectively. 
    The \textbf{convolution} of two CDF is defined as 
    \begin{equation*}
        (F*G)(z) = \int F(z-y)dG(y). 
    \end{equation*}
\end{definition}
\begin{remark}
    If $F$ and $G$ are absolutely continuous with respect to 
    the Lebesgue measure, then they have Radon-Nikodym derivatives 
    $f$ and $g$. The definition of convolution becomes 
    \begin{equation*}
        (F*G)(z) = \int F(z-y)dG(y) = \int_{-\infty}^\infty\int_{-\infty}^{z-y}f(x)g(y)dxdy. 
    \end{equation*}
    Then 
    \begin{equation*}
        (F*G)'(z) = \int f(z-y)g(y)dy = (f*g)(z),
    \end{equation*}
    which is exactly the definition of convolution of two functions. 
\end{remark}

\begin{proposition}
    Let $X$ and $Y$ be independent random variables. Then 
    \begin{equation*}
        \P(X + Y \leq z) = (F*G)(z). 
    \end{equation*}
\end{proposition}
\begin{proof}
    By \cref{thm:mean_independence}, 
    \begin{equation*}
        \begin{split}
            \P(X + Y \leq z) &= \E\sbrc{\one\set{X+Y\leq z}} 
            = \int\int \one\set{x+y\leq z}dF(x)dG(y) \\
            &= \int F(z-y)dG(y) = (F*G)(z). 
        \end{split}
    \end{equation*}
\end{proof}
\begin{remark}
    Note that the convolution is commutative since 
    \begin{equation*}
        (F*G)(z) = \P(X+Y\leq z) = \P(Y+X\leq z) = (G*F)(z). 
    \end{equation*}
\end{remark}
\begin{remark}
    For discrete $X$ and $Y$, the convolution becomes 
    \begin{equation*}
        \P(X+Y = z) = \sum_y \P(X = z-y)\P(Y = y). 
    \end{equation*}
\end{remark}

\begin{example}
    Consider $X\sim \Gamma(\alpha_1, \beta)$ and $Y\sim\Gamma(\alpha_2, \beta)$. 
    Then the density for $X+Y$ is 
    \begin{equation*}
        \begin{split}
            f_{X+Y}(z) &= \int f_X(z-y)f_Y(y)dy \\ 
            &= \int_0^z \frac{1}{\Gamma(\alpha_1)}\beta^{\alpha_1}(z-y)^{\alpha_1-1}e^{-\beta (z-y)}\frac{1}{\Gamma(\alpha_2)}\beta^{\alpha_2}y^{\alpha_2-1}e^{-\beta y}dy \\
            &= \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\beta^{\alpha_1+\alpha_2}e^{-\beta z}\int_0^z (z-y)^{\alpha_1 - 1}y^{\alpha_2 - 1}dy \\ 
            &= \frac{1}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\beta^{\alpha_1+\alpha_2}e^{-\beta z}z^{\alpha_1+\alpha_2-1}\int_0^1(1-t)^{\alpha_1-1}t^{\alpha_2-1}dt \\ 
            &= \frac{B(\alpha_1, \alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}\beta^{\alpha_1+\alpha_2}e^{-\beta z}z^{\alpha_1+\alpha_2-1} 
            = \frac{1}{\Gamma(\alpha_1+\alpha_2)}\beta^{\alpha_1+\alpha_2}e^{-\beta z}z^{\alpha_1+\alpha_2-1}. 
        \end{split}
    \end{equation*} 
    Hence $X+Y\sim \Gamma(\alpha_1 + \alpha_2, \beta)$. 
\end{example}