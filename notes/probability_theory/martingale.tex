\begin{definition}
    Let $(\Omega, \F, \P)$ be a probability space. A \textbf{stochastic process} 
    is a collection of random variables $X_t:\Omega\to (S, \S)$ where $t\in T$. 
    $T$ is a totally ordered index set. 
\end{definition}
\begin{remark}
    $T$ is often taken to be $\Z_+$ or $\R_+$, which represents the time. If 
    $T$ is countable, we say that $X_t$ is a \textbf{discrete time} process and 
    \textbf{continuous time} if $T$ is some uncountable subset of $\R$. 
\end{remark}

\begin{definition}
    A \textbf{filtration} $\set{\F_t}_{t\in T}$ is a collection of $\sigma$-algebras such that 
    $\F_t\subset\F_s$ for every $t<s$, $t,s\in T$.  
\end{definition}

\begin{definition}
    Let $X_t$ be a stochastic process and $\F_t$ be a filtration. We say that 
    $X_t$ is \textbf{adapted} to $\F_t$ or \textbf{$\F_t$-adapted} if $\sigma(X_t)\subset\F_t$ for all $t$. 
\end{definition}
\begin{remark}
    In many cases, the filtration is not mentioned since we may consider the 
    natural filtration generated by $X_t$ through the definition $\F_t = \sigma(\set{X_s|s\leq t})$. 
\end{remark}

\begin{definition}
    Let $X_t$ be a $\R$-valued stochastic process. We say that $X_t$ is an 
    \textbf{$\F_t$-martingale} if 
    \begin{thmenum}
        \item $\E\sbrc{\abs{X_t}}<\infty$. 
        \item $X_t$ is $\F_t$-adapted. 
        \item $X_t = \E\sbrc{X_s|\F_t}$ for all $s>t$. 
    \end{thmenum}
    We say that $X_t$ is a \textbf{supermartingale} if $X_t\geq \E\sbrc{X_s|\F_t}$ and 
    \textbf{submartingale} if $X_t\leq \E\sbrc{X_s|\F_t}$ for all $s>t$. 
\end{definition}
\begin{remark}
    $X_t$ is a martingale if and only if $X_t$ is both a submartingale and a supermartingale.
\end{remark}

\begin{proposition}
    Let $X_n$ be a discrete time stochastic process. 
    \begin{thmenum}
        \item If $X_n\geq \E\sbrc{X_{n+1}|\F_n}$ for all $n$, then $X_n$ is a supermartingale. 
        \item If $X_n\leq \E\sbrc{X_{n+1}|\F_n}$ for all $n$, then $X_n$ is a submartingale.
    \end{thmenum} 
\end{proposition}
\begin{proof}
    The proof for (b) is similar to (a). We prove only the case (a). Let $m>n$. 
    \begin{equation*}
        \E\sbrc{X_m|\F_n} = \E\sbrc{\E\sbrc{X_{m}|\F_{m-1}}|\F_n} 
        \geq \E\sbrc{X_{m-1}|\F_n} = \cdots \geq \E\sbrc{X_n|\F_n} = X_n.
    \end{equation*} 
    Hence $X_n$ is a supermartingale. 
\end{proof}

\begin{example}[Random Walk]
    Let $\xi_i\in\L^1$ be independent and identically distributed with $\E\sbrc{\xi_i} = 0$. 
    Set $X_0 = 0$ and $X_n = X_{n-1} + \xi_n$ for $n\in\N$. Clearly $X_n$ is 
    adapted to the natural filtration $\F_n$ generated by $X_n$ and 
    $\E\sbrc{\abs{X_n}}\leq n\E\sbrc{\abs{\xi_i}}<\infty$ for given $n$. Also, 
    \begin{equation*}
        \E\sbrc{X_{n+1}|\F_n} = \E\sbrc{X_n + \xi_{n+1}|\F_n} = X_n. 
    \end{equation*}
    Hence $X_n$ is a martingale. 
\end{example}

\begin{example}[Quadratic Martingale]
    Let $\xi_i$ be independent and identically distributed with $\E\sbrc{\xi_i} = 0$ 
    and $\E\sbrc{\xi_i^2} = \sigma^2<\infty$. Put $X_n = \sum_{i\leq n}\xi_i$. Then 
    $M_n = X_n^2 - n\sigma^2$ is a martingale. It is clear that $M_n$ is adapted 
    to the natural filtration $\F_n$ generated by $X_n$ and 
    \begin{equation*}
        \E\sbrc{\abs{M_n}} \leq \E\sbrc{X_n^2} + n\sigma^2 = 2n\sigma^2<\infty. 
    \end{equation*}
    Also, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{M_{n+1}|\F_n} &= \E\sbrc{X_{n+1}^2-(n+1)\sigma^2|\F_n} 
            = \E\sbrc{X_n^2 + 2\xi_{n+1}X_n + \xi_{n+1}^2 - (n+1)\sigma^2|\F_n} \\ 
            &= X_n^2 + \sigma^2 - (n+1)\sigma^2 = M_n. 
        \end{split}
    \end{equation*}
    Hence $M_n$ is a martingale. 
\end{example}

\begin{example}[Exponential Martingale]
    Let $\xi_i$ be independent and identically distributed with $M(t) = \E\sbrc{\exp(t\xi_i)}<\infty$ 
    for given $t$. Put $X_i = \frac{\exp(t\xi_i)}{M(t)}$ and thus $\E\sbrc{X_i} = 1$. 
    Let $M_n = \prod_{i\leq n}X_i$. $M_n$ is adapted to the natural filtration $\F_n$ generated 
    by $X_n$ and 
    \begin{equation*}
        \E\sbrc{\abs{M_n}} = \E\sbrc{\prod_{i\leq n}X_i} = \prod_{i\leq n}\E\sbrc{X_i} = 1. 
    \end{equation*}
    Also, 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = M_n\E\sbrc{X_{n+1}|\F_n} = M_n\E\sbrc{X_{n+1}} = M_n. 
    \end{equation*}
    Hence $M_n$ is a martingale. 
\end{example}

\begin{lemma}
    Let $X_t$ be a $\F_t$-martingale and $\varphi$ is a function such that 
    $\E\sbrc{\abs{\varphi(X_t)}}<\infty$. 
    \begin{thmenum}
        \item If $\varphi$ is convex, then $\varphi(X_t)$ is a submartingale. 
        \item If $\varphi$ is concave, then $\varphi(X_t)$ is a supermartingale
    \end{thmenum}
\end{lemma}
\begin{proof}
    The proof for (b) is similar to (a). We only prove the case (a). Let $s>t$. 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \geq \varphi(\E\sbrc{X_s|\F_t}) = \varphi(X_t).
    \end{equation*}
    Hence $\varphi(X_t)$ is a submartingale. 
\end{proof}

\begin{lemma}\label{lem:mart_convex}
    Let $X_t$ be a stochastic process and $\varphi$ is an increasing function such that 
    $\E\sbrc{\abs{\varphi(X_t)}}<\infty$. 
    \begin{thmenum}
        \item If $X_t$ is a $\F_t$-submartingale and $\varphi$ is convex, then 
        $\varphi(X_t)$ is a $\F_t$-submartingale. 
        \item If $X_t$ is a $\F_t$-supermartingale and $\varphi$ is concave, then 
        $\varphi(X_t)$ is a $\F_t$-supermartingale.  
    \end{thmenum}
\end{lemma}
\begin{proof}
    For (a), let $s>t$. 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \geq \varphi(\E\sbrc{X_s|\F_t}) \geq \varphi(X_t).
    \end{equation*}
    Hence $\varphi(X_t)$ is a submartingale.

    For (b), 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \leq \varphi(\E\sbrc{X_s|\F_t}) \leq \varphi(X_t). 
    \end{equation*}
    Hence $\varphi(X_t)$ is a supermartingale.
\end{proof}

\begin{definition}
    Let $X_n$ be a stochastic process and $\F_n$ be the filtration generated by $X_n$. 
    A stochastic process $H_n$ is said to be \textbf{predictable} if $H_{n+1}$ is $\F_{n}$-adapted. 
\end{definition}

\begin{definition}
    Let $X_n$, $Y_n$ be two discrete-time stochastic processes. The \textbf{discrete-time 
    stochastic integral} is defined as 
    \begin{equation*}
        (X\cdot Y)_n = \sum_{i\leq n} X_i(Y_i - Y_{i-1}). 
    \end{equation*}
\end{definition}

\begin{theorem}\label{thm:no_arbitrage}
    If $X_n$ is a $\F_n$-supermartingale and $H_n\geq 0$ is predictable and is bounded 
    for each $n$. Then $(H\cdot X)_n$ is a supermartingale.  
\end{theorem}
\begin{proof}
    First, it is clear that $(H\cdot X)_n$ is $\F_n$-adapted since the discrete-time stochastic
    integral is a function of $(H_1,\ldots H_n,X_0,\ldots,X_n)$. Second, since $H_n$ is 
    bounded, say by $c_i$, 
    \begin{equation*}
        \E\sbrc{\abs{(H\cdot X)_n}} \leq \sum_{i\leq n}\E\sbrc{\abs{H_i}\abs{X_{i}-X_{i-1}}} 
        \leq \sum_{i\leq n}c_i\max_{1\leq i\leq n}2\E\sbrc{\abs{X_i}} < \infty
    \end{equation*}
    for given $n$ since $X_n$ is a supermartingale and satisfies that $X_n\in\L^1$. 
    Finally, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{(H\cdot X)_{n+1}|\F_n} &= (H\cdot X)_n + \E\sbrc{H_{n+1}(X_{n+1}-X_n)|\F_n} \\ 
            &= (H\cdot X)_n + H_{n+1}\E\sbrc{X_{n+1}|\F_n} - H_{n+1}X_n \\ 
            &\leq (H\cdot X)_n + H_{n+1}X_n - H_{n+1}X_n = (H\cdot X)_n.
        \end{split}
    \end{equation*}
    Hence $(H\cdot X)_n$ is a supermartingale. 
\end{proof}
\begin{remark}
    If we instead assume that $X_n$ is a $\F_n$-submartingale, then $(H\cdot X)_n$ is a submartingale. 
    Furthermore, if $X_n$ is a $\F_n$-martingale, then for every predictable $H_n$, $(H\cdot X)_n$ is 
    a martingale. 
\end{remark}

\begin{definition}
    A random variable $\tau\in T$ is a \textbf{stopping time} if $\set{\tau\leq t}\in\F_t$. 
\end{definition}

\begin{example}[Hitting Time]
    Let $X_t$ be a stochastic process and $\F_t$ be the natural filtration generated by $X_t$. 
    The hitting time of $A$ is $\tau = \inf\Set{s\in T}{X_s\in A}$. Then $\tau$ is a stopping 
    time. 
\end{example}

\begin{proposition}
    Suppose that $\tau_1$ and $\tau_2$ are stopping times with respect to $\F_t$. 
    Then $\tau_1\wedge\tau_2$ and $\tau_1\vee\tau_2$ are stopping times. 
\end{proposition}
\begin{proof}
    Notice that 
    \begin{equation*}
        \set{\tau_1\wedge\tau_2\leq t} = \set{\tau_1\leq t}\cup\set{\tau_2\leq t}\in\F_t, 
        \quad\text{and}\quad 
        \set{\tau_1\vee\tau_2\leq t} = \set{\tau_1\leq t}\cap\set{\tau_2\leq t}\in\F_t.
    \end{equation*}
    Hence $\tau_1\wedge\tau_2$ and $\tau_1\vee\tau_2$ are stopping times. 
\end{proof}
\begin{remark}
    Since any constant $n$ is also a stopping time, $n\wedge\tau$ is a 
    stopping time provided that $\tau$ is. The conclusion from \cref{thm:no_arbitrage} 
    also applies to the strategy of the form $H_n' = H_{n\wedge \tau}$. In particular, 
    taking the predictable process 
    \begin{equation*}
        H_n = \begin{cases}
            1 & \tau \leq n \\ 
            0 & \tau > n
        \end{cases}
    \end{equation*}
    shows that $X_{n\wedge\tau}$ is a martingale provided that $X_n$ is.
\end{remark}

\begin{definition}
    Let $X_t$ be a stochastic process with index set $T$ and $a<b$ be constants. 
    Given $\omega$, the \textbf{$[a,b]$-crossing time} $N_a^b(\omega)$ is defined as the supremum 
    over $n\in\Z_+$ such that there is $s_1<t_1<s_2<t_2<\cdots<s_n<t_n$, $s_i, t_i\in T$ 
    with $X_{s_1}(\omega)\leq a$ and $X_{t_1}(\omega)\geq b$.
\end{definition}

\begin{lemma}[Doob's Upcrossing Lemma]
    Let $X_n$ be a submartingale on $\Z_+$ and $N_a^b(n)$ be the crossing time up to $n\in\Z_+$. 
    Then 
    \begin{equation*}
        \E\sbrc{N_a^b(n)}\leq \frac{1}{b-a}\E\sbrc{(X_n-a)^+}
    \end{equation*}
    for all $n\in\Z_+$. 
\end{lemma}
\begin{proof}
    Let $Y_n = a+(X_n-a)^+$. By \cref{lem:mart_convex}, $Y_n$ is also a submartingale 
    since $x\mapsto a+(x-a)^+$ is a convex increasing function. Notice that the 
    crossing time up to time $n$ for $Y_n$ is also $N_a^b(n)$. Now define the 
    stopping times $0 = \tau_0 \leq \sigma_1 < \tau_1 < \cdots$ by 
    \begin{equation*}
        \sigma_k = \inf\Set{n\geq\tau_{k-1}}{Y_n = a}, \quad\text{and}\quad 
        \tau_k = \inf\Set{n\geq \sigma_k}{Y_n \geq b}. 
    \end{equation*}
    Next, we consider the predictable process 
    \begin{equation*}
        H_n = \sum_{k\geq 1}\one\set{\sigma_k< n\leq \tau_k}.
    \end{equation*}
    By \cref{thm:no_arbitrage}, $((1-H)\cdot Y)_n$ is a submartingale and thus 
    \begin{equation*}
        \E\sbrc{((1-H)\cdot Y)_n} \geq \E\sbrc{((1-H)\cdot Y)_0} = 0.
    \end{equation*}
    Notice that $(H\cdot Y)_n\geq (b-a)N_a^b(n)$. We have 
    \begin{equation*}
        \E\sbrc{(X_n-a)^+} = \E\sbrc{(H\cdot Y)_n} + \E\sbrc{((1-H)\cdot Y)_n} 
        \geq (b-a)\E\sbrc{N_a^b(n)}.
    \end{equation*}
    Rearranging the inequality gives the desired result. 
\end{proof}

\begin{theorem}[Martingale Convergence Theorem]
    Let $X_n$ be a $\F_n$-submartingale. Suppose that $\sup_n\E\sbrc{X_n^+}<\infty$. 
    Then there is $X\in\L^1$ such that $X_n\to X$ almost surely. 
\end{theorem}
\begin{proof}
    For $a, b\in\Q$, consider the event $A_{a,b} = \set{\liminf_nX_n < a < b < \limsup_n X_n}$. 
    Put $S$ be the union of all $A_{a,b}$ over $a,b\in\Q$. For given $a<b$, consider the 
    crossing time $N_a^b$ on $\Z_+$ and $N_a^b(n)$ which is up to time $n$. 
    Clearly $N_a^b(n)\nearrow N_a^b$. By LMCT and Doob's upcrossing lemma,
    \begin{equation*}
        \E\sbrc{N_a^b} = \lim_{n\to\infty}\E\sbrc{N_a^b(n)} \leq \limsup_{n\to\infty}\frac{1}{b-a}\E\sbrc{(X_n-a)^+}\leq C\sup_n\E\sbrc{X_n^+}
    \end{equation*}
    for some constant $C$. Hence $\P(N_a^b<\infty) = 0$. This implies that $X_n$ crosses 
    $[a,b]$ only finite times almost surely. Thus $\P(A_{a,b}) = 0$ and $\P(S) = 0$. 
    We see that 
    \begin{equation*}
        \P(\liminf_n X_n<\limsup_n X_n) = 0.
    \end{equation*}
    Thus the limit of $X_n$ exists, possibly in extended sense, almost surely. 
    Denote the limit of $X_n$ as $X$. It now remains to show that $X\in\L^1$. 
    \begin{equation*}
        \E\sbrc{X^+} = \E\sbrc{\lim_{n\to\infty}X_n^+}
        \leq \liminf_{n\to\infty}\E\sbrc{X_n^+}\leq\sup_{n}\E\sbrc{X_n^+} < \infty
    \end{equation*}
    by Fatou's lemma. Also, $X_n^- = X_n^+ - X_n$. 
    \begin{equation*}
        \E\sbrc{X_n^-} = \E\sbrc{X_n^+} - \E\sbrc{X_n} \leq \E\sbrc{X_n^+} - \E\sbrc{X_0} < \infty. 
    \end{equation*}
    Hence $X\in\L^1$. The proof is complete. 
\end{proof}
\begin{remark}
    The theorem also holds for supermartingale. If $X_n$ is a $\F_n$-supermartingale 
    and $\sup_n\E\sbrc{X_n^-}<\infty$. Then there is $X\in\L^1$ such that $X_n\to X$ 
    almost surely. 
\end{remark}

\begin{corollary}
    If $X_n\geq 0$ is a supermartingale, then there is $X\in\L^1$ such that 
    $X_n\to X$ almost surely and $\E\sbrc{X}\leq \E\sbrc{X_0}$. 
\end{corollary}
\begin{proof}
    Since $\sup_n\E\sbrc{X_n^-} = 0$, the martingale convergence theorem implies 
    that there is $X\in\L^1$ such that $X_n\to X$ almost surely. By Fatou's 
    lemma, 
    \begin{equation*}
        \E\sbrc{X} = \E\sbrc{\liminf_{n\to\infty} X_n} 
        \leq \liminf_{n\to\infty}\E\sbrc{X_n}\leq \liminf_{n\to\infty}\E\sbrc{X_0} = \E\sbrc{X_0}. 
    \end{equation*}
    The proof is complete.  
\end{proof}
\begin{remark}
    The hard bound on $X_n$ is neccessary. Consider $\xi_i$ with 
    $\P(\xi_i = \pm 1) = 1/2$. Let $X_n = \sum_{i\leq n}\xi_i$ and 
    $X_0 = 1$. Consider the stopping time $\tau = \inf\Set{n}{X_n = 0}$.  
    Then $\P(\tau < \infty) = 1$. Since $X_{n\wedge \tau}$ is a martingale, 
    $\E\sbrc{X_{n\wedge\tau}} = X_0 = 1$. Also, the martingale convergence 
    theorem shows that there is $X$ where $X_{n\wedge\tau}\to X$ as $n\to\infty$. 
    Since $\tau<\infty$ almost surely, we have $X = 0$ almost surely and 
    $\E\sbrc{X} = 0$. 
\end{remark}

\begin{example}[P\'olya's Urn]
    Let $X_0,Y_0\in \N$. Define 
    \begin{equation*}
        (X_n, Y_n) = \begin{cases}
            (X_{n-1}+1, Y_{n-1}),& \text{prob.} = \frac{X_{n-1}}{X_{n-1} + Y_{n-1}}, \\
            (X_{n-1}, Y_{n-1}+1),& \text{prob.} = \frac{Y_{n-1}}{X_{n-1} + Y_{n-1}}.
        \end{cases}
    \end{equation*}
    Let $Z_n = X_n/(X_n + Y_n)$. Notice that 
    \begin{equation*}
        \begin{split}
            \E\sbrc{Z_{n+1}|\F_n} &= Z_n\cdot \frac{X_n+1}{X_n+1+Y_n} + (1 - Z_n)\cdot \frac{X_n}{X_n+Y_n+1} 
            = \frac{X_n}{X_n+Y_n}\cdot\frac{1}{X_n+1+Y_n} + \frac{X_n}{X_n+Y_n+1} \\ 
            &= \frac{X_n}{X_n+Y_n} = Z_n. 
        \end{split}
    \end{equation*}
    Hence $Z_n$ is a martingale. Since $Z_n$ is clearly bounded, the martingale 
    convergence theorem implies that there is some $Z\in\L^1$ such that 
    $Z_n\to Z$ almost surely. In fact, one can show that $Z\sim\text{Beta}(X_0, Y_0)$. 
\end{example}

\begin{theorem}[Optional Stopping Theorem]
    Let $\tau$ be a stopping time and $X_n$ be a martingale. Then $\E\sbrc{\abs{X_\tau}}<\infty$ 
    and $\E\sbrc{X_\tau} = \E\sbrc{X_0}$ if one of the followings holds: 
    \begin{thmenum}
        \item There is some $N$ such that $\tau\leq N$ almost surely. 
        \item There is some $M$ such that $\abs{X_{\tau\wedge n}}\leq M$ and $\P(\tau<\infty) = 1$. 
        \item $\E\sbrc{\tau}<\infty$ and $\abs{X_{n+1}-X_n}\leq M$ almost surely for some $M$. 
    \end{thmenum}
\end{theorem}
\begin{proof}
    For (a), we have that 
    \begin{equation*}
        \abs{X_{n\wedge \tau} - X_0} \leq \sum_{k=1}^{n\wedge\tau} \abs{X_{k}-X_{k-1}}\leq \sum_{k=1}^N \abs{X_{k}-X_{k-1}}\in\L^1
    \end{equation*}
    for every $n$. Hence by the LDCT, since $\tau<\infty$ almost surely and $X_{n\wedge\tau}\to X_{\tau}$, 
    \begin{equation*}
        \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}
    \end{equation*}
    by the fact that $X_{n\wedge\tau}$ is a martingale by \cref{thm:no_arbitrage}. 

    For (b), by the bounded convergence theorem and that $\tau<\infty$ almost surely, 
    \begin{equation*}
        \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}. 
    \end{equation*}

    For (c), notice that 
    \begin{equation*}
        \abs{X_{n\wedge\tau} - X_0} \leq \sum_{k=1}^{n\wedge\tau}\abs{X_{k}-X_{k-1}}\leq M\tau\in\L^1
    \end{equation*}
    since $\tau\in\L^1$. Thus $\abs{X_{n\wedge\tau}}\leq M\tau\in\L^1$. LDCT implies that 
    \begin{equation*}
            \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}.
    \end{equation*}
\end{proof}

\begin{corollary}[Wald's Identity]
    Let $X_i$ be independent and identically distributed with $\E\sbrc{X_i} = \mu < \infty$. 
    $\abs{X_i}\leq K$ almost surely. Let $S_n = \sum_{i\leq n}X_i$, $S_0 = 0$ and $\tau$ be a 
    stopping time with $\E\sbrc{\tau}<\infty$. Then $\E\sbrc{S_{\tau}} = \mu\E\sbrc{\tau}$. 
\end{corollary}
\begin{proof}
    Consider $M_n = S_n - n\mu$. Notice that 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = \E\sbrc{X_{n+1} - \mu|\F_n} + S_n - n\mu = M_n.
    \end{equation*}
    Thus $M_n$ is a martingale. Also, 
    \begin{equation*}
        \abs{M_{n+1}-M_n} = \abs{X_{n+1}-\mu}\leq K+\abs{\mu}
    \end{equation*}
    almost surely. By the optional stopping theorem (c), 
    \begin{equation*}
        \E\sbrc{S_{\tau} - \tau\mu} = \E\sbrc{M_\tau} = \E\sbrc{M_0} = 0.
    \end{equation*}
    Hence $\E\sbrc{S_{\tau}} = \mu\E\sbrc{\tau}$. 
\end{proof}

\begin{example}
    Consider a random walk $X_n = \sum_{i\leq n}\xi_i$ where $\P(\xi_i = \pm 1) = 1/2$ 
    are independent and $X_0 = x\in\Z$. Define the hitting time $\tau = \inf\Set{k}{X_k = a,b}$ 
    where $a<x<b$, $a,b\in\Z$. We first show that 
    \begin{equation*}
        \P(X_\tau = a) = \frac{b-x}{b-a}. 
    \end{equation*} 
    Since $X_n$ is a martingale, $X_{n\wedge\tau}$ is again a martingale by 
    \cref{thm:no_arbitrage}. Thus 
    \begin{equation*}
        \begin{split}
            x &= \E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_\tau\one\set{n\geq \tau}} + \E\sbrc{X_n\one\set{n<\tau}} \\
            &= a\P(X_\tau = a, \tau\leq n) + b\P(X_\tau = b, \tau\leq n) + \E\sbrc{X_n\one\set{n<\tau}}.
        \end{split}
    \end{equation*}
    Notice that $\abs{X_n}\one\set{n<\tau}\leq \abs{a}\vee\abs{b}$ and $\abs{X_n}\one\set{n<\tau}\to 0$ 
    since $\tau<\infty$ almost surely. Letting $n\to\infty$ gives 
    \begin{equation*}
        x = a\P(X_\tau = a) + b\P(X_\tau = b) = a\P(X_\tau = a) + b(1 - \P(X_\tau = a)), \quad\Rightarrow\quad 
        \P(X_\tau = a) = \frac{b-x}{b-a}. 
    \end{equation*}
    Next, we show that 
    \begin{equation*}
        \E\sbrc{\tau} = (b-x)(x-a). 
    \end{equation*}
    Consider $M_n = X_n^2 - n$. Then clearly the integrability and measurability holds. Also, 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = \E\sbrc{X_n^2 + 2X_n\xi_{n+1} + \xi_{n+1}^2 - (n+1)|\F_n} = M_n. 
    \end{equation*}
    Hence $M_n$ is a martingale. Again, by \cref{thm:no_arbitrage}, $M_{n\wedge\tau}$ 
    is still a martingale. 
    \begin{equation*}
        \begin{split}
            x^2 &= \E\sbrc{M_{n\wedge\tau}} = \E\sbrc{X_\tau^2\one\set{\tau\leq n}} + \E\sbrc{X_n^2\one\set{n< \tau}} - \E\sbrc{n\wedge\tau} \\ 
            &= a^2\P(X_\tau = a, \tau\leq n) + b^2\P(X_\tau = b, \tau\leq n) + \E\sbrc{X_n^2\one\set{n< \tau}} - \E\sbrc{n\wedge\tau}.
        \end{split}
    \end{equation*}
    Notice that $\tau<\infty$ almost surely and thus $n\wedge\tau\nearrow\tau$. 
    Also, $X_n^2\one\set{n<\tau}<(a^2\vee b^2)$ and $X_n^2\one\set{n<\tau}\to 0$ as $n\to\infty$. 
    By the LMCT and LDCT, taking $n\to\infty$, 
    \begin{equation*}
        x^2 = a^2\P(X_\tau = a) + b^2\P(X_\tau = b) - \E\sbrc{\tau} = a^2\frac{b-x}{b-a} + b^2\frac{x-a}{b-a} - \E\sbrc{\tau}.
    \end{equation*}
    Hence $\E\sbrc{\tau} = (b-x)(x-a)$. 
\end{example}