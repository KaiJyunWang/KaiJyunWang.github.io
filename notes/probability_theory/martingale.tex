\begin{definition}
    Let $(\Omega, \F, \P)$ be a probability space. A \textbf{stochastic process} 
    is a collection of random variables $X_t:\Omega\to (S, \S)$ where $t\in T$. 
    $T$ is a totally ordered index set. 
\end{definition}
\begin{remark}
    $T$ is often taken to be $\Z_+$ or $\R_+$, which represents the time. If 
    $T$ is countable, we say that $X_t$ is a \textbf{discrete time} process and 
    \textbf{continuous time} if $T$ is some uncountable subset of $\R$. 
\end{remark}

\begin{definition}
    A \textbf{filtration} $\set{\F_t}_{t\in T}$ is a collection of $\sigma$-algebras such that 
    $\F_t\subset\F_s$ for every $t<s$, $t,s\in T$.  
\end{definition}

\begin{definition}
    Let $X_t$ be a stochastic process and $\F_t$ be a filtration. We say that 
    $X_t$ is \textbf{adapted} to $\F_t$ or \textbf{$\F_t$-adapted} if $\sigma(X_t)\subset\F_t$ for all $t$. 
\end{definition}
\begin{remark}
    In many cases, the filtration is not mentioned since we may consider the 
    natural filtration generated by $X_t$ through the definition $\F_t = \sigma(\set{X_s|s\leq t})$. 
\end{remark}

\begin{definition}
    Let $X_t$ be a $\R$-valued stochastic process. We say that $X_t$ is an 
    \textbf{$\F_t$-martingale} if 
    \begin{thmenum}
        \item $\E\sbrc{\abs{X_t}}<\infty$. 
        \item $X_t$ is $\F_t$-adapted. 
        \item $X_t = \E\sbrc{X_s|\F_t}$ for all $s>t$. 
    \end{thmenum}
    We say that $X_t$ is a \textbf{supermartingale} if $X_t\geq \E\sbrc{X_s|\F_t}$ and 
    \textbf{submartingale} if $X_t\leq \E\sbrc{X_s|\F_t}$ for all $s>t$. 
\end{definition}
\begin{remark}
    $X_t$ is a martingale if and only if $X_t$ is both a submartingale and a supermartingale.
\end{remark}

\begin{proposition}
    Let $X_n$ be a discrete time stochastic process. 
    \begin{thmenum}
        \item If $X_n\geq \E\sbrc{X_{n+1}|\F_n}$ for all $n$, then $X_n$ is a supermartingale. 
        \item If $X_n\leq \E\sbrc{X_{n+1}|\F_n}$ for all $n$, then $X_n$ is a submartingale.
    \end{thmenum} 
\end{proposition}
\begin{proof}
    The proof for (b) is similar to (a). We prove only the case (a). Let $m>n$. 
    \begin{equation*}
        \E\sbrc{X_m|\F_n} = \E\sbrc{\E\sbrc{X_{m}|\F_{m-1}}|\F_n} 
        \geq \E\sbrc{X_{m-1}|\F_n} = \cdots \geq \E\sbrc{X_n|\F_n} = X_n.
    \end{equation*} 
    Hence $X_n$ is a supermartingale. 
\end{proof}

\begin{example}[Random Walk]
    Let $\xi_i\in\L^1$ be independent and identically distributed with $\E\sbrc{\xi_i} = 0$. 
    Set $X_0 = 0$ and $X_n = X_{n-1} + \xi_n$ for $n\in\N$. Clearly $X_n$ is 
    adapted to the natural filtration $\F_n$ generated by $X_n$ and 
    $\E\sbrc{\abs{X_n}}\leq n\E\sbrc{\abs{\xi_i}}<\infty$ for given $n$. Also, 
    \begin{equation*}
        \E\sbrc{X_{n+1}|\F_n} = \E\sbrc{X_n + \xi_{n+1}|\F_n} = X_n. 
    \end{equation*}
    Hence $X_n$ is a martingale. 
\end{example}

\begin{example}[Quadratic Martingale]
    Let $\xi_i$ be independent and identically distributed with $\E\sbrc{\xi_i} = 0$ 
    and $\E\sbrc{\xi_i^2} = \sigma^2<\infty$. Put $X_n = \sum_{i\leq n}\xi_i$. Then 
    $M_n = X_n^2 - n\sigma^2$ is a martingale. It is clear that $M_n$ is adapted 
    to the natural filtration $\F_n$ generated by $X_n$ and 
    \begin{equation*}
        \E\sbrc{\abs{M_n}} \leq \E\sbrc{X_n^2} + n\sigma^2 = 2n\sigma^2<\infty. 
    \end{equation*}
    Also, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{M_{n+1}|\F_n} &= \E\sbrc{X_{n+1}^2-(n+1)\sigma^2|\F_n} 
            = \E\sbrc{X_n^2 + 2\xi_{n+1}X_n + \xi_{n+1}^2 - (n+1)\sigma^2|\F_n} \\ 
            &= X_n^2 + \sigma^2 - (n+1)\sigma^2 = M_n. 
        \end{split}
    \end{equation*}
    Hence $M_n$ is a martingale. 
\end{example}

\begin{example}[Exponential Martingale]
    Let $\xi_i$ be independent and identically distributed with $M(t) = \E\sbrc{\exp(t\xi_i)}<\infty$ 
    for given $t$. Put $X_i = \frac{\exp(t\xi_i)}{M(t)}$ and thus $\E\sbrc{X_i} = 1$. 
    Let $M_n = \prod_{i\leq n}X_i$. $M_n$ is adapted to the natural filtration $\F_n$ generated 
    by $X_n$ and 
    \begin{equation*}
        \E\sbrc{\abs{M_n}} = \E\sbrc{\prod_{i\leq n}X_i} = \prod_{i\leq n}\E\sbrc{X_i} = 1. 
    \end{equation*}
    Also, 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = M_n\E\sbrc{X_{n+1}|\F_n} = M_n\E\sbrc{X_{n+1}} = M_n. 
    \end{equation*}
    Hence $M_n$ is a martingale. 
\end{example}

\begin{lemma}
    Let $X_t$ be a $\F_t$-martingale and $\varphi$ is a function such that 
    $\E\sbrc{\abs{\varphi(X_t)}}<\infty$. 
    \begin{thmenum}
        \item If $\varphi$ is convex, then $\varphi(X_t)$ is a submartingale. 
        \item If $\varphi$ is concave, then $\varphi(X_t)$ is a supermartingale
    \end{thmenum}
\end{lemma}
\begin{proof}
    The proof for (b) is similar to (a). We only prove the case (a). Let $s>t$. 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \geq \varphi(\E\sbrc{X_s|\F_t}) = \varphi(X_t).
    \end{equation*}
    Hence $\varphi(X_t)$ is a submartingale. 
\end{proof}

\begin{lemma}\label{lem:mart_convex}
    Let $X_t$ be a stochastic process and $\varphi$ is an increasing function such that 
    $\E\sbrc{\abs{\varphi(X_t)}}<\infty$. 
    \begin{thmenum}
        \item If $X_t$ is a $\F_t$-submartingale and $\varphi$ is convex, then 
        $\varphi(X_t)$ is a $\F_t$-submartingale. 
        \item If $X_t$ is a $\F_t$-supermartingale and $\varphi$ is concave, then 
        $\varphi(X_t)$ is a $\F_t$-supermartingale.  
    \end{thmenum}
\end{lemma}
\begin{proof}
    For (a), let $s>t$. 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \geq \varphi(\E\sbrc{X_s|\F_t}) \geq \varphi(X_t).
    \end{equation*}
    Hence $\varphi(X_t)$ is a submartingale.

    For (b), 
    \begin{equation*}
        \E\sbrc{\varphi(X_s)|\F_t} \leq \varphi(\E\sbrc{X_s|\F_t}) \leq \varphi(X_t). 
    \end{equation*}
    Hence $\varphi(X_t)$ is a supermartingale.
\end{proof}

\begin{definition}
    Let $X_n$ be a stochastic process and $\F_n$ be the filtration generated by $X_n$. 
    A stochastic process $H_n$ is said to be \textbf{predictable} if $H_{n+1}$ is $\F_{n}$-adapted. 
\end{definition}

\begin{definition}
    Let $X_n$, $Y_n$ be two discrete-time stochastic processes. The \textbf{discrete-time 
    stochastic integral} is defined as 
    \begin{equation*}
        (X\cdot Y)_n = \sum_{i\leq n} X_i(Y_i - Y_{i-1}). 
    \end{equation*}
\end{definition}

\begin{theorem}\label{thm:no_arbitrage}
    If $X_n$ is a $\F_n$-supermartingale and $H_n\geq 0$ is predictable and is bounded 
    for each $n$. Then $(H\cdot X)_n$ is a supermartingale.  
\end{theorem}
\begin{proof}
    First, it is clear that $(H\cdot X)_n$ is $\F_n$-adapted since the discrete-time stochastic
    integral is a function of $(H_1,\ldots H_n,X_0,\ldots,X_n)$. Second, since $H_n$ is 
    bounded, say by $c_i$, 
    \begin{equation*}
        \E\sbrc{\abs{(H\cdot X)_n}} \leq \sum_{i\leq n}\E\sbrc{\abs{H_i}\abs{X_{i}-X_{i-1}}} 
        \leq \sum_{i\leq n}c_i\max_{1\leq i\leq n}2\E\sbrc{\abs{X_i}} < \infty
    \end{equation*}
    for given $n$ since $X_n$ is a supermartingale and satisfies that $X_n\in\L^1$. 
    Finally, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{(H\cdot X)_{n+1}|\F_n} &= (H\cdot X)_n + \E\sbrc{H_{n+1}(X_{n+1}-X_n)|\F_n} \\ 
            &= (H\cdot X)_n + H_{n+1}\E\sbrc{X_{n+1}|\F_n} - H_{n+1}X_n \\ 
            &\leq (H\cdot X)_n + H_{n+1}X_n - H_{n+1}X_n = (H\cdot X)_n.
        \end{split}
    \end{equation*}
    Hence $(H\cdot X)_n$ is a supermartingale. 
\end{proof}
\begin{remark}
    If we instead assume that $X_n$ is a $\F_n$-submartingale, then $(H\cdot X)_n$ is a submartingale. 
    Furthermore, if $X_n$ is a $\F_n$-martingale, then for every predictable $H_n$, $(H\cdot X)_n$ is 
    a martingale. 
\end{remark}

\begin{definition}
    A random variable $\tau\in T$ is a \textbf{stopping time} if $\set{\tau\leq t}\in\F_t$. 
\end{definition}

\begin{example}[Hitting Time]
    Let $X_t$ be a stochastic process and $\F_t$ be the natural filtration generated by $X_t$. 
    The hitting time of $A$ is $\tau = \inf\Set{s\in T}{X_s\in A}$. Then $\tau$ is a stopping 
    time. 
\end{example}

\begin{proposition}
    Suppose that $\tau_1$ and $\tau_2$ are stopping times with respect to $\F_t$. 
    Then $\tau_1\wedge\tau_2$ and $\tau_1\vee\tau_2$ are stopping times. 
\end{proposition}
\begin{proof}
    Notice that 
    \begin{equation*}
        \set{\tau_1\wedge\tau_2\leq t} = \set{\tau_1\leq t}\cup\set{\tau_2\leq t}\in\F_t, 
        \quad\text{and}\quad 
        \set{\tau_1\vee\tau_2\leq t} = \set{\tau_1\leq t}\cap\set{\tau_2\leq t}\in\F_t.
    \end{equation*}
    Hence $\tau_1\wedge\tau_2$ and $\tau_1\vee\tau_2$ are stopping times. 
\end{proof}
\begin{remark}
    Since any constant $n$ is also a stopping time, $n\wedge\tau$ is a 
    stopping time provided that $\tau$ is. The conclusion from \cref{thm:no_arbitrage} 
    also applies to the strategy of the form $H_n' = H_{n\wedge \tau}$. In particular, 
    taking the predictable process 
    \begin{equation*}
        H_n = \begin{cases}
            1 & \tau \leq n \\ 
            0 & \tau > n
        \end{cases}
    \end{equation*}
    shows that $X_{n\wedge\tau}$ is a martingale provided that $X_n$ is.
\end{remark}

\begin{definition}
    Let $X_t$ be a stochastic process with index set $T$ and $a<b$ be constants. 
    Given $\omega$, the \textbf{$[a,b]$-crossing time} $N_a^b(\omega)$ is defined as the supremum 
    over $n\in\Z_+$ such that there is $s_1<t_1<s_2<t_2<\cdots<s_n<t_n$, $s_i, t_i\in T$ 
    with $X_{s_1}(\omega)\leq a$ and $X_{t_1}(\omega)\geq b$.
\end{definition}

\begin{lemma}[Doob's Upcrossing Lemma]
    Let $X_n$ be a submartingale on $\Z_+$ and $N_a^b(n)$ be the crossing time up to $n\in\Z_+$. 
    Then 
    \begin{equation*}
        \E\sbrc{N_a^b(n)}\leq \frac{1}{b-a}\E\sbrc{(X_n-a)^+}
    \end{equation*}
    for all $n\in\Z_+$. 
\end{lemma}
\begin{proof}
    Let $Y_n = a+(X_n-a)^+$. By \cref{lem:mart_convex}, $Y_n$ is also a submartingale 
    since $x\mapsto a+(x-a)^+$ is a convex increasing function. Notice that the 
    crossing time up to time $n$ for $Y_n$ is also $N_a^b(n)$. Now define the 
    stopping times $0 = \tau_0 \leq \sigma_1 < \tau_1 < \cdots$ by 
    \begin{equation*}
        \sigma_k = \inf\Set{n\geq\tau_{k-1}}{Y_n = a}, \quad\text{and}\quad 
        \tau_k = \inf\Set{n\geq \sigma_k}{Y_n \geq b}. 
    \end{equation*}
    Next, we consider the predictable process 
    \begin{equation*}
        H_n = \sum_{k\geq 1}\one\set{\sigma_k< n\leq \tau_k}.
    \end{equation*}
    By \cref{thm:no_arbitrage}, $((1-H)\cdot Y)_n$ is a submartingale and thus 
    \begin{equation*}
        \E\sbrc{((1-H)\cdot Y)_n} \geq \E\sbrc{((1-H)\cdot Y)_0} = 0.
    \end{equation*}
    Notice that $(H\cdot Y)_n\geq (b-a)N_a^b(n)$. We have 
    \begin{equation*}
        \E\sbrc{(X_n-a)^+} = \E\sbrc{(H\cdot Y)_n} + \E\sbrc{((1-H)\cdot Y)_n} 
        \geq (b-a)\E\sbrc{N_a^b(n)}.
    \end{equation*}
    Rearranging the inequality gives the desired result. 
\end{proof}

\begin{theorem}[Martingale Convergence Theorem \rom{1}]
    Let $X_n$ be a $\F_n$-submartingale. Suppose that $\sup_n\E\sbrc{X_n^+}<\infty$. 
    Then there is $X\in\L^1$ such that $X_n\to X$ almost surely. 
\end{theorem}
\begin{proof}
    For $a, b\in\Q$, consider the event $A_{a,b} = \set{\liminf_nX_n < a < b < \limsup_n X_n}$. 
    Put $S$ be the union of all $A_{a,b}$ over $a,b\in\Q$. For given $a<b$, consider the 
    crossing time $N_a^b$ on $\Z_+$ and $N_a^b(n)$ which is up to time $n$. 
    Clearly $N_a^b(n)\nearrow N_a^b$. By LMCT and Doob's upcrossing lemma,
    \begin{equation*}
        \E\sbrc{N_a^b} = \lim_{n\to\infty}\E\sbrc{N_a^b(n)} \leq \limsup_{n\to\infty}\frac{1}{b-a}\E\sbrc{(X_n-a)^+}\leq C\sup_n\E\sbrc{X_n^+}
    \end{equation*}
    for some constant $C$. Hence $\P(N_a^b<\infty) = 0$. This implies that $X_n$ crosses 
    $[a,b]$ only finite times almost surely. Thus $\P(A_{a,b}) = 0$ and $\P(S) = 0$. 
    We see that 
    \begin{equation*}
        \P(\liminf_n X_n<\limsup_n X_n) = 0.
    \end{equation*}
    Thus the limit of $X_n$ exists, possibly in extended sense, almost surely. 
    Denote the limit of $X_n$ as $X$. It now remains to show that $X\in\L^1$. 
    \begin{equation*}
        \E\sbrc{X^+} = \E\sbrc{\lim_{n\to\infty}X_n^+}
        \leq \liminf_{n\to\infty}\E\sbrc{X_n^+}\leq\sup_{n}\E\sbrc{X_n^+} < \infty
    \end{equation*}
    by Fatou's lemma. Also, $X_n^- = X_n^+ - X_n$. 
    \begin{equation*}
        \E\sbrc{X_n^-} = \E\sbrc{X_n^+} - \E\sbrc{X_n} \leq \E\sbrc{X_n^+} - \E\sbrc{X_0} < \infty. 
    \end{equation*}
    Hence $X\in\L^1$. The proof is complete. 
\end{proof}
\begin{remark}
    The theorem also holds for supermartingale. If $X_n$ is a $\F_n$-supermartingale 
    and $\sup_n\E\sbrc{X_n^-}<\infty$. Then there is $X\in\L^1$ such that $X_n\to X$ 
    almost surely. 
\end{remark}

\begin{corollary}
    If $X_n\geq 0$ is a supermartingale, then there is $X\in\L^1$ such that 
    $X_n\to X$ almost surely and $\E\sbrc{X}\leq \E\sbrc{X_0}$. 
\end{corollary}
\begin{proof}
    Since $\sup_n\E\sbrc{X_n^-} = 0$, the martingale convergence theorem implies 
    that there is $X\in\L^1$ such that $X_n\to X$ almost surely. By Fatou's 
    lemma, 
    \begin{equation*}
        \E\sbrc{X} = \E\sbrc{\liminf_{n\to\infty} X_n} 
        \leq \liminf_{n\to\infty}\E\sbrc{X_n}\leq \liminf_{n\to\infty}\E\sbrc{X_0} = \E\sbrc{X_0}. 
    \end{equation*}
    The proof is complete.  
\end{proof}
\begin{remark}
    The hard bound on $X_n$ is neccessary. Consider $\xi_i$ with 
    $\P(\xi_i = \pm 1) = 1/2$. Let $X_n = \sum_{i\leq n}\xi_i$ and 
    $X_0 = 1$. Consider the stopping time $\tau = \inf\Set{n}{X_n = 0}$.  
    Then $\P(\tau < \infty) = 1$. Since $X_{n\wedge \tau}$ is a martingale, 
    $\E\sbrc{X_{n\wedge\tau}} = X_0 = 1$. Also, the martingale convergence 
    theorem shows that there is $X$ where $X_{n\wedge\tau}\to X$ as $n\to\infty$. 
    Since $\tau<\infty$ almost surely, we have $X = 0$ almost surely and 
    $\E\sbrc{X} = 0$. 
\end{remark}

\begin{example}[P\'olya's Urn]
    Let $X_0,Y_0\in \N$. Define 
    \begin{equation*}
        (X_n, Y_n) = \begin{cases}
            (X_{n-1}+1, Y_{n-1}),& \text{prob.} = \frac{X_{n-1}}{X_{n-1} + Y_{n-1}}, \\
            (X_{n-1}, Y_{n-1}+1),& \text{prob.} = \frac{Y_{n-1}}{X_{n-1} + Y_{n-1}}.
        \end{cases}
    \end{equation*}
    Let $Z_n = X_n/(X_n + Y_n)$. Notice that 
    \begin{equation*}
        \begin{split}
            \E\sbrc{Z_{n+1}|\F_n} &= Z_n\cdot \frac{X_n+1}{X_n+1+Y_n} + (1 - Z_n)\cdot \frac{X_n}{X_n+Y_n+1} 
            = \frac{X_n}{X_n+Y_n}\cdot\frac{1}{X_n+1+Y_n} + \frac{X_n}{X_n+Y_n+1} \\ 
            &= \frac{X_n}{X_n+Y_n} = Z_n. 
        \end{split}
    \end{equation*}
    Hence $Z_n$ is a martingale. Since $Z_n$ is clearly bounded, the martingale 
    convergence theorem implies that there is some $Z\in\L^1$ such that 
    $Z_n\to Z$ almost surely. In fact, one can show that $Z\sim\text{Beta}(X_0, Y_0)$. 
\end{example}

\begin{theorem}[Optional Stopping Theorem]
    Let $\tau$ be a stopping time and $X_n$ be a martingale. Then $\E\sbrc{\abs{X_\tau}}<\infty$ 
    and $\E\sbrc{X_\tau} = \E\sbrc{X_0}$ if one of the followings holds: 
    \begin{thmenum}
        \item There is some $N$ such that $\tau\leq N$ almost surely. 
        \item There is some $M$ such that $\abs{X_{\tau\wedge n}}\leq M$ and $\P(\tau<\infty) = 1$. 
        \item $\E\sbrc{\tau}<\infty$ and $\abs{X_{n+1}-X_n}\leq M$ almost surely for some $M$. 
    \end{thmenum}
\end{theorem}
\begin{proof}
    For (a), we have that 
    \begin{equation*}
        \abs{X_{n\wedge \tau} - X_0} \leq \sum_{k=1}^{n\wedge\tau} \abs{X_{k}-X_{k-1}}\leq \sum_{k=1}^N \abs{X_{k}-X_{k-1}}\in\L^1
    \end{equation*}
    for every $n$. Hence by the LDCT, since $\tau<\infty$ almost surely and $X_{n\wedge\tau}\to X_{\tau}$, 
    \begin{equation*}
        \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}
    \end{equation*}
    by the fact that $X_{n\wedge\tau}$ is a martingale by \cref{thm:no_arbitrage}. 

    For (b), by the bounded convergence theorem and that $\tau<\infty$ almost surely, 
    \begin{equation*}
        \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}. 
    \end{equation*}

    For (c), notice that 
    \begin{equation*}
        \abs{X_{n\wedge\tau} - X_0} \leq \sum_{k=1}^{n\wedge\tau}\abs{X_{k}-X_{k-1}}\leq M\tau\in\L^1
    \end{equation*}
    since $\tau\in\L^1$. Thus $\abs{X_{n\wedge\tau}}\leq M\tau\in\L^1$. LDCT implies that 
    \begin{equation*}
            \E\sbrc{X_{\tau}} = \E\sbrc{\lim_{n\to\infty}X_{n\wedge\tau}} = \lim_{n\to\infty}\E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_0}.
    \end{equation*}
\end{proof}

\begin{corollary}[Wald's Identity]
    Let $X_i$ be independent and identically distributed with $\E\sbrc{X_i} = \mu < \infty$. 
    $\abs{X_i}\leq K$ almost surely. Let $S_n = \sum_{i\leq n}X_i$, $S_0 = 0$ and $\tau$ be a 
    stopping time with $\E\sbrc{\tau}<\infty$. Then $\E\sbrc{S_{\tau}} = \mu\E\sbrc{\tau}$. 
\end{corollary}
\begin{proof}
    Consider $M_n = S_n - n\mu$. Notice that 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = \E\sbrc{X_{n+1} - \mu|\F_n} + S_n - n\mu = M_n.
    \end{equation*}
    Thus $M_n$ is a martingale. Also, 
    \begin{equation*}
        \abs{M_{n+1}-M_n} = \abs{X_{n+1}-\mu}\leq K+\abs{\mu}
    \end{equation*}
    almost surely. By the optional stopping theorem (c), 
    \begin{equation*}
        \E\sbrc{S_{\tau} - \tau\mu} = \E\sbrc{M_\tau} = \E\sbrc{M_0} = 0.
    \end{equation*}
    Hence $\E\sbrc{S_{\tau}} = \mu\E\sbrc{\tau}$. 
\end{proof}

\begin{example}
    Consider a random walk $X_n = \sum_{i\leq n}\xi_i$ where $\P(\xi_i = \pm 1) = 1/2$ 
    are independent and $X_0 = x\in\Z$. Define the hitting time $\tau = \inf\Set{k}{X_k = a,b}$ 
    where $a<x<b$, $a,b\in\Z$. We first show that 
    \begin{equation*}
        \P(X_\tau = a) = \frac{b-x}{b-a}. 
    \end{equation*} 
    Since $X_n$ is a martingale, $X_{n\wedge\tau}$ is again a martingale by 
    \cref{thm:no_arbitrage}. Thus 
    \begin{equation*}
        \begin{split}
            x &= \E\sbrc{X_{n\wedge\tau}} = \E\sbrc{X_\tau\one\set{n\geq \tau}} + \E\sbrc{X_n\one\set{n<\tau}} \\
            &= a\P(X_\tau = a, \tau\leq n) + b\P(X_\tau = b, \tau\leq n) + \E\sbrc{X_n\one\set{n<\tau}}.
        \end{split}
    \end{equation*}
    Notice that $\abs{X_n}\one\set{n<\tau}\leq \abs{a}\vee\abs{b}$ and $\abs{X_n}\one\set{n<\tau}\to 0$ 
    since $\tau<\infty$ almost surely. Letting $n\to\infty$ gives 
    \begin{equation*}
        x = a\P(X_\tau = a) + b\P(X_\tau = b) = a\P(X_\tau = a) + b(1 - \P(X_\tau = a)), \quad\Rightarrow\quad 
        \P(X_\tau = a) = \frac{b-x}{b-a}. 
    \end{equation*}
    Next, we show that 
    \begin{equation*}
        \E\sbrc{\tau} = (b-x)(x-a). 
    \end{equation*}
    Consider $M_n = X_n^2 - n$. Then clearly the integrability and measurability holds. Also, 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = \E\sbrc{X_n^2 + 2X_n\xi_{n+1} + \xi_{n+1}^2 - (n+1)|\F_n} = M_n. 
    \end{equation*}
    Hence $M_n$ is a martingale. Again, by \cref{thm:no_arbitrage}, $M_{n\wedge\tau}$ 
    is still a martingale. 
    \begin{equation*}
        \begin{split}
            x^2 &= \E\sbrc{M_{n\wedge\tau}} = \E\sbrc{X_\tau^2\one\set{\tau\leq n}} + \E\sbrc{X_n^2\one\set{n< \tau}} - \E\sbrc{n\wedge\tau} \\ 
            &= a^2\P(X_\tau = a, \tau\leq n) + b^2\P(X_\tau = b, \tau\leq n) + \E\sbrc{X_n^2\one\set{n< \tau}} - \E\sbrc{n\wedge\tau}.
        \end{split}
    \end{equation*}
    Notice that $\tau<\infty$ almost surely and thus $n\wedge\tau\nearrow\tau$. 
    Also, $X_n^2\one\set{n<\tau}<(a^2\vee b^2)$ and $X_n^2\one\set{n<\tau}\to 0$ as $n\to\infty$. 
    By the LMCT and LDCT, taking $n\to\infty$, 
    \begin{equation*}
        x^2 = a^2\P(X_\tau = a) + b^2\P(X_\tau = b) - \E\sbrc{\tau} = a^2\frac{b-x}{b-a} + b^2\frac{x-a}{b-a} - \E\sbrc{\tau}.
    \end{equation*}
    Hence $\E\sbrc{\tau} = (b-x)(x-a)$. 
\end{example}

\begin{theorem}[Doob's Decomposition]
    Let $X_n$ be an integrable and $\F_n$-adapted process. Then there is an 
    almost surely unique decomposition $X_n = M_n + A_n$ where $M_n$ is an 
    $\F_n$-martingale and $A_n$ is $\F_n$-predictable with $A_0 = 0$. In 
    particular, if $X_n$ is a submartingale, then $A_n$ is non-decreasing 
    almost surely.  
\end{theorem}
\begin{proof}
    Put $A_n = \sum_{k\leq n}\E\sbrc{X_k|\F_{k-1}}-X_{k-1}$ with $A_0 = 0$ 
    and $M_n = X_n - A_n$. Notice that $A_n$ is $\F_{n-1}$ measurable and hence 
    $\F_n$ predictable. Also, $M_n$ is integrable and $\F_n$-adapted. We verify 
    the martingale condition: 
    \begin{equation*}
        \begin{split}
            \E\sbrc{X_n|\F_{n-1}} &= \E\sbrc{M_n|\F_{n-1}} + A_n 
            = \E\sbrc{M_n|\F_{n-1}} + A_{n-1} + \E\sbrc{X_n|\F_{n-1}} - X_{n-1} \\ 
            &= \E\sbrc{M_n|\F_{n-1}} - M_{n-1} + \E\sbrc{X_n|\F_{n-1}}.
        \end{split}
    \end{equation*} 
    Rearranging the equation shows that $M_{n-1} = \E\sbrc{M_n|\F_{n-1}}$. Thus 
    $M_n$ is indeed a martingale. 

    To see the uniqueness, suppose that $X_n = M_n' + A_n'$ is another decomposition. 
    Then 
    \begin{equation*}
        \E\sbrc{X_n|\F_{n-1}} = M_{n-1} + A_n = X_{n-1} + A_n - A_{n-1} 
        = X_{n-1} + A_n' - A_{n-1}'. 
    \end{equation*}
    Since $A_0 = A_0' = 0$ and $A_n-A_{n-1} = A_n'-A_{n-1}'$ for all $n$, it follows 
    that $A_n$ are unique and also the $M_n$.   

    Now suppose that $X_n$ is a submartingale, 
    \begin{equation*}
        A_{n-1} + M_{n-1} = X_{n-1}\leq \E\sbrc{X_n|\F_{n-1}} 
        = M_{n-1} + A_n.
    \end{equation*}
    Hence $A_{n-1}\leq A_n$ almost surely for all $n$. 
\end{proof}

\begin{example}
    Let $X_n$ be independent with $\E\sbrc{X_i} = 0$ and $\sigma_i^2 = \E\sbrc{X_i^2} < \infty$. 
    Let $S_n = \sum_{i\leq n}X_i$ be a martingale. Then $S_n^2$ is a submartingale. 
    Consider the Doob's decomposition $S_n^2 = M_n + A_n$. We claim that 
    $M_n = S_n^2 - \sum_{i\leq n}\sigma_i^2$ and $A_n = \sum_{i\leq n}\sigma_i^2$ 
    is the Doob's decomposition for $S_n^2$. First, the integrability and adaptedness 
    of $M_n$ are clear. 
    \begin{equation*}
        \E\sbrc{M_{n+1}|\F_n} = S_n^2 + 2S_n\E\sbrc{X_{n+1}|\F_n} + \E\sbrc{X_{n+1}^2|\F_n} - \sum_{i\leq n+1}\sigma_i^2 
        = S_n^2 - \sum_{i\leq n}\sigma_i^2 = M_n.
    \end{equation*}
    Thus $M_n$ is a martingale. Also, $A_n$ is increasing almost surely and predictable. 
    We conclude that $M_n$ and $A_n$ are the desired decomposition.  
\end{example}

\begin{theorem}\label{thm:martingale_limits}
    $X_n$ is a martingale with $\abs{X_{n+1}-X_n}\leq M$. Let 
    \begin{equation*}
        C = \set{\lim_{n\to\infty}X_n \text{ exists and is finite}}, \quad 
        D = \set{\liminf_{n\to\infty}X_n = -\infty,\text{ and } \limsup_{n\to\infty}X_n = \infty}.
    \end{equation*}
    Then $\P(C\cup D) = 1$. 
\end{theorem}
\begin{proof}
    Without loss of generality, we assume that $X_0 = 0$. For fixed $k>0$, define 
    the stopping time $\tau = \inf\Set{n}{X_n\leq k}$. Since the increments of 
    $X_n$ are bounded, we have $X_{n\wedge\tau} \geq -k-M$. Since $X_{n\wedge\tau}$ is 
    a martingale, by the martingale convergence theorem $X_{n\wedge\tau}\to X$ for some 
    $X\in\L^1$ almost surely. On $\set{\tau = \infty}$, $\lim_{n\to\infty}X_n 
    = \lim_{n\to\infty}X_{n\wedge\tau} = X$ is finite. Now taking $k\to\infty$, 
    then on the event $\set{\liminf_{n\to\infty}X_n>-\infty}$, $\lim_{n\to\infty}X_n$ exists 
    and is finite almost surely. The same argument for $-X_n$ shows that on 
    $\set{\limsup_{n\to\infty}<\infty}$, $\lim_{n\to\infty}X_n$ exists and is finite. 
    Combining the results, 
    \begin{equation*}
        D^c = \set{\liminf_{n\to\infty}X_n>-\infty}\cup\set{\limsup_{n\to\infty}<\infty}\subset C
    \end{equation*} 
    with some exception of measure zero. Hence $\P(C\cup D) = 1$.
\end{proof}

\begin{theorem}[Borel-Cantelli \rom{3}]
    Let $\F_n$ be a filtration with $\F_0 = \set{\varnothing,\Omega}$ and $A_n\in\F_n$. Then 
    \begin{equation*}
        \P\set{A_n\text{ i.o.}} = \P\set{\sum_n \P(A_n|\F_{n-1}) = \infty}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $X_0 = 0$ and $X_n = \sum_{k\leq n}\one_{A_k}-\P(A_k|\F_{k-1})$. Notice that 
    $X_n$ is bounded and $\F_n$-adapted. Also, 
    \begin{equation*}
        \E\sbrc{X_{n+1}|\F_n} = X_n + \E\sbrc{\one_{A_{n+1}}|\F_n} - \P(A_{n+1}|\F_n) = X_n.
    \end{equation*}
    Thus $X_n$ is a martingale with bounded increments. By \cref{thm:martingale_limits}, 
    \begin{equation*}
        C = \set{\lim_{n\to\infty}X_n \text{ exists and is finite}}, \quad\text{and}\quad  
        D = \set{\liminf_{n\to\infty}X_n = -\infty,\text{ and } \limsup_{n\to\infty}X_n = \infty}
    \end{equation*}
    satisfy that $\P(C\cup D) = 1$. Notice that $\set{\sum_n\one_{A_n} = \infty} = \set{A_n\text{ i.o.}}$. 
    On $C$, $\sum_n\one_{A_n} = \infty$ if and only if $\sum_n \P(A_n|\F_{n-1}) = \infty$. 
    On $D$, we have $\sum_n\one_{A_n} = \infty$ and $\sum_n\P(A_n|\F_{n-1}) = \infty$. 
    Since $\P(C\cup D) = 1$, we conclude that $\set{A_n \text{ i.o.}} = \set{\sum_n \P(A_n|\F_{n-1}) = \infty}$ 
    up to some measure zero sets and the result follows.
\end{proof}

\begin{lemma}\label{lem:stop_submart_inequality}
    Let $X_n$ be a submartingale and $\tau$ be a stopping time with $\tau\leq T$. 
    Then 
    \begin{equation*}
        \E\sbrc{X_0}\leq \E\sbrc{X_\tau}\leq \E\sbrc{X_T}. 
    \end{equation*}
\end{lemma}
\begin{proof}
    Note that $X_{n\wedge\tau}$ is again a submartingale. Then 
    \begin{equation*}
        \E\sbrc{X_0} = \E\sbrc{X_{0\wedge\tau}} \leq \E\sbrc{\E\sbrc{X_{T\wedge\tau}|\F_0}}
        = \E\sbrc{X_\tau}. 
    \end{equation*}
    Also, for fixed $0\leq k\leq T$, 
    \begin{equation*}
        \E\sbrc{X_\tau\one\set{\tau = k}} = \E\sbrc{X_k\one\set{\tau = k}} 
        \leq \E\sbrc{\E\sbrc{X_T|\F_k}\one\set{\tau = k}} 
        = \E\sbrc{X_T\one\set{\tau = k}}.
    \end{equation*}
    Summing over $0\leq k\leq T$ gives $\E\sbrc{X_\tau} \leq \E\sbrc{X_T}$. 
\end{proof}

\begin{theorem}[Doob's Submatingale Inequality]
    Let $X_n$ be a submartingale and $\lambda>0$. Define $X_n^* = \max_{0\leq k\leq n}X_k^+$. 
    Then 
    \begin{equation*}
        \P\set{X_n^*\geq \lambda} \leq \frac{1}{\lambda}\E\sbrc{X_n\one\set{X_n^*\geq \lambda}} 
        \leq \frac{1}{\lambda}\E\sbrc{X_n^+}. 
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $A = \set{X_n^*\geq \lambda}$ and $\tau = \inf\Set{k}{X_k\geq \lambda}\wedge n$. 
    On $A$, $X_\tau\geq \lambda$ and hence 
    \begin{equation*}
        \P\set{X_n^*\geq \lambda} = \E\sbrc{\one_A} \leq \frac{1}{\lambda}\E\sbrc{X_\tau\one_A}. 
    \end{equation*}
    Notice that by \cref{lem:stop_submart_inequality}, $\E\sbrc{X_\tau}\leq\E\sbrc{X_n}$. 
    Also, $X_\tau\one_{A^c} = X_n\one\tau_{A^c}$. Thus $\E\sbrc{X_\tau\one_A}\leq \E\sbrc{X_n\one_A}$. 
    The first inequality holds. The second inequality is trivial since 
    $X_n\one_A\leq X_n^+\one_A\leq X_n^+$. The proof is complete. 
\end{proof}

\begin{corollary}
    Let $M_n$ be a martingale and $\E\sbrc{\abs{M_n}^p}<\infty$ for some $p\geq 1$. 
    For any $\lambda > 0$, 
    \begin{equation*}
        \P\set{\max_{0\leq k\leq n}\abs{M_k}\geq \lambda} \leq \frac{1}{\lambda^p}\E\sbrc{\abs{M_n}^p}. 
    \end{equation*}
\end{corollary}
\begin{proof}
    For $p\geq 1$, $x\mapsto \abs{x}^p$ is convex and thus $\abs{M_n}^p$ is a submartingale. 
    By the Doob's submartingale inequality, 
    \begin{equation*}
        \P\set{\max_{0\leq k\leq n}\abs{M_k}\geq \lambda} 
        = \P\set{\max_{0\leq k\leq n}\abs{M_k}^p\geq \lambda^p} 
        \leq \frac{1}{\lambda^p}\E\sbrc{\abs{M_k}^p}.
    \end{equation*}
\end{proof}

\begin{theorem}[$\L^p$ Maximal Inequality]
    Let $X_n$ be a submartingale and $X_n^* = \max_{0\leq k\leq n}X_k^+$. For $1<p<\infty$, 
    we have 
    \begin{equation*}
        \E\sbrc{\abs{X_n^*}^p} \leq \pth{\frac{p}{p-1}}^p\E\sbrc{X_n^+}^p.
    \end{equation*}
    Furthermore, if $X_n$ is a martingale, then 
    \begin{equation*}
        \E\sbrc{\pth{\max_{0\leq k\leq n}\abs{X_k}}^p} \leq \pth{\frac{p}{p-1}}^p\E\sbrc{\abs{X_n}^p}. 
    \end{equation*}
\end{theorem}
\begin{proof}
    Suppose first that $X_n^*\in\L^p$. By \cref{lem:expectation_tail_prob}, Doob's submartingale 
    inequality, Fubini theorem and H\"older's inequality, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{(X_n^*)^p} &= \int_0^\infty p\lambda^{p-1}\P(X_n^*\geq \lambda)d\lambda 
            \leq \int_0^\infty p\lambda^{p-1}\frac{1}{\lambda}\E\sbrc{X_n\one\set{X_n^*\geq\lambda}}d\lambda \\ 
            &\leq \int_0^\infty p\lambda^{p-2}\E\sbrc{X_n^+\one\set{X_n^*\geq\lambda}}d\lambda 
            = p\E\sbrc{X_n^+\int_0^{X_n^*}\lambda^{p-2}d\lambda} \\
            &= \frac{p}{p-1}\E\sbrc{X_n^+(X_n^*)^{p-1}} 
            \leq \frac{p}{p-1}\E\sbrc{\abs{X_n^+}^p}^{1/p}\E\sbrc{\abs{X_n^*}^p}^{(p-1)/p}.
        \end{split}
    \end{equation*}
    Rearranging the inequality gives 
    \begin{equation*}
        \E\sbrc{\abs{X_n^*}^p} \leq \pth{\frac{p}{p-1}}^p\E\sbrc{(X_n^+)^p}.
    \end{equation*}
    For general $X_n$, consider $Y_n\in\L^p$ such that $Y_n\nearrow X_n$. 
    Since $Y_n^*\in\L^p$ as well, the result extends to $X_n$ by the monotone 
    convergence theorem. For the second part, apply the previous result to the 
    submartingale $\abs{X_n}$. Then 
    \begin{equation*}
        \E\sbrc{\pth{\max_{0\leq k\leq n}X_k}^p}
        \leq \pth{\frac{p}{p-1}}^p\E\sbrc{\abs{X_n}^p}. 
    \end{equation*}
    This finishes the proof. 
\end{proof}

\begin{example}
    Let $S_n$ be a symmetric simple random walk on $\Z$ with $S_0 = 1$. Let 
    $\tau = \inf\Set{k}{S_k = 0}$ be the hitting time. Put $X_n = S_{n\wedge\tau}$. 
    Then 
    \begin{equation*}
        \P\set{\sup_{k\geq 0} X_k \geq M} = \frac{1}{M}.
    \end{equation*}
    To see this, note that by the martingale convergence theorem, $X_k$ converges 
    to some $X\in\L^1$ almost surely since $X_k$ is bounded. By the optional stopping 
    theorem, for the hitting time $N = \inf\Set{k}{X_k = M\text{ or }X_k = 0}$,
    \begin{equation*}
        1 = \E\sbrc{S_0} = \E\sbrc{X_0} = \E\sbrc{X_N} = M\P(X_N = M) = M\P\set{\sup_{k\geq 0} X_k \geq M}.
    \end{equation*}  
    Rearranging the equality shows the desired equation. Now, by the LMCT, 
    \begin{equation*}
        \lim_{n\to\infty} \E\sbrc{\max_{k\leq n}\abs{X_k}} \geq \lim_{n\to\infty} \E\sbrc{\max_{k\leq n}X_k} 
        = \E\sbrc{\sup_{k\geq 0} X_k} 
        = \sum_{M\geq 1} \P\set{\sup_{k\geq 0} X_k\geq M} = \sum_{M\geq 1}\frac{1}{M} = \infty. 
    \end{equation*} 
    However, $\E\sbrc{X_n} = \E\sbrc{S_{n\wedge\tau}} = \E\sbrc{S_0} = 1$. 
    We see that the $\L^p$ inequality fails for $p = 1$. 
\end{example}

\begin{theorem}[Martingale Convergence Theorem \rom{2}]
    Let $X_n$ be a martingale and $\sup_n\E\sbrc{\abs{X_n}^p}<\infty$ for some 
    $p>1$. Then $X_n\to X$ almost surely and in $\L^p$.  
\end{theorem}
\begin{proof}
    First, note that $\abs{X_n}^p$ is a submartingale. The almost sure convergence 
    follows immediately from the martingale convergence theorem \rom{1}. Next, since 
    $\abs{X_n - X}^p\leq 2\sup_n\abs{X_n}^p$ and by LMCT and the $\L^p$ martingale 
    inequality, 
    \begin{equation*}
        \E\sbrc{\sup_n\abs{X_n}^p} 
        = \lim_{n\to\infty}\E\sbrc{\max_{0\leq k\leq n}\abs{X_k}^p} 
        \leq \lim_{n\to\infty}\pth{\frac{p}{p-1}}^p\E\sbrc{\abs{X_n}^p} 
        \leq \pth{\frac{p}{p-1}}^p\sup_n\E\sbrc{\abs{X_n}^p}<\infty.
    \end{equation*}
    We see that $2\sup_n\abs{X_n}^p$ is integrable and hence by LDCT, $X_n\to X$ 
    in $\L^p$. 
\end{proof}