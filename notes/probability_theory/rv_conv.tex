\begin{definition}
    A sequence of probability measures $\P_n$ on $(\R^n, \B(\R^n))$ 
    are  \textbf{consistent} if 
    \begin{equation*}
        \P_{n+1}((a_1, b_1]\times\cdots\times(a_n, b_n]\times\R 
        = \P_n((a_1, b_1]\times\cdots\times(a_n, b_n])
    \end{equation*}
    for every $n$. 
\end{definition}

\begin{theorem}[Kolmogorov Extension]
    Suppose that a sequence of probability measures $\P_n$ on $(\R^n, \B(\R^n))$ 
    are consistent. Then there is a unique probability measure $\P$ on $(\R^\N, \B)$ 
    satisfying that 
    \begin{equation*}
        \P(\Set{\omega\in\R^\N}{\omega_i\in(a_i, b_i], 1\leq i\leq n}) = \P_n((a_1, b_1]\times\cdots\times(a_n, b_n]),  
    \end{equation*}
    where $\B$ is generated by the collection 
    \begin{equation*}
        \Set{\omega\in\R^\N}{\omega_i\in(a_i, b_i], 1\leq i\leq n, n\in\N}. 
    \end{equation*}
\end{theorem}
\begin{proof}
    Let 
    \begin{equation*}
        \S = \Set{(a_1, b_1]\times\cdots\times(a_n, b_n]\times\R\times\cdots}{n\in\N}. 
    \end{equation*}
    Define $\P$ on $\S$ to be 
    \begin{equation*}
        \P((a_1, b_1]\times\cdots\times(a_n, b_n]\times\R\times\cdots) = \P_n((a_1, b_1]\times\cdots\times(a_n, b_n])
    \end{equation*}
    Clearly, $\S$ forms a semi-algebra. From the Carath\'eodory 
    extension theorem, it suffices to show that $\P$ is finitely 
    additive, $\sigma$-additive on $\S$ and $\P(\varnothing) = 0$. 
    Note that $\P(\varnothing) = \P(\varnothing\times\R\times\cdots) 
    = \P_1(\varnothing) = 0$. We verify the first two conditions. 

    First, if $A,B\in\S$ are disjoint, $m\leq n$,   
    \begin{equation*}
        A = \Set{\omega\in\R^\N}{\omega_i\in (a_i, b_i], 1\leq i\leq m}\quad\text{and}\quad 
        B = \Set{\omega\in\R^\N}{\omega_i\in (c_i, d_i], 1\leq i\leq n}, 
    \end{equation*}
    then 
    \begin{equation*}
        \P(A\cup B) = \P_n((\pi_n A)\cup(\pi_n B)) = \P_n(\pi_n A) + \P_n(\pi_n B) 
        = \P(A) + \P(B), 
    \end{equation*}
    where $\pi_n:\omega\to(\omega_1,\ldots,\omega_n)$ is the projection onto 
    the first $n$ components. Hence $\P$ is finitely additive. 

    Next, suppose $A_1,\ldots\in\S$ are countably many disjoint measurable sets. 
    Put $A = \cup_i A_i$. We can consider the algebra $\bar{\S} 
    = \set{\text{finite disjoint union of sets in $\S$}}$ generated by $\S$. 
    $B_n = \cup_{i>n} A_i\in\bar{\S}$. Thus 
    \begin{equation*}
        \P(A) = \P(B_n) + \sum_{i=1}^n\P(A_n) 
    \end{equation*} 
    by the previous result. It now suffices to show that 
    $\P(B_n)\to 0$ for any $B_n\searrow \varnothing$. Suppose 
    not, then there is $\delta > 0$ such that $\P(B_n)\to \delta$ 
    as $B_n\to\varnothing$ by the monotonicity of $\P$. 

    For such $\set{B_n}$, we claim that there is a sequence 
    of compact set $K_n$ such that $K_n\subset B_n$ and $\P(B_n-K_n)<2^{-(n+1)}\delta$. 
    Now since $B_1\in\bar{S}$, there are disjoint 
    $E_1^1,\ldots,E_{m_1}^1$ such that $B_1 = \cup_{i=1}^{m_1} E_i^1$. 
    Now since each $E_i^1$ is of the product of $(\cdot,\cdot]$. We can 
    find a compact subset $K_i^1$ of the product of $[\cdot,\cdot]$ such that 
    $\P(E_i^1 - K_i^1)<m_1^{-1}2^{-2}\delta$. Hence $K_1 = \cup_i K_i^1\subset B_1$ 
    satisfies that 
    \begin{equation*}
        \P(B_1 - K_1) = \sum_{i=1}^{m_1}\P(E_i^1 - K_i^1) < 2^{-2}\delta
    \end{equation*}
    as desired. Repeat the process and find $K_n$ inductively. 
    The claim follows. 

    Now, $\cap_{n=1}^m K_n\searrow K$ as $m\to\infty$. Also, 
    \begin{equation*}
        \P(B_m - (\cap_{n=1}^m K_n)) \leq \sum_{n=1}^m \P(B_n - K_n) \leq \frac{\delta}{2}. 
    \end{equation*} 
    Hence $\delta/2 \leq \P(B_m) - \delta/2 \leq \P(\cap_{n=1}^m K_n)$. 
    We see that $\cap_{n=1}^m K_n$ is non-empty for each $m$. 
    But this implies that $K\subset \cap_n B_n$ is non-empty, 
    a contradiction. Thus $\P(B_n)\to 0$. 

    Finally, the $\sigma$-additivity follows from that we can 
    take $n\to\infty$ so that 
    \begin{equation*}
        \P(A) = \lim_{n\to\infty} \P(B_n) + \sum_{i=1}^n\P(A_n) 
        = \sum_i \P(A_n). 
    \end{equation*}
    Applying Carath\'eodory extension theorem, such $\P$ can be 
    extended on $(\R^\N, \B)$. 
\end{proof}
\begin{remark}
    With Kolmogorov extension theorem, we can consider a sequence 
    of independent variable $X_i$ on the product probability space with 
    $\F = \B$, $\tilde{X}_i:\omega\mapsto\omega_i$ and 
    $\P(B_1\times\cdots B_n) = \prod_{i=1}^n\mu_i(B_i)$, where 
    $\mu_i$ is the distribution of $X_i$. 
\end{remark}

\begin{definition}
    Let $X_n$ be a sequence of random variable. $X_n$ \textbf{converges 
    almost surely} to $X$ if 
    \begin{equation*}
        \P\set{\lim_{n\to\infty}X_n = X} = 1. 
    \end{equation*} 
    We denote it as $X_n\asto X$ or $X_n\to X$ a.s.
\end{definition}

\begin{definition}
    Let $X_n$ be a sequence of random variable. $X_n$ \textbf{converges 
    in probability} to $X$ if for every $\epsilon > 0$, 
    \begin{equation*}
        \P\set{\abs{X_n - X}>\epsilon} \to 0 
    \end{equation*} 
    as $n\to\infty$. We denote it as $X_n\pto X$. 
\end{definition}

\begin{definition}
    A sequence of random variable $X_n\in\L^p$ is 
    said to \textbf{converge in $\L^p$} to $X$ if 
    \begin{equation*}
        \E\sbrc{\abs{X_n - X}^p}^{1/p} \to 0
    \end{equation*}
    as $n\to\infty$. If $p=\infty$, the definition 
    becomes 
    \begin{equation*}
        \esssup_{\omega\in\Omega} \abs{X_n(\omega) - X(\omega)} \to 0.
    \end{equation*}
    We denote it as $X_n\to X$ in $\L^p$. 
\end{definition}

\begin{proposition}
    Let $X_n$ be a sequence of independent and indentically 
    distributed random variables. Then 
    \begin{thmenum}
        \item If $X_n\to X$ almost surely, then $X_n\pto X$. 
        \item If $X_n\to X$ in $\L^p$, then $X_n\pto X$.  
    \end{thmenum}
\end{proposition}
\begin{proof}
    For (a), given $\epsilon>0$, put 
    \begin{equation*}
        E_k = \cup_{n\geq k}\set{\abs{X_n - X}>\epsilon}.
    \end{equation*}
    Note that $E_k\searrow E = \set{\abs{X_n - X}>\epsilon\text{ for infinitely many $n$}} = \set{\lim_{n\to\infty} X_n = X}^c$. 
    Hence 
    \begin{equation*}
        \P\set{\abs{X_k - X}>\epsilon} \leq \P(E_k) \to \P\set{\lim_{n\to\infty} X_n = X}^c = 0
    \end{equation*}
    Hence $X_n\to X$ in probability.  

    For (b), suppose first that $p<\infty$. By Markov inequality, 
    \begin{equation*}
        \P\set{\abs{X_n - X}>\epsilon} = \P\set{\abs{X_n-X}^p>\epsilon^p}
        \leq \frac{1}{\epsilon^p}\E\sbrc{\abs{X_n-X}^p}\to 0. 
    \end{equation*}
    Let $p = \infty$. Note that $\esssup\abs{X_n - X} = \inf\Set{c}{\P\set{\abs{X_n - X}>c} = 0}$. 
    Convergence in $\L^\infty$ implies that for $\epsilon>0$, there is 
    $N$ such that if $n\geq N$, $\inf\Set{c}{\P\set{\abs{X_n - X}>c} = 0} < \epsilon$. 
    That is, $\P\set{\abs{X_n - X}>\epsilon} = 0$ for $n\geq N$. Hence 
    $X_n\pto X$. 
\end{proof}