\begin{theorem}[Central Limit Theorem \rom{1}, Lindeberg-Levy]
    Let $X_i$ be independent and identically distributed 
    random variables with $\E\sbrc{X_i} = 0$ and $\E\sbrc{X_i^2} = 1$. 
    Let $X\sim N(0,1)$ and $S_n = \sum_{i\leq n} X_i$. Then 
    \begin{equation*}
        \frac{1}{\sqrt{n}}S_n\dto X
    \end{equation*}    
    as $n\to\infty$. 
\end{theorem}
\begin{proof}
    Notice that $\varphi_X(t) = \exp(-t^2/2)$ and by \cref{cor:char_taylor}, 
    \begin{equation*}
        \varphi_{\frac{1}{\sqrt{n}}S_n}(t) = \prod_{i=1}^n\varphi_{X_i}\pth{\frac{t}{\sqrt{n}}} 
        = \pth{1 - \frac{t^2}{2n} + o(1/n)}^n\to \exp(-t^2/2)
    \end{equation*}
    as $n\to\infty$ for fixed $t$. Hence $\frac{1}{\sqrt{n}}S_n\dto X$ 
    by the Levy's continuity theorem.  
\end{proof}

\begin{theorem}[Central Limit Theorem \rom{2}, Lindeberg-Feller]
    For each $n$, let $X_{n,i}$, $1\leq i\leq n$ be random variables 
    with mean $0$ and $S_n = \sum_{i\leq n}X_{n,i}$. Suppose
    \begin{thmenum}
        \item For given $n$, $X_{n,i}$ are independent. 
        \item $\Var\sbrc{\sum_{i=1}^n X_{n,i}} = \sum_{i=1}^n\E\sbrc{X_{n,i}^2} \to \sigma^2$.
        as $n\to\infty$. 
        \item For all $\epsilon>0$, $\lim_{n\to\infty}\sum_{i=1}^n\E\sbrc{X_{n,i}^2\one\set{\abs{X_{n,i}}>\epsilon}} = 0$.
    \end{thmenum}
    Then $S_n\dto N(0, \sigma^2)$. 
\end{theorem}
\begin{proof}
    We begin the proof of the theorem by the following claim. 
    For $t\in\R$, we have 
    \begin{equation*}
        \abs{e^{it} - \sum_{k=0}^n\frac{(it)^{k}}{k!}} \leq \min\set{\frac{2\abs{t}^n}{n!}, \frac{\abs{t}^{n+1}}{(n+1)!}}. 
    \end{equation*}
    To see this, let $g_n(t)$ be the difference on the left 
    hand side. For $n = 0$, the inequality holds trivially. 
    Now suppose that the inequality holds for $n$. 
    \begin{equation*}
        g_{n+1}(t) = e^{it} - \sum_{k=0}^{n+1} \frac{(it)^{k}}{k!}
        = i\int_{0}^t e^{is} - \sum_{k=0}^n\frac{(is)^{k}}{k!}ds 
        = i\int_0^t g_n(s)ds.
    \end{equation*} 
    Now 
    \begin{equation*}
        \abs{g_{n+1}(t)} \leq \int_0^t\frac{2\abs{s}^n}{n!}ds 
        = \frac{2\abs{t}^{n+1}}{(n+1)!} 
        \quad\text{and}\quad 
        \abs{g_{n+1}(t)} \leq \int_0^t\frac{\abs{s}^{n+1}}{(n+1)!}ds 
        = \frac{\abs{t}^{n+2}}{(n+2)!}.
    \end{equation*}
    The claim follows by induction. 

    Now notice that for given $n$, if $Z_{n,i}\sim N(0, \sigma_{n,i}^2)$ 
    are independent with $\sigma_{n,i}^2 = \E\sbrc{X_{n,i}^2}$, then 
    $\sum_{i\leq n} Z_{n,i}\dto Z\sim N(0, \sigma^2)$. Let the 
    characteristic functions for $Z_{n,i}$ be $\psi_{n,i}$. Note that 
    for complex numbers $z_i$ and $w_i$ with $\abs{z_i}, \abs{w_i}\leq 1$, 
    we have 
    \begin{equation*}
        \abs{\prod_{i\leq n}z_i - \prod_{i\leq n}w_i} \leq \sum_{i\leq n} \abs{z_i - w_i}.
    \end{equation*}
    If $n = 2$, $\abs{z_1z_2 - w_1w_2} \leq \abs{z_1z_2 - z_1w_2} + \abs{z_1w_2 - w_1w_2} 
    \leq \abs{z_2-w_2} + \abs{z_1 - w_1}$. The general case follows by 
    induction. It follows that if $\varphi_{n,i}$ are characteristic functions 
    for $X_{n,i}$,  
    \begin{equation*}
        \begin{split}
            \abs{\prod_{i\leq n}\varphi_{n,i}(t) - \prod_{i\leq n}\psi_{n,i}(t)} 
            &\leq \sum_{i\leq n} \abs{\varphi_{n,i}(t) - \psi_{n,i}(t)} \\
            &\leq \sum_{i\leq n} \abs{\varphi_{n,i}(t) - 1 + \frac{\sigma_{n,i}^2t^2}{2}} + \sum_{i\leq n} \abs{\psi_{n,i}(t) - 1 + \frac{\sigma_{n,i}^2t^2}{2}} \\ 
            &\leq \sum_{i\leq n} \E\sbrc{\abs{e^{itX_{n,i}} - 1 + \frac{X_{n,i}^2t^2}{2}}} + \sum_{i\leq n} \E\sbrc{\abs{e^{itZ_{n,i}} - 1 + \frac{Z_{n,i}^2t^2}{2}}}. 
        \end{split}
    \end{equation*}
    Now by the claim we have for all $\epsilon>0$, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{\abs{e^{itX_{n,i}} - 1 + \frac{X_{n,i}^2t^2}{2}}} 
            &\leq \E\sbrc{\min\set{\frac{2\abs{tX_{n,i}}^2}{2!}, \frac{\abs{tX_{n,i}}^{3}}{3!}}} \\
            &\leq \abs{t}^2\E\sbrc{\abs{X_{n,i}}^2\one\set{\abs{X_{n,i}}>\epsilon}} + \frac{\abs{t}^3}{6}\E\sbrc{\abs{X_{n,i}}^3\one\set{\abs{X_{n,i}}\leq \epsilon}} \\ 
            &\leq \abs{t}^2\E\sbrc{\abs{X_{n,i}}^2\one\set{\abs{X_{n,i}}>\epsilon}} + \frac{\epsilon\abs{t}^3}{6}\E\sbrc{\abs{X_{n,i}}^2\one\set{\abs{X_{n,i}}\leq \epsilon}} \\ 
            &\leq \abs{t}^2\E\sbrc{\abs{X_{n,i}}^2\one\set{\abs{X_{n,i}}>\epsilon}} + \frac{\epsilon\abs{t}^3\sigma_{n,i}^2}{6}
        \end{split}
    \end{equation*}
    Let $\sigma_n^2 = \sum_{i\leq n}\sigma_{n,i}^2$. The claim applies 
    to $Z_{n,i}$ gives 
    \begin{equation*}
        \begin{split}
            \sum_{i\leq n}\E\sbrc{\abs{e^{itZ_{n,i}} - 1 + \frac{Z_{n,i}^2t^2}{2}}} 
            &\leq \frac{\abs{t}^3}{6}\sum_{i\leq n}\E\sbrc{\abs{Z_{n,i}}^3} 
            = \frac{\abs{t}^3}{6}\sum_{i\leq n}\sigma_{n,i}^3\E\sbrc{\abs{Y}^3} \\
            &\leq \frac{\abs{t}^3}{6}\E\sbrc{\abs{Y}^3}\sigma_n^2\sup_{i\leq n}\sigma_{n,i}
        \end{split}
    \end{equation*} 
    Hence 
    \begin{equation*}
        \begin{split}
            \abs{\prod_{i\leq n}\varphi_{n,i}(t) - \prod_{i\leq n}\psi_{n,i}(t)} 
            &\leq \sum_{i\leq n} \abs{t}^2\E\sbrc{\abs{X_{n,i}}^2\one\set{\abs{X_{n,i}}>\epsilon}} + \frac{\epsilon\abs{t}^3\sigma_{n,i}^2}{6}
            + \frac{\abs{t}^3}{6}\E\sbrc{\abs{Y}^3}\sigma_n^2\sup_{i\leq n}\sigma_{n,i}\\
            &\to \frac{\epsilon\abs{t}^3}{6}\sigma^2 + \frac{\abs{t}^3}{6}\E\sbrc{\abs{Y}^3}\sigma^2\epsilon
        \end{split}
    \end{equation*}
    as $n\to\infty$ since 
    \begin{equation*}
        \begin{split}
            \sup_{i\leq n}\sigma_{n,i} &= \pth{\sup_{i\leq n}\sigma_{n,i}^2}^{1/2} 
            = \pth{\sup_{i\leq n} \E\sbrc{X_{n,i}^2\one\set{\abs{X_{n,i}}>\epsilon}} + \E\sbrc{X_{n,i}^2\one\set{\abs{X_{n,i}}\leq \epsilon}}}^{1/2} \\ 
            &\leq \pth{\epsilon^2 + \sum_{i\leq n} \E\sbrc{X_{n,i}^2\one\set{\abs{X_{n,i}}>\epsilon}}}^{1/2}\to\epsilon
        \end{split}
    \end{equation*}
    as $n\to\infty$. Since $\epsilon$ is arbitrary, we conclude that $\abs{\prod_{i\leq n}\varphi_{n,i}(t) - \prod_{i\leq n}\psi_{n,i}(t)} \to 0$ 
    and $S_n\dto N(0,\sigma^2)$. 
\end{proof}

\begin{corollary}[Central Limit Theorem \rom{3}, Lyapunov]
    Let $X_j$ be independent with $\E\sbrc{X_j} = 0$ and $\alpha_n^2 = \sum_{j\leq n}\Var(X_j)$. 
    Suppose that there is $\delta>0$ such that $\E\sbrc{\abs{X_j}^{2+\delta}}<\infty$ and 
    \begin{equation*}
        \lim_{n\to\infty} \frac{1}{\alpha_n^{2+\delta}}\sum_{j\leq n}\E\sbrc{\abs{X_j}^{2+\delta}} = 0. 
    \end{equation*}
    Then 
    \begin{equation*}
        \frac{1}{\alpha_n}\sum_{j\leq n} X_j\dto N(0,1). 
    \end{equation*}
\end{corollary}
\begin{proof}
    Put $Y_{n,j} = \alpha_n^{-1}X_j$. Note that 
    \begin{equation*}
        \Var\sbrc{\sum_{j\leq n}Y_{n,j}} = \frac{1}{\alpha_n^2}\cdot\alpha_n^2 = 1
    \end{equation*}
    and for $\epsilon>0$, 
    \begin{equation*}
        \sum_{j\leq n}\E\sbrc{Y_{n,j}^2\one\set{\abs{Y_{n,j}}>\epsilon}} 
        = \frac{1}{\alpha_n^2}\sum_{j\leq n}\E\sbrc{X_{n,j}^2\one\set{\abs{X_{n,j}}^{\delta}>(\alpha_n\epsilon)^{\delta}}} 
        \leq \frac{1}{\alpha_n^{2+\delta}\epsilon^{\delta}}\sum_{j\leq n}\E\sbrc{\abs{X_{n,j}}^{2+\delta}} \to 0
    \end{equation*}
    as $n\to\infty$. Hence by the Lindeberg-Feller theorem, $\alpha_n^{-1}\sum_{j\leq n} X_j\dto N(0,1)$. 
\end{proof}

\begin{example}
    Let $X_j\sim \Ber(p_j)$ be independent random variables and $S_n = \sum_{j\leq n}X_j$. Then 
    \begin{equation*}
        \frac{S_n - \E\sbrc{S_n}}{\sqrt{\Var(S_n)}} \dto N(0,1)
    \end{equation*}
    if $\Var(S_n)\to \infty$. To see this, write $Y_j = X_j - p_j$ and  
    $\sigma_j^2 = \Var(Y_j) = \E\sbrc{Y_j^2} = p_j(1-p_j)$. Let $\delta>0$ be given. 
    Since $\abs{Y_j}\leq 1$, $\E\sbrc{\abs{Y_j}^{2+\delta}}\leq \E\sbrc{Y_j^2} = \sigma_j^2$ 
    and 
    \begin{equation*}
        \frac{1}{\Var(S_n)^{2+\delta}}\sum_{j\leq n}\E\sbrc{\abs{Y_j}^{2+\delta}} 
        \leq \frac{1}{\Var(S_n)^{2+\delta}}\sum_{j\leq n}\sigma_j^2 = \Var(S_n)^{-\delta}\to 0. 
    \end{equation*}
    By the Lyapunov theorem, we conclude that 
    \begin{equation*}
        \frac{S_n - \E\sbrc{S_n}}{\sqrt{\Var(S_n)}} \dto N(0,1). 
    \end{equation*}
\end{example}

\begin{theorem}
    For given $n$, $X_{n,j}\sim\Ber(p_{n,j})$ with $1\leq j\leq n$ are independent. 
    Let $S_n = \sum_{j\leq n}X_{n,j}$. If 
    \begin{thmenum}
        \item $\sum_{j\leq n} p_{n,j}\to \lambda\in(0,\infty)$ 
        \item $\max_{j\leq n} p_{n,j}\to 0$, 
    \end{thmenum}
    then $S_n\dto \Poisson(\lambda)$. 
\end{theorem}
\begin{proof}
    Let $\varphi_{n,j}(t) = 1 + p_{n,j}(e^{it} - 1)$ be the characteristic functions 
    for $X_{n,j}$. Then 
    \begin{equation*}
        \begin{split}
            \log(\varphi_{S_n}(t)) &= \sum_{j=1}^n\log(\varphi_{n,j}(t))
            = \sum_{j=1}^n\log(1 + p_{n,j}(e^{it} - 1)) \\
            &= \sum_{j=1}^np_{n,j}(e^{it} - 1) + o(p_{n,j}) 
            \to \lambda(e^{it}-1), 
        \end{split}
    \end{equation*}  
    where we use the fact that $\log(1+x) = x + o(x)$ as $x\to 0$ and condition (b) implies 
    that the Taylor expansion is valid. Hence $\varphi_{S_n}(t)\to \exp(\lambda(e^{it}-1))$ 
    and $S_n\dto \Poisson(\lambda)$. 
\end{proof}

\begin{example}
    The independence assumption of the random variables can sometimes be relaxed. 
    Consider the random permutations of $\set{1,\ldots,n}$. We are interested in 
    finding the distribution of the number of fixed points for the random 
    permutations. Define $X_{n,j} = \one\set{\pi(j) = j}$ where $\pi$ is a 
    random permutation and $S_n = \sum_{j\leq n}X_{n,j}$. Put $A_{n,j} = \set{X_{n,j}=1}$. 
    Then 
    \begin{equation*}
        \P(S_n>0) = \P(\cup_{j=1}^nA_{n,j}) 
        = \sum_j\P(A_{n,j}) - \sum_{k\leq j} \P(A_{n,j}\cap\P(A_{n,k})) + \cdots + (-1)^{n+1}\P(A_{n,1}\cap\cdots\cap A_{n,n}).
    \end{equation*}
    Note that 
    \begin{equation*}
        \P(A_{n,i_1}\cap\cdots\cap A_{n,i_r}) = \frac{(n-r)!}{n!}.
    \end{equation*}
    Hence 
    \begin{equation*}
        \P(S_n>0) = n\cdot\frac{(n-1)!}{n!} - \binom{n}{2}\frac{(n-2)!}{n!} + \cdots + (-1)^{n+1}\frac{1}{n!} 
        = 1 - \frac{1}{2!} + \frac{1}{3!} - \cdots + (-1)^{n+1}\frac{1}{n!}.
    \end{equation*}
    Thus $\P(S_n = 0) = 1 - 1 + \frac{1}{2!} \cdots + (-1)^{n}\frac{1}{n!} \to e^{-1}$ as 
    $n\to\infty$. Fix $k$ entries. The remaining $n-k$ entries with no fixed points 
    has combinatorics $a_{n-k}$. Thus 
    \begin{equation*}
        \P(S_n = k) = \binom{n}{k}\frac{a_{n-k}}{n!}, 
        \quad\text{and}\quad 
        \P(S_{n-k} = 0) = \frac{a_{n-k}}{(n-k)!}.  
    \end{equation*}
    So 
    \begin{equation*}
        \P(S_n = k) = \binom{n}{k}\frac{(n-k)!\P(S_{n-k} = 0)}{n!}\to \frac{1}{k!}e^{-1}
    \end{equation*}
    for given $k$. We see that $S_n\dto \Poisson(1)$. 
\end{example}