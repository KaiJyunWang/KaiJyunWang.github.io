\begin{definition}
    Let $X_i$ be random variables with $\E\sbrc{X_i^2}<\infty$. They are called 
    \textbf{uncorrelated} if 
    \begin{equation*}
        \E\sbrc{X_iX_j} = \E\sbrc{X_i}\E\sbrc{X_j}. 
    \end{equation*} 
\end{definition}

\begin{theorem}[Weak Law of Large Number \rom{1}]
    Suppose that $X_n$ are uncorrelated random variables with $\Var\sbrc{X_n}\leq C\infty$ 
    and $\E\sbrc{X_n} = \mu$ for all $n$. Let $S_n = \sum_{i=1}^n X_i$. Then 
    \begin{equation*}
        \frac{1}{n}S_n\to \mu
    \end{equation*}
    in $\L^2$ and hence in probability. 
\end{theorem}
\begin{proof}
    Compute that 
    \begin{equation*}
        \E\sbrc{\pth{\frac{1}{n}S_n-\mu}^2} = \frac{1}{n^2}\sum_{i=1}^n\Var(X_i) 
        \leq \frac{C}{n}\to 0. 
    \end{equation*}
    Hence $\frac{1}{n}S_n\to\mu$ in $\L^2$ and thus in probability. 
\end{proof}

\begin{theorem}[Weak Law of Large Number \rom{2}, Khinchin]
    Suppose that $X_i$ is a sequence of independent and identically distributed random variables 
    with $\E\sbrc{\abs{X_1}}<\infty$. Let $S_n = \sum_{i=1}^n X_i$ and $\mu = \E\sbrc{X_1}$. 
    Then 
    \begin{equation*}
        \frac{1}{n}S_n\to\mu
    \end{equation*}
    in $\L^1$ and hence in probability. 
\end{theorem}
\begin{proof}
    By replacing $X_i$ with $X_i-\mu$, we may assume without loss of generality 
    that $\mu = 0$. Now, for $C>0$, 
    \begin{equation*}
        0 = \E\sbrc{X_i} = \E\sbrc{X_i\one\set{\abs{X_i}>C}} + \E\sbrc{X_i\one\set{\abs{X_i}\leq C}}. 
    \end{equation*}
    Also, 
    \begin{equation*}
        \begin{split}
            \frac{1}{n}S_n &= 
            \frac{1}{n}\sum_{i=1}^nX_i\one\set{\abs{X_i}>C}  
            + \frac{1}{n}\sum_{i=1}^nX_i\one\set{\abs{X_i}\leq C} \\ 
            &= \frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}>C} - \E\sbrc{X_i\one\set{\abs{X_i}>C}}} 
            + \frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}\leq C} - \E\sbrc{X_i\one\set{\abs{X_i}\leq C}}}. 
        \end{split}
    \end{equation*}
    Notice that by LDCT,
    \begin{equation*}
        \E\sbrc{\abs{\frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}>C} - \E\sbrc{X_i\one\set{\abs{X_i}>C}}}}} 
        \leq 2\E\sbrc{\abs{X_1}\one\set{\abs{X_1}>C}}\to 0
    \end{equation*}
    as $C\to\infty$ since $\abs{X_1}\one\set{\abs{X_1}>C}\leq\abs{X_1}$ and 
    $\E\sbrc{\abs{X_1}}<\infty$. Also, by H\"older inequality and the independence,  
    \begin{equation*}
        \E\sbrc{\abs{\frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}\leq C} - \E\sbrc{X_i\one\set{\abs{X_i}\leq C}}}}} 
        \leq \sqrt{\frac{1}{n}\Var(X_i\one\set{\abs{X_i}\leq C})} \leq \frac{C}{\sqrt{n}} 
    \end{equation*}
    For any given $\epsilon>0$, there is $C$ such that $2\E\sbrc{\abs{X_1}\one\set{\abs{X_1}>C}} < \epsilon$ and 
    \begin{equation*}
        \begin{split}
            \E\sbrc{\abs{\frac{1}{n}S_n}} &\leq \E\sbrc{\abs{\frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}>C} - \E\sbrc{X_i\one\set{\abs{X_i}>C}}}}} \\
            &\quad + \E\sbrc{\abs{\frac{1}{n}\sum_{i=1}^n\pth{X_i\one\set{\abs{X_i}\leq C} - \E\sbrc{X_i\one\set{\abs{X_i}\leq C}}}}} \\ 
            &\leq \epsilon + \frac{C}{\sqrt{n}}\to\epsilon
        \end{split}
    \end{equation*}
    as $n\to\infty$. Since $\epsilon$ can be arbitrarily small, we conclude 
    that $\frac{1}{n}S_n\to 0$ in $\L^1$ and hence in probability. 
\end{proof}

\begin{definition}
    Let $A_n$ be a sequence of events. 
    \begin{equation*}
        \limsup_{n\to\infty} A_n = \cap_{m=1}^\infty\cup_{n=m}^\infty A_n
    \end{equation*}
    and 
    \begin{equation*}
        \liminf_{n\to\infty} A_n = \cup_{m=1}^\infty\cap_{n=m}^\infty A_n. 
    \end{equation*}
\end{definition}
\begin{remark}
    Observe that 
    \begin{equation*}
        \limsup_{n\to\infty} A_n 
        = \Set{\omega\in\Omega}{\omega\in A_n\text{ for infinitely many $n$}} 
    \end{equation*}
    and 
    \begin{equation*}
        \liminf_{n\to\infty}A_n 
        = \Set{\omega\in\Omega}{\omega\in A_n\text{ for all but finitely many $n$}}. 
    \end{equation*}
\end{remark}

\begin{theorem}[Borel-Cantelli \rom{1}]
    Let $A_n$ be a sequence of events. If $\sum_n \P(A_n)<\infty$, then 
    \begin{equation*}
        \P\pth{\limsup_{n\to\infty}A_n} = 0.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $\epsilon>0$ be given. By assumption, there is $n_0$ such that 
    $\sum_{n\geq n_0}\P(A_n)<\epsilon$. Then 
    \begin{equation*}
        \P\pth{\limsup_{n\to\infty}A_n} = \P(\cap_{m=1}^\infty\cup_{n=m}^\infty A_n) 
        \leq \P(\cup_{n=n_0}^\infty A_n)\leq \sum_{n=n_0}^\infty \P(A_n)<\epsilon. 
    \end{equation*}
    Since $\epsilon$ can be arbitrarily small, $\P\pth{\limsup_{n\to\infty}A_n} = 0$. 
\end{proof}

\begin{corollary}\label{cor:summable_asconv}
    Suppose for $\epsilon>0$, $\sum_n \P(\abs{X_n-X}>\epsilon)<\infty$. Then 
    $X_n\to X$ almost surely. 
\end{corollary}
\begin{proof}
    Let $E_k = \set{\abs{X_n-X}> k^{-1}\text{ for finitely many }n}$. Note that 
    $E_{k+1}\subset E_k$ and $E_k\searrow E = \set{X_n\to X}$. Now we claim that 
    $\P(E_k) = 1$. Consider $E_k^n = \set{\abs{X_n-X}>k^{-1}}$. For fixed $k$, 
    by assumption we have $\sum_{n}\P(E_k^n)<\infty$. By Borel-Cantelli, 
    $\P(\limsup_{n\to\infty}E_k^n) = 0$. Hence 
    \begin{equation*}
        \P(E_k) = \P(\set{\abs{X_n-X}>k^{-1}\text{ for infinitely many }n}^c) 
        = 1 - \P(\limsup_{n\to\infty}E_k^n) = 1.
    \end{equation*}
    It now follows by the monotone convergence of measures that $\P(E) = 1$. 
\end{proof}
\begin{remark}
    Intuitively, if the convergence is sufficiently fast, the convergence in 
    probability may recover almost sure convergence. 
\end{remark}

\begin{theorem}[Strong Law of Large Number \rom{1}]
    Let $X_i$ be independent and identically distributed with $\mu = \E\sbrc{X_1}$ 
    and $\E\sbrc{X_1^4}<\infty$. Let $S_n = \sum_{i=1}^nX_i$. Then 
    \begin{equation*}
        \frac{1}{n}S_n\to\mu
    \end{equation*}
    almost surely. 
\end{theorem}
\begin{proof}
    Note that 
    \begin{equation*}
        \begin{split}
            \E\sbrc{\pth{\frac{1}{n}S_n - \mu}^4} &= \frac{1}{n^4}\pth{\sum_i \E\sbrc{(X_i-\mu)^4} + \sum_{i\neq j}\E\sbrc{(X_i-\mu)^2(X_j-\mu)^2}} \\
            &\leq \frac{1}{n^3}\E\sbrc{(X_1-\mu)^4} + \frac{1}{n^4}\binom{n}{2}\binom{4}{2}\E\sbrc{(X_1-\mu)^2}^2 \leq \frac{C}{n^2}
        \end{split}
    \end{equation*}
    for some constant $C$. By Checyshev's inequality, for $\epsilon>0$, 
    \begin{equation*}
        \P\set{\abs{\frac{1}{n}S_n - \mu}>\epsilon} \leq \frac{1}{\epsilon^4}\E\sbrc{\pth{\frac{1}{n}S_n - \mu}^4}\leq\frac{C}{\epsilon^2n^2}
    \end{equation*}
    is absolute summable. Hence by \cref{cor:summable_asconv}, 
    \begin{equation*}
        \frac{1}{n}S_n\to \mu
    \end{equation*}
    almost surely. 
\end{proof}

\begin{theorem}\label{thm:prob_conv_subseq_as}
    $X_n\pto X$ if and only if every subsequence of $X_n$ has a further subsequence 
    converging almost surely. 
\end{theorem}
\begin{proof}
    Suppose first that $X_n\pto X$. Given a subsequence $X_{n(k)}$, we can 
    choose $n(k_1)<n(k_2)<\cdots$ such that 
    \begin{equation*}
        \P\pth{\abs{X_{n(k_i)}-X}>2^{-i}} < 2^{-i}. 
    \end{equation*} 
    Since $2^{-i}$ is summable, by Borel-Cantelli we have 
    \begin{equation*}
        \P\pth{\abs{X_{n(k_i)}-X}>2^{-i}\text{ for infinitely many $i$}} = 0. 
    \end{equation*}
    In other words, 
    \begin{equation*}
        \P\set{X_{n(k_i)}\to X} = \P\set{\abs{X_{n(k_i)}-X}>2^{-i}\text{ for infinitely many $i$}}^c 
        = 1. 
    \end{equation*}

    For the converse, suppose that $X_n\not\to X$ in probability. Then there exist 
    $\epsilon, \delta>0$ and 
    \begin{equation*}
        \P\set{\abs{X_{n(k)} - X}>\epsilon} \geq \delta. 
    \end{equation*}
    By assumption there is a further subsequence converging almost surely 
    and thus in probability, i.e., 
    \begin{equation*}
        \P\set{\abs{X_{n(k_j)} - X}>\epsilon} \to 0.
    \end{equation*}
    This is a contradiction. Hence $X_n\to X$ in probability. 
\end{proof}

\begin{corollary}
    Suppose $X_n\pto X$. Then the followings are true: 
    \begin{thmenum}
        \item If $f$ is continuous, then $f(X_n)\pto f(X)$. 
        \item If $\abs{X_n}\leq Y$ for some $Y\in\L^1$, then $\E\sbrc{X_n}\to\E\sbrc{X}$. 
    \end{thmenum}
\end{corollary}
\begin{proof}
    For (a), by \cref{thm:prob_conv_subseq_as}, every subsequence 
    has a further subsequence $X_{n(k_j)}\to X$ almost surely and 
    hence $f(X_{n(k_j)})\to f(X)$ almost surely. Then by \cref{thm:prob_conv_subseq_as} 
    again we see that $f(X_n)\pto f(X)$. 

    For (b), by \cref{thm:prob_conv_subseq_as}, every subsequence 
    has a furhter subsequence $X_{n(k_j)}\to X$ almost surely and 
    LDCT gives $\E\sbrc{X_{n(k_j)}}\to \E\sbrc{X}$. This implies 
    that $\E\sbrc{X_n}\to \E\sbrc{X}$ as well. 
\end{proof}

\begin{definition}
    Let $(\Omega,\F, \P)$ be a probability space. 
    \begin{equation*}
        \L^0(\Omega) = \Set{X:\Omega\to\R}{X\text{ is $\F$-measurable}}. 
    \end{equation*}
\end{definition}
\begin{remark}
    In general, the almost convergence notion on $\L^0$ is not metrizable, 
    i.e., there is no metric $d$ on $\L^0$ such that 
    \begin{equation*}
        d(X_n, X)\to 0
        \quad\Leftrightarrow\quad 
        X_n\to X\quad\text{a.s.}
    \end{equation*} 
    To see this, suppose that the almost sure convergence is metrizable. 
    If $X_n\pto X$, any subsequence $X_{n(k)}$ converges to $X$ in probability 
    as well. By \cref{thm:prob_conv_subseq_as}, we can find a further subsequence 
    converging almost surely and hence in metric $d$, but this implies that 
    $d(X_n, X)\to 0$. Then $X_n\to X$ almost surely, which is absurd since 
    convergence in probability does not imply almost sure convergence 
    in general. 

    However, convergence in probability on $\L^0$ can be metrized. For instance, 
    \begin{equation*}
        d(X,Y) = \E\sbrc{\max\set{\abs{X-Y}, 1}}. 
    \end{equation*}
\end{remark}

\begin{theorem}[Borel-Cantelli \rom{2}]
    Let $A_n$ be independent events and $\sum_{n=1}^\infty\P(A_n)=\infty$. 
    Then 
    \begin{equation*}
        \P\pth{\limsup_{n\to\infty} A_n} = 1. 
    \end{equation*}
\end{theorem}
\begin{proof}
    By assumption we have that $\sum_{n\geq m}\P(A_n) = \infty$ for every 
    $m\in\N$. Notice that $1+x\leq e^x$. Then  
    \begin{equation*}
        \begin{split}
            \P(\limsup_{n\to\infty}A_n) &= \lim_{m\to\infty}\P(\cup_{n\geq m}A_n) 
            = 1 - \lim_{m\to\infty}\P(\cap_{n\geq m}A_n^c) \\ 
            &= 1 - \lim_{m\to\infty}\lim_{N\to\infty}\P(\cap_{n= m}^NA_n^c) 
            = 1 - \lim_{m\to\infty}\lim_{N\to\infty}\prod_{n=m}^N\P(A_n^c) \\ 
            &= 1 - \lim_{m\to\infty}\prod_{n=m}^\infty(1 - \P(A_n)) 
            \geq 1 - \lim_{m\to\infty}\exp\pth{-\sum_{n=m}^\infty\P(A_n)}
            = 1. 
        \end{split}
    \end{equation*}
    Hence $\P(\limsup_{n\to\infty}A_n) = 1$. 
\end{proof}

\begin{lemma}\label{lem:expectation_tail_prob}
    Let $X$ be a non-negative random variable and $h:\R\to\R$ be a differentiable 
    function with $h(0)=0$ and $h'\geq 0$. Then 
    \begin{equation*}
        \E\sbrc{h(X)} = \int_0^\infty h'(t)\P(X>t)dt. 
    \end{equation*}
\end{lemma}
\begin{proof}
    By Fubini-Tonelli theorem, 
    \begin{equation*}
        \begin{split}
            \E\sbrc{h(X)} &= \E\sbrc{\int_0^X h'(t)dt} 
            = \E\sbrc{\int_0^\infty\one\set{t<X}h'(t)dt} \\
            &= \int_0^\infty h'(t)\E\sbrc{\one\set{t<X}}dt 
            = \int_0^\infty h'(t)\P(X>t)dt. 
        \end{split}
    \end{equation*}
\end{proof}

\begin{proposition}
    Suppose that $X_i$ are independent and identically distributed random variables with 
    $\E\sbrc{\abs{X_i}}=\infty$. Let $S_n = \sum_{i=1}^nX_i$. Then 
    \begin{thmenum}
        \item $\P\set{\abs{X_n}>n\text{ for infinitely many $n$}} = 1$. 
        \item $\P\set{\frac{1}{n}S_n\text{ has finite limit}} = 0$. 
    \end{thmenum}
\end{proposition}
\begin{proof}
    For (a), using \cref{lem:expectation_tail_prob} with $h$ being identity, 
    \begin{equation*}
        \begin{split}
            \infty &= \E\sbrc{\abs{X_1}} = \int_0^\infty\P(\abs{X_1}>t)dt  
            \leq \sum_{n=0}^\infty\int_n^{n+1}\P(\abs{X_1}>t)dt \\
            &\leq \sum_{n=0}^\infty\int_n^{n+1}\P(\abs{X_1}>n)dt 
            = \sum_{n=0}^\infty\P(\abs{X_n}>n). 
        \end{split}
    \end{equation*}
    Now by the second Borel-Cantelli, $\P\set{\abs{X_n}>n\text{ for infinitely many $n$}} = 1$. 

    For (b), consider $\omega$ with $\frac{S_n(\omega)}{n}\to Y(\omega)\in\R$. 
    Then for such $\omega$, 
    \begin{equation*}
        \frac{X_n}{n} = \frac{S_n - S_{n-1}}{n} = \frac{S_n}{n} - \frac{n-1}{n}\frac{S_{n-1}}{n-1}\to 0
    \end{equation*}
    as $n\to\infty$. Thus 
    \begin{equation*}
        \begin{split}
            \P\set{\frac{1}{n}S_n\text{ has finite limit}} &\leq \P\set{\abs{X_n}>n\text{ for finitely many }n} \\
            &= 1 - \P\set{\abs{X_n}>n\text{ for infinitely many $n$}} = 0.           
        \end{split}
    \end{equation*}
    (b) follows. 
\end{proof}

\begin{definition}
    A collection of $\sigma$-algebra $\set{\H_k}$ is \textbf{pairwise independent} 
    if for any $\H_1, \H_2\in\set{\H_k}$, 
    \begin{equation*}
        \P(A\cap B) = \P(A)\P(B)
    \end{equation*}
    for any $A\in\H_1$ and $B\in\H_2$. 
\end{definition}
\begin{remark}
    As before, a sequence of random variables $\set{X_k}$ is 
    pairwise independent if $\set{\sigma(X_k)}$ is. 
\end{remark}

\begin{theorem}[Strong Law of Large Number \rom{2}, Kolmogorov]
    Let $X_i$ be pairwise independent, identically distributed 
    random variables with $\E\sbrc{\abs{X_1}}<\infty$ and 
    $S_n = \sum_{i=1}^nX_i$. Then 
    \begin{equation*}
        \frac{1}{n}S_n \to \E\sbrc{X_1} = \mu
    \end{equation*}
    almost surely. 
\end{theorem}
\begin{proof}
    Since we can always decompose $X_i = X_i^+ - X_i^-$ and 
    $X_i^+, X_i^-$ satisfy the assumption of the theorem, 
    we may assume without loss of generality that $X_i\geq 0$. 
    Let $Y_i = X_i\one\set{X_i\leq i}$ and $T_n = \sum_{i=1}^nY_i$. 
    Let $\alpha > 1$ and put $k_n = \floor{\alpha^n}$. 
    By Chebyshev inequality, for any given $\epsilon>0$ we have 
    \begin{equation*}
        \begin{split}
            \sum_{n=1}^\infty \P\pth{\abs{\frac{T_{k_n} - \E\sbrc{T_{k_n}}}{k_n}}>\epsilon} 
            &\leq \frac{1}{\epsilon^2}\sum_{n=1}^\infty \frac{1}{k_n^2}\Var(T_{k_n}) \\
            &= \frac{1}{\epsilon^2}\sum_{n=1}^{\infty}\frac{1}{k_n^2}\sum_{i=1}^{k_n}\Var(Y_i) 
            = \frac{1}{\epsilon^2}\sum_{i=1}^\infty\Var(Y_i)\sum_{n:k_n\geq i}\frac{1}{k_n^2}. 
        \end{split}
    \end{equation*}
    Since $1/k^2$ is summable and $k_n$ repeat at most $m_\alpha$ times, where 
    $m_\alpha$ is an integer such that $\alpha^{m_\alpha+1}\geq \alpha^{m_\alpha} + 1$, 
    we can find a constant $c_\alpha>0$ such that 
    \begin{equation*}
        \sum_{n:k_n\geq i}\frac{1}{k_n^2}\leq \frac{c_\alpha}{i^2}. 
    \end{equation*} 
    Let $F$ be the distribution of $X$. We have 
    \begin{equation*}
        \begin{split}
            \sum_{n=1}^\infty \P\pth{\abs{\frac{T_{k_n} - \E\sbrc{T_{k_n}}}{k_n}}>\epsilon} 
            &\leq \frac{c_\alpha}{\epsilon^2}\sum_{i=1}^\infty\frac{\Var(Y_i)}{i^2} 
            \leq \frac{c_\alpha}{\epsilon^2}\sum_{i=1}^\infty\frac{\E\sbrc{Y_i^2}}{i^2} \\ 
            &= \frac{c_\alpha}{\epsilon^2}\sum_{i=1}^\infty\frac{1}{i^2}\int_0^ix^2dF(x) 
            = \frac{c_\alpha}{\epsilon^2}\sum_{i=1}^\infty\sum_{k=0}^{i-1}\frac{1}{i^2}\int_k^{k+1}x^2dF(x) \\ 
            &= \frac{c_\alpha}{\epsilon^2}\sum_{k=0}^\infty\pth{\sum_{i=k+1}^\infty\frac{1}{i^2}}\int_k^{k+1}x^2dF(x)
        \end{split}
    \end{equation*}
    Also, notice that there is a constant $C$ such that 
    \begin{equation*}
        \sum_{i=k+1}^\infty\frac{1}{i^2} \leq \frac{C}{k+1}. 
    \end{equation*}
    Hence, 
    \begin{equation*}
        \begin{split}
            \sum_{n=1}^\infty \P\pth{\abs{\frac{T_{k_n} - \E\sbrc{T_{k_n}}}{k_n}}>\epsilon} 
            &\leq \frac{c_\alpha C}{\epsilon^2}\sum_{k=0}^\infty\frac{1}{k+1}\int_k^{k+1}x^2dF(x) \\
            &\leq \frac{c_\alpha C}{\epsilon^2}\sum_{k=0}^\infty\int_k^{k+1}xdF(x) 
            = \frac{c_\alpha C}{\epsilon^2}\E\sbrc{X_1}<\infty. 
        \end{split}
    \end{equation*}
    Note that for $\delta>0$ there is an integer $M$ such that 
    $\E\sbrc{X_1\one\set{X_1> M}}\leq\delta\leq\E\sbrc{X_1}$. 
    \begin{equation*}
        \frac{\E\sbrc{T_{k_n}}}{k_n} = \frac{1}{k_n}\sum_{i=1}^{k_n} \E\sbrc{Y_i}
        \geq \frac{1}{k_n}\sum_{i=1}^{M} \E\sbrc{Y_i} + \frac{1}{k_n}\sum_{i=M+1}^{k_n}\E\sbrc{X_1}-\delta.
    \end{equation*}
    Also, 
    \begin{equation*}
        \frac{1}{k_n}\sum_{i=1}^{k_n}\E\sbrc{Y_i}\leq \frac{1}{k_n}\sum_{i=1}^{k_n}\E\sbrc{X_1} = \E\sbrc{X_1}. 
    \end{equation*}
    Taking $n\to\infty$ and since $\delta$ is arbitrary, we 
    conclude that 
    \begin{equation*}
        \frac{\E\sbrc{T_{k_n}}}{k_n}\to \E\sbrc{X_1}. 
    \end{equation*}
    Thus, by the Borel-Cantelli lemma, 
    \begin{equation*}
        \P\set{\frac{T_{k_n}}{k_n}\not\to\E\sbrc{X_1}} = \P\set{\abs{\frac{T_{k_n} - \E\sbrc{T_{k_n}}}{k_n}}>\epsilon\text{ for infinitely many $n$}} = 0. 
    \end{equation*}
    In other words, $T_{k_n}/k_n\to\E\sbrc{X_1}$ almost surely. 
    Also, 
    \begin{equation*}
        \begin{split}
            \sum_{k=1}^\infty \P\set{X_k\neq Y_k} &= \sum_{k=1}^\infty \P\set{X_k>k} 
            \leq \sum_{k=1}^\infty \int_{k-1}^k\P(X_1>t)dt \\
            &= \int_0^\infty \P(X_1>t)dt = \E\sbrc{X_1}<\infty
        \end{split}
    \end{equation*}
    by \cref{lem:expectation_tail_prob}. Hence by Borel-Cantelli 
    lemma, $X_k \neq Y_k$ for finitely many $k$ almost surely. 
    This implies that 
    \begin{equation*}
        \lim_{n\to\infty}\frac{1}{k_n}S_{k_n} = \lim_{n\to\infty}\frac{T_{k_n}}{k_n} = \E\sbrc{X_1}
    \end{equation*}
    almost surely. Note that $S_m$ is monotone and for each $m$, 
    we may find $k(n_m)\leq m\leq k(n_{m+1})$ so that 
    \begin{equation*}
        \frac{S_{k(n_m)}}{k(n_{m+1})}\leq \frac{S_m}{m} 
        \leq \frac{S_{k(n_{m+1})}}{k(n_m)}.
    \end{equation*} 
    Take $m\to\infty$, we conclude that 
    \begin{equation*}
        \frac{1}{\alpha}\mu \leq \liminf_{m\to\infty}\frac{S_m}{m} 
        \leq \limsup_{m\to\infty}\frac{S_m}{m}\leq \alpha\mu
    \end{equation*}
    almost surely. Taking $\alpha\to 1^+$ gives the desired result. 
\end{proof}

\begin{theorem}
    Let $X_i$ be independent and identically distributed with $\E\sbrc{X_1^+} = \infty$ 
    and $\E\sbrc{X_1^-} <\infty$. Let $S_n = \sum_{i=1}^n X_i$. Then 
    \begin{equation*}
        \frac{1}{n}S_n \to \infty 
    \end{equation*}
    almost surely. 
\end{theorem}
\begin{proof}
    Write $X_i = X_i^+ - X_i^-$. For $X_i^+$, consider $Y_i^M = \min\set{X_i^+, M}$ 
    for some $M>0$. Note that $Y_i^M$ is independent and identically distributed with 
    finite mean. By the strong law of large number, 
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^nY_i^M\to \E\sbrc{Y_1^M}
    \end{equation*}
    almost surely. Hence, 
    \begin{equation*}
        \liminf_{n\to\infty}\frac{1}{n}\sum_{i=1}^nX_i^+ \geq \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^nY_i^M = \E\sbrc{Y_1^M}. 
    \end{equation*}
    Notice that $Y_1^M\nearrow X_1^+$ as $M\to\infty$. By LMCT, 
    \begin{equation*}
        \lim_{M\to\infty}\E\sbrc{Y_1^M} = \E\sbrc{X_1^+} = \infty. 
    \end{equation*}
    We conclude that $\liminf_{n\to\infty}\frac{1}{n}\sum_{i=1}^nX_i^+ = \infty$. 
    On the other hand, by the strong law of large number, 
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^nX_i^-\to\E\sbrc{X_1^-}
    \end{equation*}
    almost surely. We end up with 
    \begin{equation*}
        \lim_{n\to\infty} \frac{1}{n}S_n = \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^nX_i^+ - \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^nX_i^- 
        = \infty
    \end{equation*}
    almost surely. 
\end{proof}

\begin{example}
    Let $Y_i$ be independent and identically distributed with density 
    \begin{equation*}
        f(y) = \one\set{y\geq 1}\frac{1}{c}\frac{1}{y^2}, 
    \end{equation*}
    where $c$ is some normalizing constant. Let $H_i\sim \Ber(2^{-i})$. 
    Put $X_i = Y_iH_i$. Then $\E\sbrc{X_i} = \infty$ for all $i$, but  
    since 
    \begin{equation*}
        \sum_i \P(X_i > 0) = \sum_i 2^{-i} <\infty, 
    \end{equation*}
    by the Borel-Cantelli lemma, $X_i\to 0$ almost surely 
    and 
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^n X_i \to 0
    \end{equation*} 
    almost surely. 
\end{example}

\begin{example}
    $Y\geq 0$ is a random variable with $\E\sbrc{Y} = \infty$. Put $X_i = Y$ 
    for all $i$. Then $X_i$ are identically distributed with $\E\sbrc{X_i} = \infty$. 
    But 
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^n X_i = Y\not\to \infty 
    \end{equation*}
    almost surely. 
\end{example}

\begin{example}[Event Streaks]
    $X_i\iidsim\Ber(2^{-1})$. Let $L_n$ be the longest streaks of $1$ in the first $n$ 
    trials. We have the following: 
    \begin{equation*}
        \lim_{n\to\infty}\frac{L_n}{\log_2(n)} = 1. 
    \end{equation*}
    To see this, let $\ell_n$ be the length of the current streaks. For 
    instance, the following sequence 
    \begin{equation*}
        1,0,1,1,1,1,0,\ldots 
    \end{equation*}
    generates $\ell_1 = 1, \ell_2 = 0, \ell_6 = 4$. Observe that $L_n = \max_{m\leq n}\ell_m$. 
    Now, 
    \begin{equation*}
        \P(\ell_n\geq k) = \sum_{m=k}^n \P(\ell_n = k) = \sum_{m=k}^n 2^{-k-1} 
        \leq 2^{-k}
    \end{equation*}
    as $n\to\infty$. For $\epsilon>0$, 
    \begin{equation*}
        \P(\ell_n \geq (1+\epsilon)\log_2(n)) = \P(\ell_n \geq \ceil{(1+\epsilon)\log_2(n)}) 
        \leq 2^{-\ceil{(1+\epsilon)\log_2(n)}} \leq 2^{-(1+\epsilon)\log_2(n)}
        = \frac{1}{n^{1+\epsilon}}
    \end{equation*}
    is summable. By the Borel-Cantelli lemma, 
    \begin{equation*}
        \P\set{\ell_n \geq (1+\epsilon)\log_2(n)\text{ for infinitely many $n$}} = 0. 
    \end{equation*}
    Hence 
    \begin{equation*}
        \P\set{\ell_n < (1+\epsilon)\log_2(n)\text{ for all but finitely many $n$}} = 1. 
    \end{equation*}
    That is, for almost every $\omega$, there is $N(\omega)$ such that 
    $\ell_n < (1+\epsilon)\log_2(n)$ for $n\geq N(\omega)$. For such $\omega$, 
    we have 
    \begin{equation*}
        L_n(\omega) = \max_{m\leq n}\ell_n(\omega) \leq\max_{m\leq n}(1+\epsilon)\log_2(n) = (1+\epsilon)\log_2(n)
    \end{equation*}
    as $n > N(\omega)$. Thus
    \begin{equation*}
        \limsup_{n\to\infty} \frac{L_n}{\log_2(n)}\leq 1+\epsilon
    \end{equation*}
    almost surely. Note that 
    \begin{equation*}
        \set{\limsup_{n\to\infty}\frac{L_n}{\log_2(n)}\leq 1+\epsilon} 
        \searrow 
        \set{\limsup_{n\to\infty}\frac{L_n}{\log_2(n)}\leq 1}
    \end{equation*}
    as $\epsilon\to 0^+$ and by the monotone convergence of the measures, 
    \begin{equation*}
        \limsup_{n\to\infty} \frac{L_n}{\log_2(n)}\leq 1
    \end{equation*}
    almost surely. 

    For the other side, note that for large $n$, we may split the sequence into 
    blocks of size $\ceil{(1-\epsilon)\log_2(n)}$ and 
    \begin{equation*}
        \frac{n}{\ceil{(1-\epsilon)\log_2(n)}} \geq \frac{n}{\log_2(n)}
    \end{equation*}
    for large $n$. 
    \begin{equation*}
        \begin{split}
            \P(L_n\leq (1-\epsilon)\log_2(n)) &\leq \P(\text{each block did not have all 1s}) \\
            &\leq (1-2^{-\ceil{(1-\epsilon)\log_2(n)/2}})^{n/\ceil{(1-\epsilon)\log_2(n)/2}} \\ 
            &\leq \pth{1 - \frac{1}{n^{1-\epsilon}}}^{n^{1-\epsilon}\frac{n^\epsilon}{\log_2(n)}} 
            \leq \exp\pth{-\frac{n^\epsilon}{\log_2(n)}},
        \end{split}
    \end{equation*}
    whcih is summable, so by the Borel Cantelli lemma, 
    \begin{equation*}
        \P\set{L_n\leq (1-\epsilon)\log_2(n)\text{ for infinitely many $n$}} = 0.
    \end{equation*}
    By a similar argument as above, 
    \begin{equation*}
        \liminf_{n\to\infty} \frac{L_n}{\log_2(n)} \geq 1-\epsilon
    \end{equation*}
    almost surely and by the monotone convergence of the measures 
    \begin{equation*}
        \liminf_{n\to\infty} \frac{L_n}{\log_2(n)} \geq 1
    \end{equation*}
    almost surely. We conclude that 
    \begin{equation*}
        1 \leq \liminf_{n\to\infty} \frac{L_n}{\log_2(n)} 
        \leq \limsup_{n\to\infty} \frac{L_n}{\log_2(n)}\leq 1
    \end{equation*}
    and the claim follows. 
\end{example}

\begin{example}[Counting Process]
    Let $X_i\in(0,\infty)$ be independent and identically distributed random variable. 
    Put $\mu = \E\sbrc{X_1}$, $T_n = \sum_{i=1}^n X_i$ and $N_t = \sup\Set{n}{T_n\leq t}$. 
    Then we have the following claim: 
    \begin{equation*}
        \lim_{t\to\infty} \frac{N_t}{t} = \frac{1}{\mu} 
    \end{equation*}
    almost surely. To see this, note that since $X_i<\infty$ for all $i$, 
    \begin{equation*}
        \lim_{t\to\infty} N_t = \lim_{t\to\infty}\sup\Set{n}{T_n\leq t} = \infty. 
    \end{equation*}
    Now, observe that $T_{N_t} \leq t\leq T_{N_t+1}$ and hence 
    \begin{equation*}
        \frac{T_{N_t}}{N_t}\leq \frac{t}{N_t} \leq \frac{T_{N_t+1}}{N_t+1}\frac{N_t+1}{N_t}. 
    \end{equation*}
    By the strong law of large number, $T_{N_t}/N_t\to \mu$ almost surely. 
    Thus 
    \begin{equation*}
        \lim_{t\to\infty} \frac{N_t}{t} = \frac{1}{\mu}. 
    \end{equation*}
\end{example}

\begin{theorem}[Glivenko-Cantelli]
    Suppose that $X_i\iidsim F$ with $X_i\in(-\infty, \infty)$ and 
    \begin{equation*}
        F_n(x) = \frac{1}{n}\sum_{i=1}^n\one\set{X_i\leq x}
    \end{equation*}
    is the empirical CDF. Then 
    \begin{equation*}
        \norm{F_n - F}_{\infty} \to 0
    \end{equation*}
    almost surely when $n\to\infty$. 
\end{theorem}
\begin{proof}
    We first claim that for $\epsilon>0$, we may find a finite partition 
    $\set{t_j}$ such that $-\infty = t_0 <\cdots<t_j = \infty$ and 
    \begin{equation*}
        F(t_{j+1}^-) - F(t_j) \leq \epsilon
    \end{equation*}
    for all $j$. To see the existence of such partition, put 
    $t_0 = -\infty$ and let 
    \begin{equation*}
        t_{j+1} = \sup\Set{t\in\R}{F(t)\leq F(t_j)+\epsilon}. 
    \end{equation*}
    Observe that $F(t_{j+1})\geq F(t_j) + \epsilon$. If not, 
    then $F(t_{j+1}) < F(t_j) + \epsilon$. By the right-continuity 
    of $F$, there is $\delta > 0$ such that $F(t_{j+1} + \delta)\leq F(t_j) + \epsilon$, 
    contradicting to the definition of $t_{j+1}$. It now also 
    follows from the definition that 
    \begin{equation*}
        F(t_{j+1}^-) \leq F(t_j) + \epsilon. 
    \end{equation*}
    Finally, since $F$ is of finite total variation, the 
    jumps of sizes greater than $\epsilon$ can occur only 
    finitely many times and we conclude the existence of such 
    partition. 

    Next, by the strong law of large number, for almost every $\omega$ 
    there is $N(\omega)$ uniform in $j$ such that 
    \begin{equation*}
        \abs{F_n(t_j) - F(t_j)} \leq \epsilon
    \end{equation*}
    for all $n>N(\omega)$. For any $t\in[t_j, t_{j+1})$, we have 
    \begin{equation*}
        F(t) - F(t_j) \leq F(t_{j+1}^{-1}) - F(t_j) \leq \epsilon. 
    \end{equation*}
    Again, by the strong law of the large number, 
    \begin{equation*}
        F_n(t_{j+1}^-) - F_n(t_j) = \frac{1}{n}\sum_{i=1}^n\one\set{t_j < X_i < t_{j+1}}
        \to \E\sbrc{\one\set{t_j < X_i < t_{j+1}}} = F(t_{j+1}^-) - F(t_j)
    \end{equation*}
    almost surely. That is, for almost every $\omega$, there is $N'(\omega)>N(\omega)$ 
    such that for all $j$, 
    \begin{equation*}
        F_n(t_{j+1}^-) - F_n(t_j) \leq F(t_{j+1}^-) - F(t_j) + \epsilon
    \end{equation*}
    if $n\geq N'(\omega)$. Combining the above estimates, if $n\geq N'(\omega)$, 
    \begin{equation*}
        \begin{split}
            \abs{F_n(t)-F(t)} &\leq \abs{F_n(t) - F_n(t_j)} + \abs{F_n(t_j) - F(t_j)} 
            + \abs{F(t_j) - F(t)} \\
            &\leq \abs{F_n(t_{j+1}^-) - F_n(t_j)} + 2\epsilon \\ 
            &\leq F(t_{j+1}^-) - F(t_j) + 3\epsilon \leq 4\epsilon. 
        \end{split}
    \end{equation*}
    Since $\epsilon$ is arbitrary, we conclude that $F_n\to F$ uniformly for 
    almost every $\omega$ and the proof is complete. 
\end{proof}

\begin{theorem}[Kolmogorov Maximal Inequality]
    Suppose that $X_i$ are independent with $\E\sbrc{X_i} = 0$ and $\Var\sbrc{X_i}<\infty$. 
    Let $S_n = \sum_{i=1}^n X_i$. Then 
    \begin{equation*}
        \P\pth{\max_{1\leq k\leq n}\abs{S_k}\geq x} \leq \frac{1}{x^2}\Var\sbrc{S_n}. 
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $A_k = \set{\abs{S_k}\geq x\text{ and }\abs{S_j}<x\text{ for $1\leq j\leq k-1$}}$. 
    Note that
    \begin{equation*}
        \sum_{k=1}^n\one_{A_k} = \one\set{\max_{1\leq k\leq n}\abs{S_k}\geq x}. 
    \end{equation*} 
    \begin{equation*}
        \begin{split}
            \E\sbrc{S_n^2} &\geq \E\sbrc{S_n^2\sum_{k=1}^n\one_{A_k}} 
            = \sum_{k=1}^n\E\sbrc{S_n^2\one_{A_k}} 
            = \sum_{k=1}^n\E\sbrc{(S_n - S_k + S_k)^2\one_{A_k}} \\
            &= \sum_{k=1}^n \E\sbrc{(S_n-S_k)^2\one_{A_k}} + 2\E\sbrc{(S_n-S_k)S_k\one_{A_k}} + \E\sbrc{S_k^2\one_{A_k}} \\ 
            &\geq \sum_{k=1}^n \E\sbrc{S_k^2\one_{A_k}} + 2\E\sbrc{(S_n-S_k)S_k\one_{A_k}}.
        \end{split}
    \end{equation*}
    Notice that $S_n-S_k\in\sigma(X_{k+1},\ldots,X_n)$ and $S_k\one_{A_k}\in\sigma(X_1,\ldots,X_k)$ 
    are independent. Thus 
    \begin{equation*}
        \begin{split}
            \E\sbrc{S_n^2} &\geq \sum_{k=1}^n \E\sbrc{S_k^2\one_{A_k}} + 2\E\sbrc{(S_n-S_k)S_k\one_{A_k}} \\
            &= \sum_{k=1}^n \E\sbrc{S_k^2\one_{A_k}}\geq x^2\sum_{k=1}^n \E\sbrc{\one_{A_k}}
            = x^2\P\pth{\max_{1\leq k\leq n}\abs{S_k}\geq x}. 
        \end{split} 
    \end{equation*}
    Hence 
    \begin{equation*}
        \P\pth{\max_{1\leq k\leq n}\abs{S_k}\geq x} \leq \frac{1}{x^2}\E\sbrc{S_n^2}. 
    \end{equation*}
\end{proof}

\begin{definition}
    Let $(\Omega,\F,\P)$ be a probability space. A sub-$\sigma$-algebra $\G$ 
    is \textbf{$\P$-trivial} if for all $A\in\G$, $\P(A)\in\set{0,1}$. 
\end{definition}

\begin{theorem}[Kolmogorov Zero-One Law]
    Let $\F_i$ be independent $\sigma$-algebras, $\G_n = \sigma(\F_n,\ldots)$ and 
    $\G_{\infty} = \cap_{n=1}^\infty\G_n$. Then $\G_\infty$ is $\P$-trivial. 
\end{theorem}
\begin{proof}
    Observe that a $\sigma$-algebra $\G$ satisfies that for $A\in\G$, $\P(A)\in\set{0,1}$ 
    if $\G$ is independent of itself. Indeed, if $\G$ is independent of 
    itself, then for any $A\in\G$, 
    \begin{equation*}
        \P(A) = \P(A\cap A) = \P(A)^2
    \end{equation*}
    implies that $\P(A) = 0$ or $1$. Now, for any given $n$, 
    $\sigma(\F_1,\ldots,\F_{n-1})$ is independent with $\G_n$ and $\G_\infty\subset\G_n$. 
    Hence $\G_\infty$ is independent of $\sigma(\F_1,\ldots,\F_{n-1})$ for all $n$. 
    
    In particular, $\G_\infty$ is independent of $\sigma(\cup_n\F_n)$. 
    To see this, note that $\cup_n\sigma(\cup_{k=1}^n\F_k)$ is a $\pi$ system that 
    generates $\sigma(\cup_n\F_n)$ and for $A\in\cup_n\sigma(\cup_{k=1}^n\F_k)$, 
    $A\in\sigma(\cup_{k=1}^n\F_k)$ for some $n$. For $B\in\G_\infty$, 
    \begin{equation*}
        \P(A\cap B) = \P(A)\P(B)
    \end{equation*}
    since $\sigma(\cup_{k=1}^n\F_k)$ and $\G_\infty$ is independent. Now it 
    follows from \cref{thm:pi-system_independence} that $\G_\infty$ and 
    $\sigma(\cup_n\F_n)$ are independent. Notice that $\G_\infty\subset\sigma(\cup_n\F_n)$ 
    and hence $\G_\infty$ is independent of itself. The proof is complete. 
\end{proof}

\begin{corollary}
    Let $X_i$ be independent and identically distributed and 
    put $S_n = \sum_{i=1}^n X_i$. Then 
    \begin{thmenum}
        \item $S_n$ is either almost surely convergent or almost surely divergent. 
        \item If $\frac{1}{n}S_n$ converges almost surely, its limit is almost surely 
        a constant. 
    \end{thmenum}
\end{corollary}
\begin{proof}
    Define $\F_i = \sigma(X_i)$ and note that $\set{S_n\text{ converges}}\in\G_{\infty} 
    = \cap_{n}\sigma(\cup_{i\geq n}\F_i)$ since 
    \begin{equation*}
        \set{S_n\text{ converges}} = \set{\lim_{n\to\infty}\sum_{i\geq n}X_i = 0}
    \end{equation*}
    is $\G_\infty$-measurable. By the Kolmogorov zero-one law, 
    \begin{equation*}
        \P\set{S_n\text{ converges}}\in\set{0,1}. 
    \end{equation*}
    This proves (a). 
    
    For (b), note that by a similar argument, we have 
    \begin{equation*}
        \set{\frac{1}{n}S_n\text{ converges}}\in\G_\infty,
    \end{equation*} 
    where $\G_\infty$ is $\P$-trivial. Also, $\lim_{n\to\infty}\frac{1}{n}S_n$ 
    is $\G_\infty$-measurable. Since $\G_\infty$ is $\P$-trivial, 
    \begin{equation*}
        F(t)\coloneq\P\set{\lim_{n\to\infty}\frac{1}{n}S_n\leq t} \in \set{0,1}. 
    \end{equation*}
    Thus 
    \begin{equation*}
        \lim_{n\to\infty}\frac{1}{n}S_n = \sup\Set{t}{F(t)= 0}
    \end{equation*}
    almost surely, proving that the limit is almost surely a constant. 
\end{proof}